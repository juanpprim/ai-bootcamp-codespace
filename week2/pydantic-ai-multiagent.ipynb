{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25cfbbc0-2036-44aa-a9b8-6101fdbce91e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee309e64-fa2a-4dca-9c48-a6595dc50eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from minsearch import Index\n",
    "\n",
    "import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4045df6-3dbd-4294-8ca8-9cdb4f52af3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0078d48de64235a33e753848a84e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "data_folder = Path('../data_cache/youtube_videos/')\n",
    "\n",
    "for f in tqdm(data_folder.glob('*.txt')):\n",
    "    filename = f.name\n",
    "    video_id, _ = filename.split('.')\n",
    "\n",
    "    transcript = f.read_text(encoding='utf-8')\n",
    "\n",
    "    chunks = docs.sliding_window(transcript, size=3000, step=1500)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk['video_id'] = video_id\n",
    "        documents.append(chunk)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8dadd9-153d-43b8-96ef-4de9ada0a29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x70dbedd099a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=['content'],\n",
    "    keyword_fields=['video_id']\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735ae8ee-f8ad-4576-b6f0-0cedd38896fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, TypedDict, Optional\n",
    "\n",
    "class SearchResult(TypedDict):\n",
    "    \"\"\"Represents a single search result entry.\"\"\"\n",
    "    start: int\n",
    "    content: str\n",
    "    video_id: str\n",
    "    _id: int # added\n",
    "\n",
    "class SearchTools:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        \n",
    "    def search(self, query: str) -> List[SearchResult]:\n",
    "        \"\"\"\n",
    "        Search the index for documents matching the given query.\n",
    "    \n",
    "        Args:\n",
    "            query (str): The search query string.\n",
    "    \n",
    "        Returns:\n",
    "            List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "                - start (int): The starting position or offset within the source file.\n",
    "                - content (str): A text excerpt or snippet containing the match.\n",
    "                - video_id (str): Youtube video_id for the snippet.\n",
    "                - _id (int): The unique id for the document\n",
    "        \"\"\"\n",
    "        return self.index.search(\n",
    "            query=query,\n",
    "            num_results=5,\n",
    "            output_ids=True,\n",
    "        )\n",
    "\n",
    "    def get_document_by_id(self, _id: int) -> Optional[SearchResult]:\n",
    "        \"\"\"\n",
    "        Retrieve a document by its unique ID.\n",
    "\n",
    "        Args:\n",
    "            _id (int): The document id.\n",
    "\n",
    "        Returns:\n",
    "            SearchResult: The document corresponding to the given ID or None if it's not in the index.\n",
    "        \"\"\"\n",
    "        if _id < 0 or _id >= len(self.index.docs):\n",
    "            return None\n",
    "\n",
    "        return self.index.docs[_id]\n",
    "\n",
    "tools = SearchTools(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c41fa18f-7b71-493b-9975-7aec38264c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import FunctionToolCallEvent\n",
    "\n",
    "class NamedCallback:\n",
    "\n",
    "    def __init__(self, agent):\n",
    "        self.agent_name = agent.name\n",
    "\n",
    "    async def print_function_calls(self, ctx, event):\n",
    "        # Detect nested streams\n",
    "        if hasattr(event, \"__aiter__\"):\n",
    "            async for sub in event:\n",
    "                await self.print_function_calls(ctx, sub)\n",
    "            return\n",
    "\n",
    "        if isinstance(event, FunctionToolCallEvent):\n",
    "            tool_name = event.part.tool_name\n",
    "            args = event.part.args\n",
    "            print(f\"TOOL CALL ({self.agent_name}): {tool_name}({args})\")\n",
    "\n",
    "    async def __call__(self, ctx, event):\n",
    "        return await self.print_function_calls(ctx, event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e8f5d-ccb5-47c0-9a81-f4ab2e3d4f53",
   "metadata": {},
   "source": [
    "## Clarifier Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01956e1a-a00a-4ce6-a600-760629dc3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "654b80cd-bc75-407e-a6cc-9a4fd2bdc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clarifier_instructions = \"\"\"\n",
    "You are the CLARIFIER agent.\n",
    "\n",
    "ROLE\n",
    "Your job is to interpret and refine the user's research request so that it can be passed\n",
    "to the RESEARCH agent for structured exploration.\n",
    "\n",
    "OBJECTIVES\n",
    "1. Understand what the user truly wants to learn or achieve (their intent).\n",
    "2. Identify the core topic and any implicit goals (e.g., learn, compare, evaluate, predict, build).\n",
    "3. Ask the user one targeted clarification question — to confirm scope, focus, or purpose.\n",
    "4. Once the user responds, synthesize a refined version of their request that includes:\n",
    "   - The clarified intent (what the user ultimately wants)\n",
    "   - The initial request (in their own words)\n",
    "   - The refined research focus (a precise version suitable for the RESEARCH agent)\n",
    "   - 3–7 search queries that capture the clarified scope and intent\n",
    "   - A short instruction summary for the RESEARCH agent explaining what to explore\n",
    "\n",
    "DATA SOURCES\n",
    "- You may use your own general knowledge to infer user intent.\n",
    "- You may use the `search()` tool to quickly check ambiguous terms or context.\n",
    "\n",
    "CONSTRAINTS\n",
    "- Ask the user for clarification **once only**.\n",
    "- Do not fabricate information; if uncertain, clarify directly with the user.\n",
    "- The goal is to output a structured handoff ready for the RESEARCH agent's Stage 1 process.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96ae92b-88f0-4006-873c-8b1ea9a89593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchInstructions(BaseModel):\n",
    "    \"\"\"\n",
    "    Output of the CLARIFIER agent.\n",
    "    Provides both the user's raw input and the refined, structured guidance\n",
    "    for the RESEARCH agent to begin its first stage.\n",
    "    \"\"\"\n",
    "    initial_request: str = Field(\n",
    "        ...,\n",
    "        description=\"The user's original question or request, captured verbatim.\"\n",
    "    )\n",
    "    refined_request: str = Field(\n",
    "        ...,\n",
    "        description=\"A clarified, rephrased, and contextually grounded version of the initial request.\"\n",
    "    )\n",
    "    user_intent: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A short summary (1–2 sentences) of what the user truly wants to accomplish \"\n",
    "            \"or learn, inferred from both the initial request and clarification.\"\n",
    "        )\n",
    "    )\n",
    "    queries: List[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A list of 3–7 specific search queries derived from the refined request, \"\n",
    "            \"covering complementary angles or subtopics the RESEARCH agent should explore.\"\n",
    "        )\n",
    "    )\n",
    "    instructions: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Concise operational guidance for the RESEARCH agent, explaining how to use \"\n",
    "            \"the queries and what to prioritize during Stage 1 research.\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2480fb14-2eea-4db3-b4af-98eaba0d6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "clarifier = Agent(\n",
    "    name='clarifier',\n",
    "    instructions=clarifier_instructions,\n",
    "    tools=[tools.search],\n",
    "    model='gpt-4o-mini',\n",
    ")\n",
    "\n",
    "clarifier_callback = NamedCallback(clarifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdde881b-3848-4549-919c-72d34f52cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how do I get started with data engineering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1661baeb-04fc-48d9-9838-af69f81f43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "clarifier_results1 = await clarifier.run(\n",
    "    user_prompt=question,\n",
    "    event_stream_handler=clarifier_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dcab848-b28b-4726-b9a3-044302070916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To better understand what you're looking for in regard to getting started with data engineering, could you clarify which specific aspects you're interested in? For example, are you looking for resources, courses, tools, skills to learn, or an overview of the field itself?\n"
     ]
    }
   ],
   "source": [
    "print(clarifier_results1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2480813f-e7a1-4ad2-90d3-6b82505caf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"I want a learning path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "895c7658-ae64-4529-a6cc-992e404fddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clarifier_results2 = await clarifier.run(\n",
    "    user_prompt=answer,\n",
    "    message_history=clarifier_results1.new_messages(),\n",
    "    output_type=ResearchInstructions,\n",
    "    event_stream_handler=clarifier_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b578c1e-43d7-4f01-8813-d0a5fe1d15ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_request='how do I get started with data engineering' refined_request='I want a structured learning path for data engineering.' user_intent='The user wants to receive a comprehensive and structured learning path to become proficient in data engineering.' queries=['data engineering learning path', 'best resources for learning data engineering', 'data engineering skills to learn', 'data engineering course recommendations', 'data engineering bootcamps', 'key tools and technologies in data engineering'] instructions='Explore comprehensive learning paths, resources, courses, and skills necessary for data engineering. Provide structured recommendations that cater to beginners.'\n"
     ]
    }
   ],
   "source": [
    "print(clarifier_results2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad1ad4e-4bfe-4e71-a583-e81eb8a578fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_task = clarifier_results2.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448a309-5b8e-4fa6-83ec-3514ae3795a6",
   "metadata": {},
   "source": [
    "## Researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18edfdce-682e-4d2b-a1e3-a9a0feb25b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher_instructions = \"\"\"\n",
    "You are the RESEARCH agent.\n",
    "\n",
    "ROLE\n",
    "You perform structured research on a proprietary podcast/video database for a specific stage\n",
    "of exploration (Stage 1, 2, or 3).\n",
    "\n",
    "DATA SOURCE\n",
    "- You may ONLY use the `search()` function\n",
    "- Every reference must cite a real snippet with a valid `youtube_id`, `timestamp` and `_id`.\n",
    "- Do not invent data, names, or timestamps.\n",
    "\n",
    "INTENT HANDLING\n",
    "- Before searching, infer the underlying intent behind the user's request.\n",
    "  Examples:\n",
    "    - “getting into ML” → learning pathways, beginner resources, first projects\n",
    "    - “AI safety concerns” → risks, ethical challenges, mitigation strategies\n",
    "    - “startup funding trends” → investment patterns, valuations, stages\n",
    "- Generate searches that reflect this **intent**, not just literal words.\n",
    "\n",
    "STAGES\n",
    "\n",
    "Stage 1 — Initial Search\n",
    "- Use the user’s question or clarified keywords from context.\n",
    "- Identify 3–5 primary keywords, run one or more searches.\n",
    "- Summarize the main findings, highlighting initial insights and directions.\n",
    "\n",
    "Stage 2 — Expansion\n",
    "- Build upon Stage 1 outputs (from context).\n",
    "- Generate 5–7 related or complementary queries.\n",
    "- Summarize recurring ideas and patterns across new results.\n",
    "\n",
    "Stage 3 — Deep Dive\n",
    "- Build upon Stage 1 and Stage 2.\n",
    "- Generate 5–7 deeper or contrasting queries.\n",
    "- Explore nuances, counterpoints, or mechanisms.\n",
    "- Provide a more analytical synthesis.\n",
    "\n",
    "CONSTRAINTS\n",
    "- Use context from previous stages to guide deeper exploration.\n",
    "- You must perform the necessary amount of queries for each stage:\n",
    "    - 3-5 for stage 1\n",
    "    - 5-7 for stage 2\n",
    "    - 5-7 for stage 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aa22541-194d-4d0d-b3ef-3db2f04f7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Reference(BaseModel):\n",
    "    \"\"\"\n",
    "    A single, verifiable citation to a transcript snippet or video segment.\n",
    "    Must correspond to a real snippet returned by the `search()` tool.\n",
    "    \"\"\"\n",
    "    document_id: int = Field(..., description=\"Internal ID of the transcript snippet.\")\n",
    "    quote: str = Field(..., description=\"Exact snippet that supports the keyword or insight.\")\n",
    "    timestamp: str = Field(..., description=\"Timestamp in the source video where the quote occurs, 'mm:ss' or 'h:mm:ss'\")\n",
    "    relevance_to_keyword: str = Field(..., description=\"Explanation of *how* this quote supports or illustrates the specific keyword or concept being explored.\")\n",
    "    relevance_to_user_intent:  str = Field(..., description=\"Explanation of *how* this quote help the user with their intent.\")\n",
    "\n",
    "class ResearchKeyword(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a keyword explicitly searched during this research stage.\n",
    "    Each keyword must match an actual query used in the search tool calls.\n",
    "    \"\"\"\n",
    "    keyword: str = Field(..., description=\"The exact keyword or phrase used in the search() tool call.\")\n",
    "    relevant_references: List[Reference] = Field(\n",
    "        ..., \n",
    "        description=\"List of transcript snippets directly relevant to this keyword. Each must include a 'relevance_to_keyword' explanation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class VerifiableInsight(BaseModel):\n",
    "    \"\"\"\n",
    "    A synthesized insight that can be traced back to specific evidence.\n",
    "    Each insight must be supported by at least one real reference.\n",
    "    \"\"\"\n",
    "    insight: str = Field(..., description=\"An insight derived from the research, phrased in an evidence-based, verifiable way.\")\n",
    "    references: List[Reference] = Field(..., description=\"Citations that directly support this insight. Must contain valid timestamps and IDs.\")\n",
    "\n",
    "\n",
    "class ResearchStageReport(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured output for each research stage (1–3).\n",
    "    Ensures traceability between searches, keywords, and findings.\n",
    "    \"\"\"\n",
    "    stage: int = Field(..., description=\"The research stage number (1 = Initial Search, 2 = Expansion, 3 = Deep Dive).\")\n",
    "    explored_keywords: List[ResearchKeyword] = Field(\n",
    "        ..., \n",
    "        description=\"List of the *exact* keywords used in this stage's search() calls, along with references showing their relevance.\"\n",
    "    )\n",
    "    verifiable_insights: List[VerifiableInsight] = Field(\n",
    "        ..., \n",
    "        description=\"List of data-backed insights derived from the references gathered at this stage.\"\n",
    "    )\n",
    "    stage_summary: str = Field(..., description=\"Analytical summary of what was learned at this stage, connecting evidence to emerging themes.\")\n",
    "    recommended_next_steps: str = Field(..., description=\"Guidance for what to do in the next stage — e.g., new angles, counterpoints, or subtopics.\")\n",
    "    recommended_next_keywords: List[str] = Field(\n",
    "        ..., \n",
    "        description=\"Suggested next queries based on gaps or promising directions discovered in this stage.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "910035b7-99d6-46b9-8f6e-571e09eaaaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = Agent(\n",
    "    name='researcher',\n",
    "    instructions=researcher_instructions,\n",
    "    tools=[tools.search],\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=ResearchStageReport\n",
    ")\n",
    "\n",
    "researcher_callback = NamedCallback(researcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4780fa73-f3b6-4b92-a49c-73b0354c5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 1\n",
    "stage_instructions = research_task.model_dump_json()\n",
    "previous_stages_json = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d436d2e-ead5-44a2-84fa-2b8b0ca7a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def do_research(\n",
    "    stage: int,\n",
    "    stage_instructions: str,\n",
    "    previous_stages: List[ResearchStageReport]\n",
    ") -> ResearchStageReport:\n",
    "    previous_stages_json = '\\n'.join([r.model_dump_json() for r in previous_stages])\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Current stage: {stage}\n",
    "\n",
    "    Stage instrustructions:\n",
    "    {stage_instructions}\n",
    "\n",
    "    Previous stages:\n",
    "    {previous_stages_json}\n",
    "    \"\"\"\n",
    "\n",
    "    callback = NamedCallback(researcher)\n",
    "\n",
    "    results = await researcher.run(\n",
    "        user_prompt=user_prompt,\n",
    "        event_stream_handler=callback,\n",
    "    )\n",
    "\n",
    "    return results.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6d2633b-80c3-4a8e-aa90-fa0747c77d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (researcher): search({\"query\": \"data engineering learning path\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"best resources for learning data engineering\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data engineering skills to learn\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data engineering course recommendations\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data engineering bootcamps\"})\n"
     ]
    }
   ],
   "source": [
    "stage1 = await do_research(\n",
    "    stage=1,\n",
    "    stage_instructions=research_task.model_dump_json(),\n",
    "    previous_stages=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79e5ca8-d820-4c43-92a7-88f0d80c5746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (researcher): search({\"query\": \"data engineering tools and technologies\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"practical projects in data engineering\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"real-world applications of data engineering\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"contrasting educational methods for data engineers\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"industry trends in data engineering\"})\n"
     ]
    }
   ],
   "source": [
    "stage2 = await do_research(\n",
    "    stage=2,\n",
    "    stage_instructions=\"continue research\",\n",
    "    previous_stages=[stage1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a424ca32-efac-4e71-b50a-26af1f9a663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (researcher): search({\"query\": \"hands-on data engineering tools\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"emerging trends in data engineering\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"real-time data processing tools\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data governance in engineering\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"advanced analytics technologies\"})\n"
     ]
    }
   ],
   "source": [
    "stage3 = await do_research(\n",
    "    stage=3,\n",
    "    stage_instructions=\"finish research\",\n",
    "    previous_stages=[stage1, stage2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cc5ae-efe5-42e0-bf7e-f99254da1f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "065b3a9d-4245-4a5a-a83f-ccf7eae5d6a8",
   "metadata": {},
   "source": [
    "## Synthesizer/Verifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66574a8f-3fe2-4ee2-bc89-46550c517ceb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d4f158d-bb35-43c1-8603-972c2f7a5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer_instructions = \"\"\"\n",
    "You are the SYNTHESIZER agent.\n",
    "\n",
    "ROLE\n",
    "You create a cohesive, factual article by synthesizing verified information from all\n",
    "three research stages (StageReports 1–3).\n",
    "\n",
    "DATA SOURCES\n",
    "- You will receive one or more `ResearchStageReport` objects, each containing\n",
    "  verifiable references with document_ids, timestamps, and quotes.\n",
    "- You have access to the tool `get_document_by_id` to retrieve full source text\n",
    "  for any reference.\n",
    "- You must use this tool to verify every claim that appears in your article.\n",
    "\n",
    "TASKS\n",
    "1. Carefully read all StageReports and extract recurring insights and verified facts.\n",
    "2. Use `get_document_by_id` to check each cited reference and confirm that\n",
    "   the quote or insight is correctly represented.\n",
    "3. Only include claims that are explicitly supported by at least one verified source.\n",
    "4. Synthesize related findings into 5–6 cohesive sections with a logical flow.\n",
    "5. Ensure that the article aligns with the original user intent (as passed from the clarifier).\n",
    "\n",
    "ARTICLE STRUCTURE\n",
    "- Introduction: Summarize what the article will explore and why it matters.\n",
    "- 5-6 body sections, each:\n",
    "  - Centered on one major theme or subtopic.\n",
    "  - Contains 3–4 related claims (each 3–4 sentences long).\n",
    "  - Each claim includes an in-text reference\n",
    "- Conclusion: Summarize the most important insights and actionable takeaways.\n",
    "\n",
    "VERIFICATION RULES\n",
    "- For every claim, retrieve at least one cited source using `get_document_by_id`\n",
    "  and confirm that the text supports the claim.\n",
    "- If a reference cannot be verified or is inconsistent, omit it.\n",
    "- Do not invent or infer facts beyond what’s supported by verified material.\n",
    "\n",
    "STYLE\n",
    "- Maintain factual, neutral, and coherent tone.\n",
    "- Avoid speculation, exaggeration, or unsupported synthesis.\n",
    "- Write in clear prose suitable for an informed but general audience.\n",
    "\n",
    "OUTPUT\n",
    "- A single, well-structured factual article ready for presentation.\n",
    "- All references cited\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04a7b78a-73f6-4938-9be0-a7dec7deaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer = Agent(\n",
    "    name='synthesizer',\n",
    "    instructions=synthesizer_instructions,\n",
    "    tools=[tools.get_document_by_id],\n",
    "    model='gpt-4o-mini',\n",
    ")\n",
    "\n",
    "synthesizer_callback = NamedCallback(synthesizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "949f1fd7-fed1-4e06-a20b-3679b232845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports = [stage1, stage2, stage3]\n",
    "reports = '\\n'.join([r.model_dump_json() for r in all_reports])\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "initial request:\n",
    "{research_task.model_dump_json()}\n",
    "\n",
    "reports:\n",
    "{reports}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2701270-9f78-4c7e-af98-9e52ca2eb229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 4492})\n",
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 5104})\n",
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 6796})\n",
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 4348})\n",
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 7208})\n",
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 6764})\n",
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 6920})\n",
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 6941})\n",
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 5100})\n",
      "TOOL CALL (synthesizer): get_document_by_id({\"_id\": 6943})\n"
     ]
    }
   ],
   "source": [
    "synthesizer_results = await synthesizer.run(\n",
    "    user_prompt=user_prompt,\n",
    "    event_stream_handler=synthesizer_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86a9cc64-9ab3-413b-b27c-0dc5c13526fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A Comprehensive Learning Path to Data Engineering\n",
      "\n",
      "Data engineering is a critical field that involves the collection, storage, and transformation of data, enabling businesses to harness information for improved decision-making. As the demand for data engineers has surged, individuals interested in this profession need a structured learning path that encompasses essential skills, tools, and resources. This article provides a detailed roadmap for aspiring data engineers, outlining key areas of focus for effective skill acquisition.\n",
      "\n",
      "## Importance of Mentorship and Guidance\n",
      "\n",
      "A structured learning path in data engineering is often facilitated by mentorship. Many successful individuals attribute their growth to guidance from experienced professionals in the field. For instance, one learner emphasized that working with data science experts significantly shaped their career, allowing them to identify critical areas of focus (Document ID: 4492). This illustrates how finding a mentor can steer aspiring data engineers toward essential topics, enhancing their chances of success.\n",
      "\n",
      "## Fundamental Skills and Knowledge Areas\n",
      "\n",
      "The foundational skills for a data engineer primarily revolve around programming and data management. Coding proficiency is vital, as many data-related tasks rely on scripting and automation to handle data effectively. One expert noted that coding abilities are crucial, particularly in environments where data scientists may lack coding skills (Document ID: 6796). Additionally, understanding data structures, ETL (Extract, Transform, Load) processes, and database management systems is essential for efficient data handling. Aspiring engineers should also familiarize themselves with relevant theoretical knowledge as they progress.\n",
      "\n",
      "## Recommended Educational Resources\n",
      "\n",
      "When it comes to learning tools and technologies, various educational resources are available for aspiring data engineers. The book \"Fundamentals of Data Engineering\" by Joe and Matt is often recommended for beginners as a solid resource to build foundational skills (Document ID: 5104). Furthermore, popular online courses have emerged in this field, with one highly praised data engineering course reportedly experiencing a surge in enrollment due to its effectiveness (Document ID: 4348). Course creators often promote their programs through recommendations from alumni, indicating the value of quality content in structured learning paths.\n",
      "\n",
      "## Exploring Bootcamps and Practical Experiences\n",
      "\n",
      "For those seeking immersive experiences, data engineering bootcamps offer an accelerated way to gain skills in a structured environment. Although fewer in number compared to data science bootcamps, there are meaningful options available (Document ID: 7208). Attending a bootcamp allows learners to engage in hands-on projects and collaborate with peers, fostering an environment conducive to skill development. This practical experience not only enhances learning but also prepares individuals for real-world challenges in data management.\n",
      "\n",
      "## Familiarization with Tools and Technologies\n",
      "\n",
      "A critical component of data engineering is the understanding of various tools utilized in building data pipelines and managing data. Data engineers primarily work with ETL processes that involve extracting data from sources, transforming it into a usable format, and loading it into databases (Document ID: 6764). Popular tools in this realm include Apache Kafka, Spark, and cloud services, which are pivotal for modern data workflows. It’s essential for aspiring engineers to explore these tools and engage in hands-on projects to understand their practical applications (Document ID: 5100).\n",
      "\n",
      "## Collaborative Nature of Data Engineering\n",
      "\n",
      "The data engineering landscape is inherently collaborative, requiring teamwork to solve complex data challenges. Effective data management relies on a team of skilled professionals, including at least one dedicated data engineer (Document ID: 6920). Understanding team dynamics, stakeholder management, and project organization are critical for aspiring engineers. This collaborative ethos enhances the effectiveness of data engineering initiatives and supports continuous learning among team members.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Data engineering presents exciting opportunities for those interested in transforming data into actionable insights. By following a structured learning path that encompasses mentorship, foundational skills, educational resources, practical experiences, tool familiarity, and teamwork, aspiring data engineers can position themselves for success in this dynamic field. The emphasis on continuous learning and adaptation is crucial as the landscape of data engineering continually evolves, ensuring professionals remain competitive and effective in their roles. With dedication and the right support, individuals can embark on a fulfilling career in data engineering. \n",
      "\n",
      "### References\n",
      "1. Document ID: 4492\n",
      "2. Document ID: 5104\n",
      "3. Document ID: 6796\n",
      "4. Document ID: 4348\n",
      "5. Document ID: 7208\n",
      "6. Document ID: 6764\n",
      "7. Document ID: 5100\n",
      "8. Document ID: 6920\n"
     ]
    }
   ],
   "source": [
    "print(synthesizer_results.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710e9e0-6d20-42fd-a4df-b250fc24bc25",
   "metadata": {},
   "source": [
    "## Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25a8b668-9e33-483c-ac5c-35ba598d1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import RunContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4cc2707e-e98f-49ab-9859-967be5db6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_messages(messages):\n",
    "    contents = []\n",
    "    \n",
    "    for m in messages:\n",
    "        print(m.kind)\n",
    "\n",
    "        for p in m.parts:\n",
    "            print(p.part_kind)\n",
    "            kind = p.part_kind\n",
    "            if kind == 'user-prompt' or kind == 'text':\n",
    "                print(p.content)\n",
    "            if kind == 'tool-call': \n",
    "                print(p.tool_name, p.args)\n",
    "            if kind == 'tool-return':\n",
    "                print(type(p.content), p.content)\n",
    "            print()\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0407ff26-e5d8-43c0-bc6b-9c8ae36f2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator_instructions = \"\"\"\n",
    "first, ask user an initial question via clarifier (clarify_tool_initial)\n",
    "then formulate requiremets for the researcher (clarify_tool_research_task)\n",
    "then execute research via researcher in three stages: 1, 2, 3 (reserch_tool)\n",
    "each research step should be done after the previous one is completed\n",
    "\n",
    "make it timeless: don't add years to queries. for example:\n",
    "\"learning machine learning\" is better than \"learning machine learning in 2023\"\n",
    "\n",
    "when the resarch it ready, output a short summary of the research\n",
    "\"\"\"\n",
    "\n",
    "orchestrator = Agent(\n",
    "    name='orchestrator',\n",
    "    instructions=orchestrator_instructions,\n",
    "    model='gpt-4o-mini',\n",
    ")\n",
    "\n",
    "orchestrator_callback = NamedCallback(orchestrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a73d94df-ccc8-497b-b4e8-5b630560115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@orchestrator.tool\n",
    "async def clarify_tool_initial(ctx: RunContext, query: str) -> str:\n",
    "    \"\"\"Runs the clarifier once to interpret the user's request.\n",
    "\n",
    "    Args:\n",
    "        query: Raw user question.\n",
    "\n",
    "    Returns:\n",
    "        A short text summary describing the user's intent.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Clarifier (Initial) ===\")\n",
    "    callback = NamedCallback(clarifier)\n",
    "    results = await clarifier.run(user_prompt=query, event_stream_handler=callback)\n",
    "    return results.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "784c8352-7219-4a43-903d-3245bd6fbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "@orchestrator.tool\n",
    "async def clarify_tool_research_task(ctx: RunContext, query: str) -> ResearchInstructions:\n",
    "    \"\"\"Runs the clarifier again using both the user query and prior clarifier output\n",
    "    to create a structured ResearchInstructions object.\n",
    "\n",
    "    Args:\n",
    "        query: User's original question.\n",
    "\n",
    "    Returns:\n",
    "        ResearchInstructions with refined request, intent, and search queries.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Clarifier (Research Task) ===\")\n",
    "    prior_outputs = []\n",
    "    for m in ctx.messages:\n",
    "        for p in m.parts:\n",
    "            if p.part_kind == \"tool-return\" and p.tool_name == \"clarify_tool_initial\":\n",
    "                prior_outputs.append(p.content)\n",
    "\n",
    "    prior_text = \"\\n\".join(str(x) for x in prior_outputs)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    User query:\n",
    "    {query}\n",
    "    \n",
    "    Prior clarification:\n",
    "    {prior_text}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    callback = NamedCallback(clarifier)\n",
    "\n",
    "    results = await clarifier.run(\n",
    "        user_prompt=prompt,\n",
    "        event_stream_handler=callback,\n",
    "        output_type=ResearchInstructions\n",
    "    )\n",
    "\n",
    "    return results.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15a4b09f-0bb1-4cee-b0d8-bbae15511f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@orchestrator.tool\n",
    "async def research_tool(ctx: RunContext, stage: int, stage_instructions: str) -> ResearchStageReport:\n",
    "    \"\"\"Runs one stage of research using prior reports as context.\n",
    "\n",
    "    Args:\n",
    "        stage: Research stage number (1–3).\n",
    "        stage_instructions: Description of what this stage should focus on.\n",
    "\n",
    "    Returns:\n",
    "        ResearchStageReport with insights, references, and next steps.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== RESEARCH stage {stage} ===\")\n",
    "    \n",
    "    prior_reports: List[ResearchStageReport] = []\n",
    "\n",
    "    for m in ctx.messages:\n",
    "        for p in m.parts:\n",
    "            if p.part_kind == \"tool-return\" and p.tool_name == \"research_tool\":\n",
    "                if isinstance(p.content, ResearchStageReport):\n",
    "                    prior_reports.append(p.content)\n",
    "    \n",
    "    result = await do_research(\n",
    "        stage=stage,\n",
    "        stage_instructions=stage_instructions,\n",
    "        previous_stages=prior_reports,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f210b4c-8408-48f2-9393-14a709c5e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d87e4ce0-29b8-444d-9ab8-717f13c62deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how do I get started with data engineering?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce903c69-bd36-487b-8803-25d7f084ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (orchestrator): clarify_tool_initial({\"query\":\"how do I get started with data engineering?\"})\n",
      "\n",
      "=== Clarifier (Initial) ===\n"
     ]
    }
   ],
   "source": [
    "orchestrator_results = await orchestrator.run(\n",
    "    user_prompt=question,\n",
    "    message_history=message_history,\n",
    "    event_stream_handler=orchestrator_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0086e914-9094-4939-b8a7-801be8e931e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "user-prompt\n",
      "how do I get started with data engineering?\n",
      "\n",
      "\n",
      "response\n",
      "tool-call\n",
      "clarify_tool_initial {\"query\":\"how do I get started with data engineering?\"}\n",
      "\n",
      "\n",
      "request\n",
      "tool-return\n",
      "<class 'str'> To refine your request, could you clarify whether you're looking for specific educational resources (like online courses or books), skill sets to develop, tools to learn, or career advice in the field of data engineering?\n",
      "\n",
      "\n",
      "response\n",
      "text\n",
      "Could you clarify whether you're looking for specific educational resources (like online courses or books), skill sets to develop, tools to learn, or career advice in the field of data engineering?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = orchestrator_results.new_messages()\n",
    "message_history.extend(messages)\n",
    "print_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ea3a1-e720-451c-9a3e-83e61f8b2d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2d8b3c4-5685-49d5-ab92-4eb7957cd1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (orchestrator): clarify_tool_research_task({\"query\":\"how do I get started with data engineering?\"})\n",
      "\n",
      "=== Clarifier (Research Task) ===\n",
      "TOOL CALL (orchestrator): research_tool({\"stage\":1,\"stage_instructions\":\"Explore the best online courses for data engineering.\"})\n",
      "\n",
      "=== RESEARCH stage 1 ===\n",
      "TOOL CALL (researcher): search({\"query\":\"best online courses for data engineering\"})\n",
      "TOOL CALL (orchestrator): research_tool({\"stage\":2,\"stage_instructions\":\"Identify essential skills needed for data engineers.\"})\n",
      "\n",
      "=== RESEARCH stage 2 ===\n",
      "TOOL CALL (researcher): search({\"query\": \"essential skills for data engineers\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data engineering tools and technologies\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data engineering best practices\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data engineer role responsibilities\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data engineering skills needed for the future\"})\n",
      "TOOL CALL (orchestrator): research_tool({\"stage\":3,\"stage_instructions\":\"Research common tools used in data engineering.\"})\n",
      "\n",
      "=== RESEARCH stage 3 ===\n",
      "TOOL CALL (researcher): search({\"query\": \"common tools used in data engineering\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"tools for SQL optimization in data engineering\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"cloud computing tools for data engineers\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data transformation tools for data engineers\"})\n",
      "TOOL CALL (researcher): search({\"query\": \"data pipeline tools for data engineering\"})\n"
     ]
    }
   ],
   "source": [
    "orchestrator_results = await orchestrator.run(\n",
    "    user_prompt=question,\n",
    "    message_history=message_history,\n",
    "    event_stream_handler=orchestrator_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db0f2f21-4f2d-47d2-9970-38418cfa0e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "user-prompt\n",
      "how do I get started with data engineering?\n",
      "\n",
      "\n",
      "response\n",
      "tool-call\n",
      "clarify_tool_research_task {\"query\":\"how do I get started with data engineering?\"}\n",
      "\n",
      "\n",
      "request\n",
      "tool-return\n",
      "<class '__main__.ResearchInstructions'> initial_request='how do I get started with data engineering?' refined_request=\"I'm looking for resources and a roadmap to start a career in data engineering.\" user_intent='The user wants to understand what steps to take and which resources to use to begin a career in data engineering.' queries=['best online courses for data engineering', 'essential skills for data engineers', 'tools commonly used in data engineering', 'career paths in data engineering', 'books on data engineering', 'data engineering project ideas for beginners'] instructions='Explore resources, courses, skills, and tools necessary for starting a career in data engineering.'\n",
      "\n",
      "\n",
      "response\n",
      "tool-call\n",
      "research_tool {\"stage\":1,\"stage_instructions\":\"Explore the best online courses for data engineering.\"}\n",
      "\n",
      "\n",
      "request\n",
      "tool-return\n",
      "<class '__main__.ResearchStageReport'> stage=1 explored_keywords=[ResearchKeyword(keyword='best online courses for data engineering', relevant_references=[Reference(document_id=1554, quote=\"so right now we have three courses free different courses so ml engineering course data engineering course and mops course...the data engineering course is quite different so it's a different kind of job right so then people take all these courses...\", timestamp='18:05', relevance_to_keyword='The speaker discusses the availability of different online courses, including data engineering, which indicates there are structured learning paths available for aspiring data engineers.', relevance_to_user_intent='This shows that there are formal education options especially tailored for data engineering that the user might explore.'), Reference(document_id=888, quote=\"if you want to pick up some knowledge about some specific package to analyze or visualize data, it's enough to just go to code academy...\", timestamp='20:00', relevance_to_keyword='Mentions online platforms like Codecademy, which offer targeted courses for specific skills needed in data engineering.', relevance_to_user_intent='This provides actionable information for the user to consider specific courses available on popular platforms.'), Reference(document_id=889, quote='i personally find this fundamental online class...designed by yuri kashnicki a super well structured course which get your deep understanding of the algorithm and also provide a lot of practice...', timestamp='21:18', relevance_to_keyword='Highlights a specific course by Yuri Kashnicki that could be beneficial for achieving depth in data engineering concepts and practices.', relevance_to_user_intent='This leads the user to a particular online course that may enhance their understanding and skills in data engineering.')])] verifiable_insights=[VerifiableInsight(insight='There are several online courses available specifically designed for data engineering, including distinct courses in ML engineering and MLOps.', references=[Reference(document_id=1554, quote='so right now we have three courses free different courses so ml engineering course data engineering course and mops course...the data engineering course is quite different...', timestamp='18:05', relevance_to_keyword='The speaker discusses the availability of different online courses, including data engineering.', relevance_to_user_intent='Indicates that there are formal education options for aspiring data engineers.')]), VerifiableInsight(insight='Codecademy and other platforms offer introductory courses that can help in acquiring specific tools used in data engineering, facilitating a tailored learning experience.', references=[Reference(document_id=888, quote=\"if you want to pick up some knowledge about some specific package to analyze or visualize data, it's enough to just go to code academy...\", timestamp='20:00', relevance_to_keyword='Mentions online platforms like Codecademy, offering targeted courses for data engineering skills.', relevance_to_user_intent='Provides practical options for the user to start learning online.')]), VerifiableInsight(insight='Structured online classes, like those created by recognized instructors such as Yuri Kashnicki, offer in-depth knowledge and practical experience in data engineering concepts.', references=[Reference(document_id=889, quote='i personally find this fundamental online class...designed by yuri kashnicki a super well structured course...', timestamp='21:18', relevance_to_keyword='Highlights a specific course that enhances understanding in data engineering.', relevance_to_user_intent='Directs the user toward a valuable resource for advancing their career in data engineering.')])] stage_summary='The initial search identified several resources and references related to online courses for data engineering. Notably, several structured courses exist that cater directly to this field, including ML engineering and data engineering courses. Many platforms, such as Codecademy, provide targeted learning options, which the user could pursue to build relevant skills. Specialized courses designed by recognized experts can also offer deeper understanding and practical experience.' recommended_next_steps='Explore specific course offerings on various platforms and consider comparing them to select the most beneficial ones. Dig deeper into individual course content and user experiences with those courses.' recommended_next_keywords=['course curricula for data engineering', 'top-rated data engineering courses', 'data engineering bootcamps', 'student reviews on data engineering courses', 'industry-recognized data engineering certifications']\n",
      "\n",
      "\n",
      "response\n",
      "tool-call\n",
      "research_tool {\"stage\":2,\"stage_instructions\":\"Identify essential skills needed for data engineers.\"}\n",
      "\n",
      "\n",
      "request\n",
      "tool-return\n",
      "<class '__main__.ResearchStageReport'> stage=2 explored_keywords=[ResearchKeyword(keyword='essential skills for data engineers', relevant_references=[Reference(document_id=6769, quote='coding skills is often very important for data engineers, allowing them to write effective queries and manage data pipelines effectively.', timestamp='12:12', relevance_to_keyword='Emphasizes the crucial need for coding skills, which is foundational for executing data engineering tasks.', relevance_to_user_intent='This aids in identifying the core skills the user should focus on or develop.'), Reference(document_id=6791, quote='data engineers should be adept at writing SQL queries and optimizing them, switching between SQL and no-SQL databases as required.', timestamp='12:06', relevance_to_keyword='Highlights the flexibility required in using various database types and optimizing query performance.', relevance_to_user_intent='This insight directly points to the technical abilities the user must cultivate in data engineering.'), Reference(document_id=6772, quote='skills like cloud computing, data modeling, and understanding data quality are critical.', timestamp='16:58', relevance_to_keyword='This discussion outlines important technical and theoretical knowledge needed in data engineering.', relevance_to_user_intent='Informs the user about broader areas of expertise they should explore in data engineering.')])] verifiable_insights=[VerifiableInsight(insight='Core skills needed for data engineers include strong coding abilities, particularly in SQL, and familiarity with both SQL and no-SQL databases.', references=[Reference(document_id=6769, quote='coding skills is often very important for data engineers, allowing them to write effective queries and manage data pipelines effectively.', timestamp='12:12', relevance_to_keyword='Emphasizes the crucial need for coding skills.', relevance_to_user_intent='This helps the user understand a foundational skill they must develop.'), Reference(document_id=6791, quote='data engineers should be adept at writing SQL queries and optimizing them, switching between SQL and no-SQL databases as required.', timestamp='12:06', relevance_to_keyword='Highlights the flexibility required in using various database types.', relevance_to_user_intent='Points to the technical abilities necessary for data engineering.')])] stage_summary='The expansion phase identified critical skills necessary for data engineers, particularly emphasizing coding proficiency, SQL query optimization, and familiarity with both SQL and no-SQL databases. Additionally, knowledge of cloud computing and data modeling emerged as important areas to explore. These core skills lay the groundwork for a successful career in data engineering.' recommended_next_steps='Examine specific areas in each of these skill categories to identify preferred learning pathways. Consider researching toolsets, and stack frameworks commonly used in data engineering roles to enhance practical knowledge.' recommended_next_keywords=['data engineering skill assessment', 'SQL optimization best practices', 'popular data engineering tools', 'cloud computing for data engineers', 'data modeling techniques']\n",
      "\n",
      "\n",
      "response\n",
      "tool-call\n",
      "research_tool {\"stage\":3,\"stage_instructions\":\"Research common tools used in data engineering.\"}\n",
      "\n",
      "\n",
      "request\n",
      "tool-return\n",
      "<class '__main__.ResearchStageReport'> stage=3 explored_keywords=[ResearchKeyword(keyword='common tools used in data engineering', relevant_references=[Reference(document_id=5127, quote=\"i think in startups it's pretty common that they hire a data scientist and then it turns out that this data scientist actually needs to do data engineering work before they can start with data science\", timestamp='40:51', relevance_to_keyword='This snippet shows how data scientists often overlap with data engineering roles, necessitating shared tools and workflows in modern data practices.', relevance_to_user_intent='Understanding the fabric that connects different roles will help in grasping common tools used across functions.'), Reference(document_id=6764, quote=\"my main responsibility is building data pipelines; usually it's ETL format, extract transform load\", timestamp='4:33', relevance_to_keyword='Introduces ETL processes as a foundational practice in data engineering, emphasizing the importance of relevant tools for managing data flows.', relevance_to_user_intent='For someone exploring data engineering, understanding pipeline management tools is key.'), Reference(document_id=6922, quote='modern data stack for analytics is like how I would describe it where you have an ELT tool like five friends, stage etc.', timestamp='42:05', relevance_to_keyword='Mentions modern tools such as FiveTran along with others that constitute a modern data stack, essential for contemporary data engineering practices.', relevance_to_user_intent='The user gains insight into specific tools and frameworks to consider in data engineering projects.')])] verifiable_insights=[VerifiableInsight(insight='Data engineering relies heavily on tools that facilitate ETL (Extract, Transform, Load) processes and modern data stacks, including cloud-based solutions.', references=[Reference(document_id=5127, quote=\"i think in startups it's pretty common that they hire a data scientist and then it turns out that this data scientist actually needs to do data engineering work before they can start with data science\", timestamp='40:51', relevance_to_keyword='Highlights the integration between data engineering and data science roles.', relevance_to_user_intent='Understanding these overlaps informs the user about the versatility and tools needed in data engineering.'), Reference(document_id=6764, quote=\"my main responsibility is building data pipelines; usually it's ETL format, extract transform load\", timestamp='4:33', relevance_to_keyword='Discusses ETL as a foundational process in data engineering.', relevance_to_user_intent='Emphasizes the significance of efficient data pipeline tools and techniques.'), Reference(document_id=6922, quote='modern data stack for analytics is like how I would describe it where you have an ELT tool like five friends, stage etc.', timestamp='42:05', relevance_to_keyword='Identifies specific tools in the modern data stack essential for data engineering.', relevance_to_user_intent='Provides specific modern tools that are critical for building data engineering capabilities.')])] stage_summary='This stage revealed a rich landscape of tools essential for data engineering, such as ETL processes and modern data stack solutions like FiveTran. The examination of integration between data engineering and data science roles highlighted the necessity for adaptable tools and workflows across different functions. Key insights include the critical role of cloud solutions and specific software tools that streamline pipeline management and data transformation.' recommended_next_steps='Delve deeper into specific tools mentioned, such as exploring FiveTran, Stage, and other ETL tools. Investigate best practices for implementing these tools in various data engineering contexts, including case studies or user experiences.' recommended_next_keywords=['best ETL tools for data engineering', 'cloud platforms for data engineering', 'data pipeline frameworks', 'automation tools for data engineering', 'comparing data engineering tools']\n",
      "\n",
      "\n",
      "response\n",
      "text\n",
      "### Summary of the Research on Getting Started with Data Engineering\n",
      "\n",
      "1. **Online Courses**: There are various structured online courses available specifically for data engineering. Notable platforms include:\n",
      "   - **Codecademy** offers targeted courses that help build specific skills related to data engineering, such as data visualization and analysis.\n",
      "   - A specific **fundamental online class** structured by Yuri Kashnicki is recommended for deep understanding and practical experience in data engineering concepts.\n",
      "\n",
      "2. **Essential Skills**: Key skills needed for aspiring data engineers include:\n",
      "   - **Coding proficiency**, particularly in SQL.\n",
      "   - Ability to optimize SQL queries and work with both SQL and no-SQL databases.\n",
      "   - Knowledge of **cloud computing, data modeling**, and understanding data quality.\n",
      "\n",
      "3. **Common Tools**: Familiarity with tools that facilitate data engineering workflows is crucial. This includes:\n",
      "   - **ETL (Extract, Transform, Load)** processes as foundational practices.\n",
      "   - Modern data stack solutions, including tools like **FiveTran**, which are essential for building effective data pipelines.\n",
      "\n",
      "### Recommended Next Steps\n",
      "- **Explore specific course offerings** on platforms such as Codecademy and delve into instructor-led courses for comprehensive learning.\n",
      "- **Develop coding skills** focusing on SQL and relational database management systems.\n",
      "- **Familiarize yourself with data engineering tools** like ETL frameworks and cloud platforms to understand their usage and advantages.\n",
      "\n",
      "This roadmap serves as a foundational guide to efficiently start a career in data engineering.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = orchestrator_results.new_messages()\n",
    "message_history.extend(messages)\n",
    "print_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef107dbf-c253-48d5-ac5c-a6f4093bacd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_reports: List[ResearchStageReport] = []\n",
    "requitements = None\n",
    "\n",
    "for m in message_history:\n",
    "    for p in m.parts:\n",
    "        if p.part_kind == \"tool-return\":\n",
    "            if p.tool_name == \"research_tool\":\n",
    "                if isinstance(p.content, ResearchStageReport):\n",
    "                    prior_reports.append(p.content)\n",
    "            if p.tool_name == 'clarify_tool_research_task':\n",
    "                requitements= p.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "733ddf86-549f-4924-a0f0-de7af1bad213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResearchInstructions(initial_request='how do I get started with data engineering?', refined_request=\"I'm looking for resources and a roadmap to start a career in data engineering.\", user_intent='The user wants to understand what steps to take and which resources to use to begin a career in data engineering.', queries=['best online courses for data engineering', 'essential skills for data engineers', 'tools commonly used in data engineering', 'career paths in data engineering', 'books on data engineering', 'data engineering project ideas for beginners'], instructions='Explore resources, courses, skills, and tools necessary for starting a career in data engineering.')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "564348ca-6cc5-426c-92d9-48cf6101456a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prior_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4a5cf-c3b7-4b0a-a207-a0be5a340d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
