{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5bf726-968a-42ff-bd59-e5623d8b801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the code we wrote in docs.ipynb is in the github.py file\n",
    "\n",
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed4ac42-7f34-436d-b67c-b0ee0575abff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21712"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = parsed_data[45]['content']\n",
    "len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4285258-d56a-4023-bd7a-811bf6801686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm_structured(instructions, user_prompt, output_type, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_type\n",
    "    )\n",
    "\n",
    "    return response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200844a5-6435-4778-811c-946c32a9b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Split the provided document into logical sections that make sense for a Q&A system.\n",
    "Each section should be self-contained and cover a specific topic or concept.\n",
    "Sections should be relatively large (3000-5000 characters).\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724058e7-ae55-447d-a2ab-f3e3e80b5ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ba06d6-b6aa-4d16-9a9a-a0d4e39976dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Section(BaseModel):\n",
    "    title: str\n",
    "    markdown: str\n",
    "\n",
    "class Document(BaseModel):\n",
    "    title: str\n",
    "    sections: list[Section]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f82b9d-28c7-4a93-9876-fb99bb6725fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_structured(\n",
    "    instructions=instructions,\n",
    "    user_prompt=long_text,\n",
    "    output_type=Document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671c64de-2f22-40f5-8f2e-a1e4cd666469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f56d962-7088-4f17-a53d-8f9159a369e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Testing for LLM Outputs\n",
      "\n",
      "-- Introduction to Regression Testing for LLM Outputs --\n",
      "\n",
      "In this tutorial, you will learn how to perform regression testing for LLM outputs in a structured way. \\n\\nRegression testing is crucial when modifying systems, as it allows you to verify that new changes don't negatively affect existing functionality. In the context of language models, this involves comparing new responses generated by the model against reference responses to spot significant changes over time.\\n\\nBy the end of the tutorial, you'll be able to run tests that compare the outputs of your LLM before and after modifications, ensuring that updates maintain quality and accuracy. You can make updates with confidence and quickly identify any arising issues that need addressing.\n",
      "\n",
      "-- Setup and Prerequisites --\n",
      "\n",
      "### Required Tools\\nTo complete this tutorial, ensure you have the following:  \\n1. **Basic Python knowledge:** You should be comfortable with basic Python syntax and data structures.  \\n2. **OpenAI API key:** You will need a valid key to access the LLM evaluator.  \\n3. **Evidently Cloud account:** Sign up for a free account on [Evidently AI](https://www.evidentlyai.com/register) to track and visualize test results.\\n\\n### Example Framework\\nThis example uses Evidently Cloud to facilitate regression testing. For running the tests, you will execute evaluations in Python and choose to either upload them to the cloud or view the reports locally.\\n\\n**Installation Notes:** \\nBefore you start, ensure you have Evidently installed using the command:  \\n```bash\\npip install evidently[llm]\\n```\\nIf you want to create monitoring panels as code or perform other advanced operations, additional imports will be necessary.\n",
      "\n",
      "-- Tutorial Scope and Steps Overview --\n",
      "\n",
      "### Tutorial Overview\\nIn this tutorial, we will go through the following steps: \\n1. Create a toy dataset with questions and reference responses.\\n2. Simulate generating new answers to the questions.\\n3. Create and run a Report with Tests to compare old answers with the new ones, focusing on length, correctness, and style consistency using LLM as an evaluator.\\n4. Build a monitoring Dashboard to visualize the results of the Tests over time.  \\n\\n### Key Functions To Be Implemented\\n- **Dataset Creation:** We will build a dataset containing question-response pairs for evaluation.\\n- **Answer Generation:** This step simulates how you would obtain new answers after modifying your model or prompt.\\n- **LLM Evaluations:** We will implement several tests to ensure new outputs meet specific quality metrics.\\n- **Dashboard Creation:** Finally, we will set up a dashboard to visualize ongoing results.\n",
      "\n",
      "-- Installation and Imports for Regression Testing --\n",
      "\n",
      "### Required Imports\\nIn your Python environment, make sure to import the necessary modules as follows:  \\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\n### Connecting to Evidently Cloud\\nNext, you'll need to connect to Evidently Cloud. Make sure to pass your actual token:  \\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\nws = CloudWorkspace(token='YOUR_API_TOKEN', url='https://app.evidently.cloud')\\n```\\n\\n### Setting Your OpenAI API Key\\nRemember to set your OpenAI API key using the following method:  \\n```python\\nimport os\\nos.environ['OPENAI_API_KEY'] = 'YOUR_KEY'\\n```\\nWith these setups done, you'll be ready for the following steps in the tutorial.\n",
      "\n",
      "-- Creating a Toy Dataset --\n",
      "\n",
      "In this section, we will create a toy dataset that includes a list of questions along with their corresponding reference answers. This dataset will serve as our benchmark for evaluating the answers generated by the LLM following any prompt modifications.\\n\\n### Step 1: Define the Dataset\\nYou can use the following Python code to create the toy dataset: \\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n### Step 2: Preview the Dataset\\nTo ensure your dataset is correctly structured, you can preview its contents using:  \\n```python\\npd.set_option('display.max_colwidth', None)\\nref_data.head()\\n```\\nThis preview allows you to confirm that your dataset has been created as expected, with each question matched to its proper reference response.\n",
      "\n",
      "-- Generating New Answers --\n",
      "\n",
      "Once you have the original dataset in place, you would now want to generate new answers after making changes to your prompt or model parameters. \\n\\nWe will simulate the generation of new answers by adding a new column to the existing DataFrame. \\n\\n### Step 1: Simulate Answer Generation\\nHere is a sample code to illustrate how to add new responses to your DataFrame: \\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\", \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\", \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\", \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\", \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon's shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n] \\n\\ncolumns = [\"question\", \"target_response\", \"response\"]\\n\\neval_data = pd.DataFrame(data, columns=columns)\\n```\\n### Step 2: Review the New Dataset\\nOnce the new data has been added, you can review the resulting DataFrame to confirm that the new responses align with the expected outputs.\n",
      "\n",
      "-- Designing the Test Suite for Evaluations --\n",
      "\n",
      "With both the reference and new answers prepared, we can now design a test suite to compare these answers.\\n\\n### Key Evaluation Metrics\\nWe will focus on three main criteria in our tests:  \\n1. **Length check:** Ensure that all new responses do not exceed a specified length (e.g., 200 characters).  \\n2. **Correctness check:** Confirm that new responses do not contradict the reference answers.  \\n3. **Style check:** Evaluate whether the style of the new responses matches that of the reference answers.\\n\\nUsing LLM-as-a-judge can help in implementing the correctness and style checks effectively.\n",
      "\n",
      "-- Implementing Correctness and Style Evaluators --\n",
      "\n",
      "To evaluate correctness and style, we will implement custom judges using `BinaryClassificationPromptTemplate`. \\n\\n### Correctness Judge\\nHere, we will create a judge that assesses if the new response is correct based on the reference response. \\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n    criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n    The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n    REFERENCE:\\n    =====\\n    {target_response}\\n    =====\"\"\",\\n    target_category=\"incorrect\",\\n    non_target_category=\"correct\",\\n    uncertainty=\"unknown\",\\n    include_reasoning=True,\\n    pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n)\\n```\\n\\n### Style Judge\\nNext, create a judge focusing solely on matching the style attributes of the responses. \\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n    criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\n    The ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n    Consider the following STYLE attributes:\\n    - tone (friendly, formal, casual, sarcastic, etc.)\\n    - sentence structure (simple, compound, complex, etc.)\\n    - verbosity level (relative length of answers)\\n    - and other similar attributes that may reflect difference in STYLE.\\n    You must focus only on STYLE. Ignore any differences in contents.\\n    =====\\n    {target_response}\\n    =====\"\"\",\\n    target_category=\"style-mismatched\",\\n    non_target_category=\"style-matching\",\\n    uncertainty=\"unknown\",\\n    include_reasoning=True,\\n    pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n)\\n```\\nThese custom judges will help in checking both correctness and style of the LLM outputs.\n",
      "\n",
      "-- Running the Evaluations and Generating Reports --\n",
      "\n",
      "### Step 1: Score the New Responses\\nOnce we have defined the descriptors for correctness and style checking, we need to apply them to the `eval_dataset`. This involves setting up row-level descriptors that process each individual response and add a score or label to the dataset.\\n\\n```python\\ndescriptors=[\\n    LLMEval(\"response\",\\n        template=correctness,\\n        provider=\"openai\",\\n        model=\"gpt-4o-mini\",\\n        alias=\"Correctness\",\\n        additional_columns={\"target_response\": \"target_response\"}),\\n    LLMEval(\"response\",\\n        template=style_match,\\n        provider=\"openai\",\\n        model=\"gpt-4o-mini\",\\n        alias=\"Style\",\\n        additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")\\n]\\n\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\n### Step 2: Create a Report\\nThis step will summarize the test results and include both metrics and tests with defined conditions. \\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)])\\n])\\n```\\n### Step 3: Run the Report\\nNow that we have constructed our report, we can execute it to evaluate the new responses against the reference data. \\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\nThis will allow you to view the aggregate results and raw text outputs in the Evidently Cloud.\n",
      "\n",
      "-- Testing with Updated Responses --\n",
      "\n",
      "### Repeat the Evaluation Process\\nIn case you modify your prompt again or have new responses after further iterations, you can create a new dataset and repeat the evaluation process. Simply create a new dataset for the modified outputs using similar code as before and employ the same descriptors and report structure.\n",
      "\n",
      "-- Creating Dashboards for Monitoring Results --\n",
      "\n",
      "To track the results over time effectively, you can create dashboards that display test metrics and allow for visual monitoring. \\n\\n### Step 1: Configure Dashboard Panels\\nYou can add various panels to visually represent your results. Here’s how to create a couple of useful panels: \\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n### Step 2: Visualize the Results\\nNavigate to the UI of Evidently Cloud to see your newly created dashboard which summarizes and visualizes your test results effectively.\n",
      "\n",
      "-- Conclusion and Next Steps --\n",
      "\n",
      "In conclusion, this tutorial guided you through the key steps for conducting regression testing on outputs generated by language models. You learned how to create datasets, generate responses, set up a testing framework, implement evaluation metrics, and create monitoring dashboards.\\n\\n### Moving Forward\\nAs you build out your regression testing, consider automating tests to run during your CI/CD workflows to ensure every change is validated adequately. Additionally, setting up alerts for significant failures can help maintain high-quality outputs from your models as they evolve.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{result.title}')\n",
    "print()\n",
    "\n",
    "for section in result.sections:\n",
    "    print(f'-- {section.title} --')\n",
    "    print()\n",
    "    print(section.markdown)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5ddac-f4f0-4ee7-9c6f-4da35964151d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99bd0d-d272-4c2e-8a8d-8ec4b7ccc9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
