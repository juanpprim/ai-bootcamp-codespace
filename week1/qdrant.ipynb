{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13189b45-2944-47d1-ba20-0b8942401a67",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ae64c0-df56-444f-950e-7915fa4f2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the boilerplate code we have in docs.ipynb is now also in the docs.py file\n",
    "\n",
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "chunks = docs.chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caaf7488-973d-492c-a2bc-874430c32acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "qd_client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099b138f-19e8-4610-a13e-0dd378163aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensionality = 512\n",
    "model_name = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "collection_name = \"evidently-docs\"\n",
    "\n",
    "# qd_client.delete_collection(collection_name=collection_name)\n",
    "\n",
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=dimensionality,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbdc1605-46ad-4c5a-b405-ada825b34681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'content': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       " 'title': 'Introduction',\n",
       " 'description': 'Example section for showcasing API endpoints',\n",
       " 'filename': 'api-reference/introduction.mdx'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5ef98d-6e00-48b9-b729-9b41dac0ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "\n",
    "for i, doc in enumerate(chunks):\n",
    "    text = doc.get('title', '') + ' ' + doc.get('description', '') + ' ' + doc['content']\n",
    "    text = text.strip()\n",
    "    vector = models.Document(text=text, model=model_name)\n",
    "    point = models.PointStruct(\n",
    "        id=i,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d8a0ddc-b346-40a3-acf9-3f5f57251716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc238923-70d0-40e3-a202-2d2233c7eb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb6e6c2b0884e8da3cbd89efa5d961c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f4f862641746128a741b5c9912f2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1a919f95f446a6b15c25f9d5c4a5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2cbb78fa4944fa84ee5a0268181b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90095d1285654cddb341a57ad484bdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5524ba25a103451ab759529709a76f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969fbce-2590-46c4-8a32-420adb2d75cf",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3790dee2-6349-4e15-af3d-ffc6da3f1b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "qd_client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "model_name = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "collection_name = \"evidently-docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaeb6da9-2fd1-42a5-81ec-9c3217e5a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, num_results=15):    \n",
    "    vector = models.Document(text=query, model=model_name)\n",
    "\n",
    "    query_points = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=vector,\n",
    "        limit=num_results,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for point in query_points.points:\n",
    "        results.append(point.payload)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0220bb3-da2c-429c-adaa-b40312f8b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = vector_search('how do I use llm-as-a-judge for evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199017bd-3248-4565-9c81-0647f8f47f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 20000,\n",
       " 'content': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).',\n",
       " 'title': 'LLM as a judge',\n",
       " 'description': 'How to create and evaluate an LLM judge.',\n",
       " 'filename': 'examples/LLM_judge.mdx'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eec6ce1-cc00-4dc3-b76e-fb63677db9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(question):\n",
    "    search_results = vector_search(question)\n",
    "    ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997fc80-fbdb-4b88-af20-591fb0df286e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
