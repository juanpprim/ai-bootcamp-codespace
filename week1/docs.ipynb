{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5bf726-968a-42ff-bd59-e5623d8b801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de20fd57-a793-4224-9730-f23d3a3cbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_github_data():\n",
    "    repo_owner = 'evidentlyai'\n",
    "    repo_name = 'docs'\n",
    "    \n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions,\n",
    "    )\n",
    "    \n",
    "    return reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbb2e3f-4eae-462e-9ad0-c35b09e763cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m183 packages\u001b[0m \u001b[2min 0.66ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m177 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add python-frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddddc36d-0480-4dd3-9b84-ede5a05476aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = read_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c0f1193-f206-4728-b89b-3bca1307a4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: 'Introduction'\n",
      "description: 'Example section for showcasing API endpoints'\n",
      "---\n",
      "\n",
      "<Note>\n",
      "  If you're not looking to build API reference documentation, you can delete\n",
      "  this section by removing the api-reference folder.\n",
      "</Note>\n",
      "\n",
      "## Welcome\n",
      "\n",
      "There are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\n",
      "\n",
      "<Card\n",
      "  title=\"Plant Store Endpoints\"\n",
      "  icon=\"leaf\"\n",
      "  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\n",
      ">\n",
      "  View the OpenAPI specification file\n",
      "</Card>\n",
      "\n",
      "## Authentication\n",
      "\n",
      "All API endpoints are authenticated using Bearer tokens and picked up from the specification file.\n",
      "\n",
      "```json\n",
      "\"security\": [\n",
      "  {\n",
      "    \"bearerAuth\": []\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(github_data[3].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30e142e-39a3-4b1b-84da-de5337d52a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "\n",
    "def parse_data(data_raw):\n",
    "    data_parsed = []\n",
    "    for f in data_raw:\n",
    "        post = frontmatter.loads(f.content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = f.filename\n",
    "        data_parsed.append(data)\n",
    "\n",
    "    return data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13f7fe4-c596-45b3-99d4-fff4e1eceed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = parse_data(github_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0181d85d-e8bf-49c0-ba17-7b47057ea60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view or export Reports in multiple formats.\n",
      "\n",
      "**Pre-requisites**:\n",
      "\n",
      "* You know how to [generate Reports](/docs/library/report).\n",
      "\n",
      "## Log to Workspace\n",
      "\n",
      "You can save the computed Report in Evidently Cloud or your local workspace.\n",
      "\n",
      "```python\n",
      "ws.add_run(project.id, my_eval, include_data=False)\n",
      "```\n",
      "\n",
      "<Info>\n",
      "  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\n",
      "</Info>\n",
      "\n",
      "## View in Jupyter notebook\n",
      "\n",
      "You can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\n",
      "\n",
      "After running the Report, simply call the resulting Python object:\n",
      "\n",
      "```python\n",
      "my_report\n",
      "```\n",
      "\n",
      "This will render the HTML object directly in the notebook cell.\n",
      "\n",
      "## HTML\n",
      "\n",
      "You can also save this interactive visual Report as an HTML file to open in a browser:\n",
      "\n",
      "```python\n",
      "my_report.save_html(“file.html”)\n",
      "```\n",
      "\n",
      "This option is useful for sharing Reports with others or if you're working in a Python environment that doesn’t display interactive visuals.\n",
      "\n",
      "## JSON\n",
      "\n",
      "You can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\n",
      "\n",
      "To view the JSON in Python:\n",
      "\n",
      "```python\n",
      "my_report.json()\n",
      "```\n",
      "\n",
      "To save the JSON as a separate file:\n",
      "\n",
      "```python\n",
      "my_report.save_json(\"file.json\")\n",
      "```\n",
      "\n",
      "## Python dictionary\n",
      "\n",
      "You can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\n",
      "\n",
      "To get the dictionary:\n",
      "\n",
      "```python\n",
      "my_report.dict()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parsed_data[10]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81ed32b-4ab1-4508-a778-3577156efec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk in the original sequence\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If size or step are not positive integers.\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content'\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split a collection of documents into smaller chunks using sliding windows.\n",
    "\n",
    "    Takes documents and breaks their content into overlapping chunks while preserving\n",
    "    all other document metadata (filename, etc.) in each chunk.\n",
    "\n",
    "    Args:\n",
    "        documents: An iterable of document dictionaries. Each document must have a content field.\n",
    "        size (int, optional): The maximum size of each chunk. Defaults to 2000.\n",
    "        step (int, optional): The step size between chunks. Defaults to 1000.\n",
    "        content_field_name (str, optional): The name of the field containing document content.\n",
    "                                          Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunk dictionaries. Each chunk contains:\n",
    "            - All original document fields except the content field\n",
    "            - 'start': Starting position of the chunk in original content\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Example:\n",
    "        >>> documents = [{'content': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, size=100, step=50)\n",
    "        >>> # Or with custom content field:\n",
    "        >>> documents = [{'text': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, content_field_name='text')\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name)\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20eafaad-e57e-4901-a8b4-909d5707c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb7598fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'content': 'This page shows the core eval workflow with the Evidently library and links to guides.\\n\\n## Define and run the eval\\n\\n<Tip>\\n  To log the evaluation results to the Evidently Platform, first connect to [Evidently Cloud](/docs/setup/cloud) or your [local workspace](/docs/setup/self-hosting) and [create a Project](/docs/platform/projects_manage). It\\'s optional: you can also run evals locally.\\n</Tip>\\n\\n<Steps>\\n  <Step title=\"Prepare the input data\">\\n    Get your data in a table like a `pandas.DataFrame`. More on [data requirements](/docs/library/overview#dataset). You can also [load data](/docs/platform/datasets_workflow) from Evidently Platform, like tracing or synthetic datasets.\\n  </Step>\\n\\n  <Step title=\"Create a Dataset object\">\\n    Create a Dataset object with `DataDefinition()` that specifies column role and types. You can also use default type detection. [How to set Data Definition](/docs/library/data_definition).\\n\\n    ```python\\n    eval_data = Dataset.from_pandas(\\n        source_df,\\n        data_definition=DataDefinition()\\n    )\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add descriptors\">\\n    For text evals, choose and compute row-level `descriptors`. Optionally, add row-level tests to get pass/fail for specific inputs. [How to use Descriptors](/docs/library/descriptors).\\n\\n    ```python\\n    eval_data.add_descriptors(descriptors=[\\n        TextLength(\"Question\", alias=\"Length\"),\\n        Sentiment(\"Answer\", alias=\"Sentiment\")\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"Configure Report\">\\n    For dataset-level evals (classification, data drift) or to summarize descriptors, create a `Report` with chosen `metrics`  or `presets`. How to [configure Reports](/docs/library/report).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset()\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Test conditions\">\\n    Add dataset-level Pass/Fail conditions, like to check if all texts are in \\\\< 100 symbols length. How to [configure Tests](/docs/library/tests).\\n\\n    ```pytho',\n",
       " 'title': 'Overview',\n",
       " 'description': 'End-to-end evaluation workflow.',\n",
       " 'filename': 'docs/library/evaluations_overview.mdx'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e66732cd-41ce-439c-9216-e22f175ab1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 1000,\n",
       " 'content': '      data_definition=DataDefinition()\\n    )\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add descriptors\">\\n    For text evals, choose and compute row-level `descriptors`. Optionally, add row-level tests to get pass/fail for specific inputs. [How to use Descriptors](/docs/library/descriptors).\\n\\n    ```python\\n    eval_data.add_descriptors(descriptors=[\\n        TextLength(\"Question\", alias=\"Length\"),\\n        Sentiment(\"Answer\", alias=\"Sentiment\")\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"Configure Report\">\\n    For dataset-level evals (classification, data drift) or to summarize descriptors, create a `Report` with chosen `metrics`  or `presets`. How to [configure Reports](/docs/library/report).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset()\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Test conditions\">\\n    Add dataset-level Pass/Fail conditions, like to check if all texts are in \\\\< 100 symbols length. How to [configure Tests](/docs/library/tests).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset(),\\n        MaxValue(column=\"Length\", tests=[lt(100)]),\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Tags and Timestamps\">\\n    Add `tags` or `metadata` to identify specific evaluation runs or datasets, or override the default `timestamp `. [How to add metadata](/docs/library/tags_metadata).\\n  </Step>\\n\\n  <Step title=\"Run the Report\">\\n    To execute the eval, `run`the Report on the `Dataset` (or two).\\n\\n    ```python\\n    my_eval = report.run(eval_data, None)\\n    ```\\n  </Step>\\n\\n  <Step title=\"Explore the results\">\\n    * To upload to the Evidently Platform. [How to upload results](/docs/platform/evals_api).\\n\\n    ```python\\n    ws.add_run(project.id, my_eval, include_data=True)\\n    ```\\n\\n    * To view locally. [All output formats](/docs/library/output_formats).\\n\\n    ```python\\n    my_eval\\n    ##my_eval.json()\\n    ```\\n  </Step>\\n</Steps>\\n\\n## Quickstarts\\n\\nCheck for end-to-end examples:\\n\\n<CardGroup cols={2}>\\n  <Card title=\"LLM quickstart\" ',\n",
       " 'title': 'Overview',\n",
       " 'description': 'End-to-end evaluation workflow.',\n",
       " 'filename': 'docs/library/evaluations_overview.mdx'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70048471-e8e2-40df-b748-416f81b52e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29440a9c-3d88-4048-8dc4-4f833472e0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7facba2ca960>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f436111-9193-4b22-a23b-fbc96335ef13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ',\n",
       " 'title': 'LLM as a judge',\n",
       " 'description': 'How to create and evaluate an LLM judge.',\n",
       " 'filename': 'examples/LLM_judge.mdx'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = index.search('how do I use llm-as-a-judge for evals')\n",
    "print(len(search_results))\n",
    "search_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b148021e-8858-4ab1-81c9-2ae723981cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a409f97-7b3b-4628-a5eb-9b48549afaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how do I use llm-as-a-judge for evals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "200844a5-6435-4778-811c-946c32a9b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88cf5d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"start\": 0, \"content\": \"import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\\\n\\\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\\\n\\\\n<Info>\\\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we\\\\u2019ll optionally show how to upload results to the Evidently Platform for easy exploration.\\\\n</Info>\\\\n\\\\nWe\\'ll explore two ways to use an LLM as a judge:\\\\n\\\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \\\\\"ground truth\\\\\" (approved responses) to compare against.\\\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\\\n\\\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\\\n\\\\n<Info>\\\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\\\n</Info>\\\\n\\\\n## Tutorial scope\\\\n\\\\nHere\\'s what we\\'ll do:\\\\n\\\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\\\n\\\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\\\n\\\\nTo complete the tutorial, you will need:\\\\n\\\\n- Basic Python knowledge.\\\\n- An OpenAI API key to use for the LLM evaluator.\\\\n\\\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\\\n\\\\n<Info>\\\\n  Run a sample notebook: [Jupyter \", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 1000, \"content\": \"n.\\\\n\\\\n<Info>\\\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\\\n</Info>\\\\n\\\\n## Tutorial scope\\\\n\\\\nHere\\'s what we\\'ll do:\\\\n\\\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\\\n\\\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\\\n\\\\nTo complete the tutorial, you will need:\\\\n\\\\n- Basic Python knowledge.\\\\n- An OpenAI API key to use for the LLM evaluator.\\\\n\\\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\\\n\\\\n<Info>\\\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\\\n</Info>\\\\n\\\\n## 1.  Installation and Imports\\\\n\\\\nInstall Evidently:\\\\n\\\\n```python\\\\npip install evidently\\\\n```\\\\n\\\\nImport the required modules:\\\\n\\\\n```python\\\\nimport pandas as pd\\\\nimport numpy as np\\\\n\\\\nfrom evidently import Dataset\\\\nfrom evidently import DataDefinition\\\\nfrom evidently import Report\\\\nfrom evidently import BinaryClassification\\\\nfrom evidently.descriptors import *\\\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\\\nfrom evidently.metrics import *\\\\n\\\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\\\n```\\\\n\\\\nPass your OpenAI key as an environment variable:\\\\n\\\\n```python\\\\nimport os\\\\nos.environ[\\\\\"OPENAI_API_KEY\\\\\"] = \\\\\"YOUR_KEY\\\\\"\\\\n```\\\\n\\\\n<Info>\\\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 2000, \"content\": \"notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\\\n</Info>\\\\n\\\\n## 1.  Installation and Imports\\\\n\\\\nInstall Evidently:\\\\n\\\\n```python\\\\npip install evidently\\\\n```\\\\n\\\\nImport the required modules:\\\\n\\\\n```python\\\\nimport pandas as pd\\\\nimport numpy as np\\\\n\\\\nfrom evidently import Dataset\\\\nfrom evidently import DataDefinition\\\\nfrom evidently import Report\\\\nfrom evidently import BinaryClassification\\\\nfrom evidently.descriptors import *\\\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\\\nfrom evidently.metrics import *\\\\n\\\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\\\n```\\\\n\\\\nPass your OpenAI key as an environment variable:\\\\n\\\\n```python\\\\nimport os\\\\nos.environ[\\\\\"OPENAI_API_KEY\\\\\"] = \\\\\"YOUR_KEY\\\\\"\\\\n```\\\\n\\\\n<Info>\\\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\\\n</Info>\\\\n\\\\n## 2.  Create the Dataset\\\\n\\\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\\\n\\\\n- **Questions**. The inputs sent to the LLM app.\\\\n- **Target responses**. The approved responses you consider accurate.\\\\n- **New responses**. Imitated new responses from the system.\\\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\\\n\\\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\\\n\\\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\\\n- Get the \\\\\"ground truth\\\\\". You can use it to evaluate the quality of the LLM judge.\\\\n\\\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\\\\\!\\\\n\\\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\\\n\\\\n<Accordion title=\\\\\"Toy data to run the example\\\\\" defaultOpen={false}>\\\\n  ```python\\\\n  \", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 20000, \"content\": \"openai\\\\\",\\\\n            model = \\\\\"gpt-4o-mini\\\\\",\\\\n            alias=\\\\\"Verbosity\\\\\")\\\\n    ])\\\\n```\\\\n\\\\nRun the Report and view the summary results:\\\\u00a0\\\\n\\\\n```python\\\\nreport = Report([\\\\n    TextEvals()\\\\n])\\\\n\\\\nmy_eval = report.run(eval_dataset, None)\\\\nmy_eval\\\\n```\\\\n\\\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\\\n\\\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\\\n\\\\n<Info>\\\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\\\n</Info>\\\\n\\\\n## What\\'s next?\\\\n\\\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\\\n\\\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\\\n\\\\n### Set up Evidently Cloud\\\\n\\\\n<CloudSignup />\\\\n\\\\nImport the components to connect with Evidently Cloud:\\\\n\\\\n```python\\\\nfrom evidently.ui.workspace import CloudWorkspace\\\\n```\\\\n\\\\n### Create a Project\\\\n\\\\n<CreateProject />\\\\n\\\\n### Send your eval\\\\n\\\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\\\n\\\\n```python\\\\nws.add_run(project.id, my_eval, include_data=True)\\\\n```\\\\n\\\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\\\n\\\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\\\n\\\\n<Info>\\\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\\\n</Info>\\\\n\\\\n# Reference documentation\\\\n\\\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 3000, \"content\": \"s/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\\\n</Info>\\\\n\\\\n## 2.  Create the Dataset\\\\n\\\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\\\n\\\\n- **Questions**. The inputs sent to the LLM app.\\\\n- **Target responses**. The approved responses you consider accurate.\\\\n- **New responses**. Imitated new responses from the system.\\\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\\\n\\\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\\\n\\\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\\\n- Get the \\\\\"ground truth\\\\\". You can use it to evaluate the quality of the LLM judge.\\\\n\\\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\\\\\!\\\\n\\\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\\\n\\\\n<Accordion title=\\\\\"Toy data to run the example\\\\\" defaultOpen={false}>\\\\n  ```python\\\\n  data = [\\\\n    [\\\\\"Hi there, how do I reset my password?\\\\\",\\\\n     \\\\\"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\\\\\",\\\\n     \\\\\"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\\\\\",\\\\n     \\\\\"incorrect\\\\\", \\\\\"adds new information (contact support)\\\\\"],\\\\n  \\\\n    [\\\\\"Where can I find my transaction history?\\\\\",\\\\n     \\\\\"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\\\\\",\\\\n     \\\\\"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quic\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 19000, \"content\": \"w to set up the prompt template for verbosity:\\\\n\\\\n```python\\\\nverbosity = BinaryClassificationPromptTemplate(\\\\n        criteria = \\\\\"\\\\\"\\\\\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\\\n            A concise response should:\\\\n            - Provide the necessary information without unnecessary details or repetition.\\\\n            - Be brief yet comprehensive enough to address the query.\\\\n            - Use simple and direct language to convey the message effectively.\\\\\"\\\\\"\\\\\",\\\\n        target_category=\\\\\"concise\\\\\",\\\\n        non_target_category=\\\\\"verbose\\\\\",\\\\n        uncertainty=\\\\\"unknown\\\\\",\\\\n        include_reasoning=True,\\\\n        pre_messages=[(\\\\\"system\\\\\", \\\\\"You are an expert text evaluator. You will be given a text of the response to a user question.\\\\\")],\\\\n        )\\\\n```\\\\n\\\\nAdd this new descriptor to our existing dataset:\\\\n\\\\n```python\\\\neval_dataset.add_descriptors(descriptors=[\\\\n    LLMEval(\\\\\"new_response\\\\\",\\\\n            template=verbosity,\\\\n            provider = \\\\\"openai\\\\\",\\\\n            model = \\\\\"gpt-4o-mini\\\\\",\\\\n            alias=\\\\\"Verbosity\\\\\")\\\\n    ])\\\\n```\\\\n\\\\nRun the Report and view the summary results:\\\\u00a0\\\\n\\\\n```python\\\\nreport = Report([\\\\n    TextEvals()\\\\n])\\\\n\\\\nmy_eval = report.run(eval_dataset, None)\\\\nmy_eval\\\\n```\\\\n\\\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\\\n\\\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\\\n\\\\n<Info>\\\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\\\n</Info>\\\\n\\\\n## What\\'s next?\\\\n\\\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\\\n\\\\nTo be able to easily run and compare evals, systematically tr\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 17000, \"content\": \"\\\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \\\\\"incorrect\\\\\" answers .\\\\n\\\\n## 4. Evaluate the LLM Eval quality\\\\n\\\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\\\\\! We can treat it as a simple **binary classification** problem.\\\\n\\\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \\\\\"target\\\\\", and the LLM-judge response is the \\\\\"prediction\\\\\":\\\\n\\\\n```python\\\\ndf=eval_dataset.as_dataframe()\\\\n\\\\ndefinition_2 = DataDefinition(\\\\n    classification=[BinaryClassification(\\\\n        target=\\\\\"label\\\\\",\\\\n        prediction_labels=\\\\\"Correctness\\\\\",\\\\n        pos_label = \\\\\"incorrect\\\\\")],\\\\n    categorical_columns=[\\\\\"label\\\\\", \\\\\"Correctness\\\\\"])\\\\n\\\\nclass_dataset = Dataset.from_pandas(\\\\n    pd.DataFrame(df),\\\\n    data_definition=definition_2)\\\\n```\\\\n\\\\n<Info>\\\\n  `Pos_label` refers to the class that is treated as the target (\\\\\"what we want to predict better\\\\\") for metrics like precision, recall, F1-score.\\\\n</Info>\\\\n\\\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\\\n\\\\n```python\\\\nreport = Report([\\\\n    ClassificationPreset()\\\\n])\\\\n\\\\nmy_eval = report.run(class_dataset, None)\\\\nmy_eval\\\\n\\\\n# or my_eval.as_dict()\\\\n```\\\\n\\\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\\\\\! You can also refine the prompt to try to improve them.\\\\n\\\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\\\n\\\\n## 5. Verbosity evaluator\\\\n\\\\nNext, let\\\\u2019s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don\\\\u2019t have a reference answer.\\\\n\\\\nHere\\'s ho\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 16000, \"content\": \"view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\\\n\\\\n```python\\\\neval_dataset.as_dataframe()\\\\n```\\\\n\\\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\\\n\\\\n<Info>\\\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\\\n</Info>\\\\n\\\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\\\n\\\\n```python\\\\neval_dataset.add_descriptors(descriptors=[\\\\n    ExactMatch(columns=[\\\\\"label\\\\\", \\\\\"Correctness\\\\\"], alias=\\\\\"Judge_match\\\\\")])\\\\n```\\\\n\\\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\\\n\\\\n```python\\\\nreport = Report([\\\\n    TextEvals()\\\\n])\\\\n\\\\nmy_eval = report.run(eval_dataset, None)\\\\nmy_eval\\\\n```\\\\n\\\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\\\n\\\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\\\n\\\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \\\\\"incorrect\\\\\" answers .\\\\n\\\\n## 4. Evaluate the LLM Eval quality\\\\n\\\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\\\\\! We can treat it as a simple **binary classification** problem.\\\\n\\\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \\\\\"target\\\\\", and the LLM-judge response is the \\\\\"prediction\\\\\":\\\\n\\\\n```python\\\\ndf=eval_dataset.as_dataframe()\\\\n\\\\ndefinition_2 = DataDefinition(\\\\n    classification=[BinaryClassification(\\\\n        target=\\\\\"label\\\\\",\\\\n        prediction_labels=\\\\\"Correctness\\\\\",\\\\n        pos_label = \\\\\"incorrect\\\\\")],\\\\n    categorical_columns=[\\\\\"label\\\\\", \\\\\"Correctness\\\\\"])\\\\n\\\\nclass_dataset = Dataset.from_pandas(\\\\n    pd.DataFra\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 14000, \"content\": \"ow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\\\n\\\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\\\n\\\\n```python\\\\ncorrectness = BinaryClassificationPromptTemplate(\\\\n        criteria = \\\\\"\\\\\"\\\\\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\\\n        REFERENCE:\\\\n        =====\\\\n        {target_response}\\\\n        =====\\\\\"\\\\\"\\\\\",\\\\n        target_category=\\\\\"incorrect\\\\\",\\\\n        non_target_category=\\\\\"correct\\\\\",\\\\n        uncertainty=\\\\\"unknown\\\\\",\\\\n        include_reasoning=True,\\\\n        pre_messages=[(\\\\\"system\\\\\", \\\\\"You are an expert evaluator. You will be given an ANSWER and REFERENCE\\\\\")],\\\\n        )\\\\n```\\\\n\\\\n<Info>\\\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure \\\\u2014 that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\\\n</Info>\\\\n\\\\nIn this example, we\\'ve set up the prompt to be strict (\\\\\"all fact and details\\\\\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\\\n\\\\n**Score your data**. To add this new descriptor to your dataset, run:\\\\n\\\\n```python\\\\neval_dataset.add_descriptors(descriptors=[\\\\n    LLMEval(\\\\\"new_response\\\\\",\\\\n            template=correctness,\\\\n            provider = \\\\\"openai\\\\\",\\\\n            model = \\\\\"gpt-4o-mini\\\\\",\\\\n            alias=\\\\\"Correctness\\\\\",\\\\n            additional_columns={\\\\\"target_response\\\\\": \\\\\"target_response\\\\\"}),\\\\n    ])\\\\n```\\\\n\\\\n**Preview the results**. You can \", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 13000, \"content\": \"eate an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\\\n\\\\n```python\\\\ndefinition = DataDefinition(\\\\n    text_columns=[\\\\\"question\\\\\", \\\\\"target_response\\\\\", \\\\\"new_response\\\\\"],\\\\n    categorical_columns=[\\\\\"label\\\\\"]\\\\n    )\\\\n\\\\neval_dataset = Dataset.from_pandas(\\\\n    golden_dataset,\\\\n    data_definition=definition)\\\\n```\\\\n\\\\nTo preview the dataset:\\\\n\\\\n```python\\\\npd.set_option(\\'display.max_colwidth\\', None)\\\\ngolden_dataset.head(5)\\\\n```\\\\n\\\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\\\n\\\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\\\n\\\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\\\n\\\\n<Accordion title=\\\\\"How to preview\\\\\" defaultOpen={false}>\\\\n  Run this to preview the distribution of the column.\\\\n\\\\n  ```python\\\\n  report = Report([\\\\n    ValueStats(column=\\\\\"label\\\\\")\\\\n  ])\\\\n  \\\\n  my_eval = report.run(eval_dataset, None)\\\\n  my_eval\\\\n  \\\\n  # my_eval.dict()\\\\n  # my_eval.json()\\\\n  ```\\\\n</Accordion>\\\\n\\\\n## 3. Correctness evaluator\\\\n\\\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\\\n\\\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\\\n\\\\n```python\\\\ncorrectness = BinaryClassificationPromptTemplate(\\\\n        criteria = \\\\\"\\\\\"\\\\\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\\\n        REFERENCE:\\\\n        =====\\\\n        {target_response}\\\\n        =====\\\\\"\\\\\"\\\\\",\\\\n        target_category=\\\\\"incorrect\\\\\",\\\\n        non_target_category=\\\\\"correct\\\\\",\\\\n        uncertainty=\\\\\"unknown\\\\\",\\\\n        include_reasoning=True,\\\\n        pre_messages=[(\\\\\"system\\\\\", \\\\\"You are an expert evaluator. You will be given an ANSWER and\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}]'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(json.dumps(search_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4285258-d56a-4023-bd7a-811bf6801686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b301942-5338-4c8e-9999-7d65d4075771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f22a08c-d5d1-4865-868b-207752d6c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7726bab7-8d8b-4088-b13f-d2f5ec8e7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag('How can I build an eval report with llm as a judge?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c16691e-058f-4e3f-a889-dec2d312c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an evaluation report using a large language model (LLM) as a judge can be effectively accomplished through the following steps:\n",
      "\n",
      "### 1. **Setup and Installation**\n",
      "Ensure you have the required libraries installed. You will need `Evidently`, which you can install via pip:\n",
      "```bash\n",
      "pip install evidently\n",
      "```\n",
      "\n",
      "### 2. **Import Necessary Libraries**\n",
      "Start by importing the relevant modules in your Python script or Jupyter Notebook:\n",
      "```python\n",
      "import os\n",
      "import pandas as pd\n",
      "from evidently import Dataset, DataDefinition, Report\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "from evidently.presets import TextEvals\n",
      "```\n",
      "\n",
      "### 3. **Set Up OpenAI API Key**\n",
      "Set your OpenAI API key to enable LLM usage:\n",
      "```python\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "```\n",
      "\n",
      "### 4. **Create Your Evaluation Dataset**\n",
      "Develop a dataset that includes:\n",
      "- **Questions**: Inputs for the LLM.\n",
      "- **Target Responses**: Approved responses you deem accurate.\n",
      "- **New Responses**: AI-generated or imitated responses.\n",
      "- **Manual Labels**: Indicate whether the response is correct and provide explanations.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "data = [\n",
      "    [\"Question 1?\", \"Correct response 1.\", \"New response 1.\", \"incorrect\", \"additional detail\"],\n",
      "    [\"Question 2?\", \"Correct response 2.\", \"New response 2.\", \"correct\", \"matches reference\"]\n",
      "]\n",
      "df = pd.DataFrame(data, columns=[\"Question\", \"Target Response\", \"New Response\", \"Label\", \"Explanation\"])\n",
      "```\n",
      "\n",
      "### 5. **Map Data Types**\n",
      "Define the data types for your dataset:\n",
      "```python\n",
      "definition = DataDefinition(\n",
      "    text_columns=[\"Question\", \"Target Response\", \"New Response\"],\n",
      "    categorical_columns=[\"Label\"]\n",
      ")\n",
      "\n",
      "eval_dataset = Dataset.from_pandas(df, data_definition=definition)\n",
      "```\n",
      "\n",
      "### 6. **Configure LLM Evaluator Prompts**\n",
      "Setup the LLM evaluator prompts, specifying what you're judging:\n",
      "```python\n",
      "correctness_prompt = BinaryClassificationPromptTemplate(\n",
      "    criteria=\"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\n",
      "    REFERENCE: {target_response}\"\"\",\n",
      "    target_category=\"incorrect\",\n",
      "    non_target_category=\"correct\",\n",
      "    uncertainty=\"unknown\",\n",
      "    include_reasoning=True\n",
      ")\n",
      "```\n",
      "\n",
      "### 7. **Score Your Data with LLM**\n",
      "Add the evaluator to your dataset:\n",
      "```python\n",
      "eval_dataset.add_descriptors(descriptors=[\n",
      "    LLMEval(\"New Response\", template=correctness_prompt, provider=\"openai\", model=\"gpt-4o-mini\", alias=\"Correctness\")\n",
      "])\n",
      "```\n",
      "\n",
      "### 8. **Generate the Report**\n",
      "Create and run the report to evaluate the outcomes:\n",
      "```python\n",
      "report = Report([TextEvals()])\n",
      "my_eval = report.run(eval_dataset, None)\n",
      "```\n",
      "\n",
      "### 9. **View Results**\n",
      "You can view the evaluated dataset and report results:\n",
      "```python\n",
      "print(eval_dataset.as_dataframe())\n",
      "print(my_eval)  # or my_eval.as_dict() for JSON format\n",
      "```\n",
      "\n",
      "### 10. **Iterate and Improve**\n",
      "If desired, refine your evaluation prompts based on the results. You can try different LLMs or modify your prompts to enhance accuracy.\n",
      "\n",
      "### 11. **Export and Share Results**\n",
      "You can export the report to various formats (JSON, HTML, etc.) as needed for sharing and further analysis.\n",
      "\n",
      "### Conclusion\n",
      "This setup will provide a comprehensive evaluation of your responses, leveraging the LLM for nuanced judgment and reporting, helping to ensure quality control in your workflows.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
