{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5bf726-968a-42ff-bd59-e5623d8b801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de20fd57-a793-4224-9730-f23d3a3cbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_github_data():\n",
    "    repo_owner = 'evidentlyai'\n",
    "    repo_name = 'docs'\n",
    "    \n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions,\n",
    "    )\n",
    "    \n",
    "    return reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbb2e3f-4eae-462e-9ad0-c35b09e763cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m152 packages\u001b[0m \u001b[2min 0.67ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m147 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add python-frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddddc36d-0480-4dd3-9b84-ede5a05476aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = read_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0f1193-f206-4728-b89b-3bca1307a4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Evidently and GitHub actions\"\n",
      "description: \"Testing LLM outputs as part of the CI/CD flow.\"\n",
      "---\n",
      "\n",
      "You can use Evidently together with GitHub Actions to automatically test the outputs of your LLM agent or application - as part of every code push or pull request.\n",
      "\n",
      "## How the integration work:\n",
      "\n",
      "- You define a test dataset of inputs (e.g. test prompts with or without reference answers). You can store it as a file, or save the dataset at Evidently Cloud callable by Dataset ID.\n",
      "- Run your LLM system or agent against those inputs inside CI.\n",
      "- Evidently automatically evaluates the outputs using the user-specified config (which defines the Evidently descriptors, tests and Report composition), including methods like:\n",
      "  - LLM judges (e.g., tone, helpfulness, correctness)\n",
      "  - Custom Python functions\n",
      "  - Dataset-level metrics like classification quality\n",
      "- If any test fails, the CI job fails.\n",
      "- You get a detailed test report with pass/fail status and metrics.\n",
      "\n",
      "![](/images/examples/github_actions.gif)\n",
      "\n",
      "Results are stored locally or pushed to Evidently Cloud for deeper review and tracking.\n",
      "\n",
      "The final result is CI-native testing for your LLM behavior - so you can safely tweak prompts, models, or logic without breaking things silently.\n",
      "\n",
      "## Code example and tutorial\n",
      "\n",
      "ðŸ‘‰ Check the full tutorial and example repo: https://github.com/evidentlyai/evidently-ci-example\n",
      "\n",
      "Action is also available on GitHub Marketplace: https://github.com/marketplace/actions/run-evidently-report\n"
     ]
    }
   ],
   "source": [
    "print(github_data[40].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30e142e-39a3-4b1b-84da-de5337d52a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "\n",
    "def parse_data(data_raw):\n",
    "    data_parsed = []\n",
    "    for f in data_raw:\n",
    "        post = frontmatter.loads(f.content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = f.filename\n",
    "        data_parsed.append(data)\n",
    "\n",
    "    return data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13f7fe4-c596-45b3-99d4-fff4e1eceed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = parse_data(github_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0181d85d-e8bf-49c0-ba17-7b47057ea60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can view or export Reports in multiple formats.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Log to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>\\n\\n## View in Jupyter notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.\\n\\n## HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(â€œfile.htmlâ€)\\n```\\n\\nThis option is useful for sharing Reports with others or if you\\'re working in a Python environment that doesnâ€™t display interactive visuals.\\n\\n## JSON\\n\\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```\\n\\n## Python dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data[10]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d81ed32b-4ab1-4508-a778-3577156efec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk in the original sequence\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If size or step are not positive integers.\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content'\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split a collection of documents into smaller chunks using sliding windows.\n",
    "\n",
    "    Takes documents and breaks their content into overlapping chunks while preserving\n",
    "    all other document metadata (filename, etc.) in each chunk.\n",
    "\n",
    "    Args:\n",
    "        documents: An iterable of document dictionaries. Each document must have a content field.\n",
    "        size (int, optional): The maximum size of each chunk. Defaults to 2000.\n",
    "        step (int, optional): The step size between chunks. Defaults to 1000.\n",
    "        content_field_name (str, optional): The name of the field containing document content.\n",
    "                                          Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunk dictionaries. Each chunk contains:\n",
    "            - All original document fields except the content field\n",
    "            - 'start': Starting position of the chunk in original content\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Example:\n",
    "        >>> documents = [{'content': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, size=100, step=50)\n",
    "        >>> # Or with custom content field:\n",
    "        >>> documents = [{'text': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, content_field_name='text')\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name)\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20eafaad-e57e-4901-a8b4-909d5707c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e66732cd-41ce-439c-9216-e22f175ab1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 1000,\n",
       " 'content': '      data_definition=DataDefinition()\\n    )\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add descriptors\">\\n    For text evals, choose and compute row-level `descriptors`. Optionally, add row-level tests to get pass/fail for specific inputs. [How to use Descriptors](/docs/library/descriptors).\\n\\n    ```python\\n    eval_data.add_descriptors(descriptors=[\\n        TextLength(\"Question\", alias=\"Length\"),\\n        Sentiment(\"Answer\", alias=\"Sentiment\")\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"Configure Report\">\\n    For dataset-level evals (classification, data drift) or to summarize descriptors, create a `Report` with chosen `metrics`  or `presets`. How to [configure Reports](/docs/library/report).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset()\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Test conditions\">\\n    Add dataset-level Pass/Fail conditions, like to check if all texts are in \\\\< 100 symbols length. How to [configure Tests](/docs/library/tests).\\n\\n    ```python\\n    report = Report([\\n        DataSummaryPreset(),\\n        MaxValue(column=\"Length\", tests=[lt(100)]),\\n    ])\\n    ```\\n  </Step>\\n\\n  <Step title=\"(Optional) Add Tags and Timestamps\">\\n    Add `tags` or `metadata` to identify specific evaluation runs or datasets, or override the default `timestamp `. [How to add metadata](/docs/library/tags_metadata).\\n  </Step>\\n\\n  <Step title=\"Run the Report\">\\n    To execute the eval, `run`the Report on the `Dataset` (or two).\\n\\n    ```python\\n    my_eval = report.run(eval_data, None)\\n    ```\\n  </Step>\\n\\n  <Step title=\"Explore the results\">\\n    * To upload to the Evidently Platform. [How to upload results](/docs/platform/evals_api).\\n\\n    ```python\\n    ws.add_run(project.id, my_eval, include_data=True)\\n    ```\\n\\n    * To view locally. [All output formats](/docs/library/output_formats).\\n\\n    ```python\\n    my_eval\\n    ##my_eval.json()\\n    ```\\n  </Step>\\n</Steps>\\n\\n## Quickstarts\\n\\nCheck for end-to-end examples:\\n\\n<CardGroup cols={2}>\\n  <Card title=\"LLM quickstart\" ',\n",
       " 'title': 'Overview',\n",
       " 'description': 'End-to-end evaluation workflow.',\n",
       " 'filename': 'docs/library/evaluations_overview.mdx'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70048471-e8e2-40df-b748-416f81b52e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29440a9c-3d88-4048-8dc4-4f833472e0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7fc5a3c82090>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f436111-9193-4b22-a23b-fbc96335ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.search('how do I use llm-as-a-judge for evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b148021e-8858-4ab1-81c9-2ae723981cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a409f97-7b3b-4628-a5eb-9b48549afaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how do I use llm-as-a-judge for evals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "200844a5-6435-4778-811c-946c32a9b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4285258-d56a-4023-bd7a-811bf6801686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b301942-5338-4c8e-9999-7d65d4075771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f22a08c-d5d1-4865-868b-207752d6c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7726bab7-8d8b-4088-b13f-d2f5ec8e7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag('How can I build an eval report with llm as a judge?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c16691e-058f-4e3f-a889-dec2d312c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build an evaluation report with a large language model (LLM) as a judge, you can follow these structured steps:\n",
      "\n",
      "### 1. **Install Required Libraries**\n",
      "Make sure to install the necessary libraries, such as `evidently`. You can do this via pip:\n",
      "```bash\n",
      "pip install evidently\n",
      "```\n",
      "\n",
      "### 2. **Import Required Modules**\n",
      "Import the necessary modules from the Evidently library:\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "from evidently import Dataset, DataDefinition, Report\n",
      "from evidently.presets import TextEvals\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "```\n",
      "\n",
      "### 3. **Set Up Your OpenAI API Key**\n",
      "Ensure your OpenAI API key is set as an environment variable:\n",
      "```python\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "```\n",
      "\n",
      "### 4. **Create Your Evaluation Dataset**\n",
      "Prepare a dataset with the following structure:\n",
      "- **Questions**: Inputs sent to the LLM.\n",
      "- **Target Responses**: Approved correct responses.\n",
      "- **New Responses**: Responses generated by the system.\n",
      "- **Manual Labels**: Indicate if responses are correct or incorrect.\n",
      "\n",
      "Here's an example of creating a toy dataset:\n",
      "```python\n",
      "data = [\n",
      "    [\"Question 1\", \"Correct Answer 1\", \"Generated Answer 1\", \"incorrect\"],\n",
      "    [\"Question 2\", \"Correct Answer 2\", \"Generated Answer 2\", \"correct\"],\n",
      "    # Add more rows as needed\n",
      "]\n",
      "\n",
      "columns = [\"question\", \"target_response\", \"new_response\", \"label\"]\n",
      "golden_dataset = pd.DataFrame(data, columns=columns)\n",
      "\n",
      "definition = DataDefinition(\n",
      "    text_columns=[\"question\", \"target_response\", \"new_response\"],\n",
      "    categorical_columns=[\"label\"]\n",
      ")\n",
      "\n",
      "eval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\n",
      "```\n",
      "\n",
      "### 5. **Set Up the LLM Judge (Evaluator)**\n",
      "Define the evaluation criteria:\n",
      "```python\n",
      "correctness_prompt = BinaryClassificationPromptTemplate(\n",
      "    criteria=\"An ANSWER is correct when it is the same as the REFERENCE in all facts.\",\n",
      "    target_category=\"correct\",\n",
      "    non_target_category=\"incorrect\",\n",
      "    include_reasoning=True\n",
      ")\n",
      "\n",
      "eval_dataset.add_descriptors(descriptors=[\n",
      "    LLMEval(\n",
      "        \"new_response\",\n",
      "        template=correctness_prompt,\n",
      "        provider=\"openai\",\n",
      "        model=\"gpt-4o-mini\",\n",
      "        alias=\"Correctness\",\n",
      "        additional_columns={\"target_response\": \"target_response\"}\n",
      "    )\n",
      "])\n",
      "```\n",
      "\n",
      "### 6. **Run the Evaluation Report**\n",
      "Generate the evaluation report:\n",
      "```python\n",
      "report = Report([TextEvals()])\n",
      "my_eval = report.run(eval_dataset, None)\n",
      "```\n",
      "\n",
      "### 7. **View Results**\n",
      "You can preview the scored dataset and evaluation report:\n",
      "```python\n",
      "print(eval_dataset.as_dataframe())\n",
      "print(my_eval)\n",
      "```\n",
      "\n",
      "### 8. **Iterate and Improve**\n",
      "If the results are not satisfactory, refine your LLM prompt or try different models. You can compare feedback from the LLM with the manual labels to make adjustments.\n",
      "\n",
      "### 9. **Export Results (Optional)**\n",
      "You can export results in various formats such as HTML, JSON, etc. Check the Evidently documentation for output formats.\n",
      "\n",
      "### 10. **Next Steps**\n",
      "Explore integration with the Evidently Cloud for easier evaluation tracking and visualization.\n",
      "\n",
      "By following these structured steps, you can effectively create an evaluation report with an LLM as a judge, analyzing responses based on custom criteria.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba06d6-b6aa-4d16-9a9a-a0d4e39976dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e98055-6fbc-4f92-8f66-474867dade4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
