question,summary_answer,difficulty,intent,relevant_lines,filename
create Dataset object in Evidently,You can create a Dataset object by utilizing `Dataset.from_pandas` along with a `DataDefinition` object to specify column mappings.,beginner,code,lines 23-29,docs/library/data_definition.mdx
Column types in DataDefinition,"Column types in DataDefinition can include categorical, numerical, and text types, which help in determining appropriate evaluations and statistics.",beginner,text,lines 3-4,docs/library/data_definition.mdx
manually setting DataDefinition,You can manually set the `DataDefinition` to map the data types accurately for different column roles in your dataset.,intermediate,code,lines 37-37,docs/library/data_definition.mdx
import datasets from pandas,"To import datasets from a pandas DataFrame, you need to include `from evidently import Dataset` in your imports.",beginner,code,lines 10-14,docs/library/data_definition.mdx
automatically map data columns,"Evidently can automatically map data columns using an empty `DataDefinition()`, matching column types and names to their respective roles.",intermediate,code,lines 32-35,docs/library/data_definition.mdx
ID and timestamp in DataDefinition,You can specify ID and timestamp columns in DataDefinition to enhance data mapping and avoid confusion in evaluations.,intermediate,code,lines 115-121,docs/library/data_definition.mdx
mapping text columns for LLM evaluation,"To map text columns for LLM evaluation, define them in the `DataDefinition` specifically under `text_columns` for better processing.",beginner,code,lines 59-61,docs/library/data_definition.mdx
evaluation quality checks in regression,"In regression checks, you must map target and prediction columns to evaluate the quality of the models correctly.",intermediate,code,lines 151-156,docs/library/data_definition.mdx
difference between timestamp and datetime_columns,"Timestamp refers to the role of a column indicating the time of data input, while datetime_columns refers to the data type and can have many instances in a dataset.",advanced,text,lines 130-132,docs/library/data_definition.mdx
best practices for data mapping,Explicit data mapping is recommended to avoid misclassifying column types and to ensure that necessary evaluations can be performed correctly.,intermediate,text,lines 96-99,docs/library/data_definition.mdx
define categorical_columns in DataDefinition,"To define categorical columns in DataDefinition, specify them directly in your mapping to enhance the clarity and accuracy of evaluations.",beginner,code,lines 86-87,docs/library/data_definition.mdx
evaluating text data with descriptors,"The article explains how to use Descriptors as a universal interface for evaluating text data, including LLM inputs and outputs.",beginner,text,1-1,docs/library/descriptors.mdx
how to create custom descriptors,"You can create custom descriptors using LLM prompts or Python, allowing for tailored evaluations of text data.",beginner,code,3-3,docs/library/descriptors.mdx
sample code for generating toy data,The article provides a Python code snippet to create sample data that can be used for testing text evaluations.,beginner,code,9-13,docs/library/descriptors.mdx
adding descriptors to dataset,"It outlines two options for adding descriptors to a Dataset object, either during creation or afterward, with example code for both methods.",intermediate,code,56-70,docs/library/descriptors.mdx
using sentiment descriptor,The article includes examples of incorporating a sentiment descriptor to evaluate the sentiment of text responses.,intermediate,code,66-66,docs/library/descriptors.mdx
optional export results in DataFrame,You can export and preview the results of the evaluations in a DataFrame format using the provided method.,beginner,code,97-101,docs/library/descriptors.mdx
configuring and running reports,"A step-by-step guide is provided for configuring and running reports using the `TextEvals` preset, summarizing descriptor results.",intermediate,code,105-113,docs/library/descriptors.mdx
multi-column descriptors,"It describes how to use multi-column descriptors to compare different columns for evaluations, such as semantic similarity checks.",advanced,text,148-148,docs/library/descriptors.mdx
defining pass/fail checks in descriptors,"You can define pass/fail checks using Descriptor Tests, allowing for more detailed evaluations of text quality against specified conditions.",intermediate,code,172-176,docs/library/descriptors.mdx
creating a custom report with metrics,The article explains how to create a customized report that uses different metrics for a more specific analysis of the text data.,advanced,code,281-287,docs/library/descriptors.mdx
integrating LLM as a judge,"There are built-in descriptors that utilize external LLMs to score text evaluations, requiring API keys for operation.",advanced,code,156-161,docs/library/descriptors.mdx
testing existing column values,You can run tests on existing columns using the `ColumnTest` feature to work with precomputed values or metadata.,intermediate,code,250-256,docs/library/descriptors.mdx
how to log eval results in Evidently,The article explains how to log evaluation results by connecting to Evidently Cloud or a local workspace and creating a project.,beginner,text,lines 5-7,docs/library/evaluations_overview.mdx
create Dataset object with DataDefinition,"The article details the process of creating a Dataset object using the DataDefinition(), which specifies column roles and types, along with an implementation example.",intermediate,code,lines 14-21,docs/library/evaluations_overview.mdx
add descriptors in text evaluation,"The article outlines how to choose and compute row-level descriptors for text evaluations, including a code snippet demonstrating the addition of `TextLength` and `Sentiment` descriptors.",intermediate,code,lines 25-32,docs/library/evaluations_overview.mdx
metric generator examples in evidently,The article provides several examples demonstrating how to use metric generator functions in Evidently to apply metrics like `ValueDrift` across different columns in datasets.,beginner,code,lines 51-84,docs/library/metric_generator.mdx
how to apply metrics to dataset columns evidently,It explains how to use ColumnMetricGenerator to apply metrics to all columns or specific columns using metric_kwargs parameters.,intermediate,text,lines 62-72,docs/library/metric_generator.mdx
export evaluation results in different formats,"The article explains how to export evaluation results in multiple formats, such as HTML, JSON, and Python dictionaries, with relevant code examples provided for each format.",beginner,code,"lines 1, 33-37, 43-54, 57-64",docs/library/output_formats.mdx
evidently python library overview,"The article introduces the Evidently Python library as an open-source tool for evaluating, testing, and monitoring AI systems from experimentation to production.",beginner,text,lines 1-3,docs/library/overview.mdx
how to integrate evidently library,"You can integrate the Evidently library into existing workflows by exporting scores as JSON or DataFrames, or using it with the Evidently Monitoring Platform.",intermediate,code,lines 33-38,docs/library/overview.mdx
evidently library features,"The Evidently library includes features for AI/ML evaluations, synthetic data generation, prompt optimization, and a visualization UI for tracking results.",beginner,text,lines 7-8,docs/library/overview.mdx
json export in evidently library,"Evidently allows you to export evaluation results as JSON or Python dictionaries, making it easy to share and integrate with other systems.",intermediate,code,lines 33-33,docs/library/overview.mdx
generating synthetic data evidently,"Evidently includes functionality for generating synthetic datasets tailored for LLMs, such as RAG-style question-answer pairs.",beginner,text,lines 57-57,docs/library/overview.mdx
evidently evaluation metrics,"The library provides over 100 built-in metrics for evaluating AI and ML systems, covering various tasks including classification, regression, and data quality checks.",intermediate,text,lines 15-15,docs/library/overview.mdx
visual reports with evidently,"You can generate visual reports in Jupyter or export as HTML, which effectively present evaluation results over time.",intermediate,code,lines 35-35,docs/library/overview.mdx
evidently library for testing,"Evidently is designed to run evaluations on AI systems, testing inputs and outputs for various metrics, including data quality checks.",intermediate,code,lines 19-19,docs/library/overview.mdx
building evaluations in evidently,"You can build evaluations using the 'Dataset' objects, which allow for detailed tracking and evaluation of AI outputs.",intermediate,code,lines 166-166,docs/library/overview.mdx
how to install evidently library,"The article does not provide installation steps, but typically you can install Python libraries through pip in your terminal.",beginner,text,lines 1-1,docs/library/overview.mdx
evidently tracking UI,"Evidently includes a minimal UI for tracking and visualizing evaluation results over time, storing multiple evaluations for comparison.",intermediate,text,lines 75-81,docs/library/overview.mdx
speed optimization evidently evaluations,"For large datasets, it's efficient to use samples for evaluations to speed up processing, especially in data drift detection.",advanced,text,lines 180-187,docs/library/overview.mdx
evaluating classification quality evidently,"To evaluate classification quality, use classification logs with predicted and actual labels in a dataset for assessments.",intermediate,code,lines 128-128,docs/library/overview.mdx
evidently test suites,"Test suites in Evidently allow for running multiple conditional tests on datasets, validating results against specific expectations.",advanced,code,lines 285-291,docs/library/overview.mdx
custom descriptors in evidently,"You can create custom descriptors for evaluating LLM outputs, assigning specific quality assessments at the row level.",advanced,code,lines 197-199,docs/library/overview.mdx
data input types evidently,"Evidently can handle various data types, such as numerical, categorical, and text, across multiple datasets for evaluations.",intermediate,text,lines 102-104,docs/library/overview.mdx
integration with airflow evidently,You can integrate Evidently with Airflow to automate evaluation processes and trigger alerts based on test results.,advanced,code,lines 354-354,docs/library/overview.mdx
working with pandas DataFrame in evidently,"Evidently evaluates data by preparing it as a pandas DataFrame, which can include various types of columns for analysis.",beginner,text,lines 104-104,docs/library/overview.mdx
classifying data drift in evidently,Evidently provides tools to identify shifts in data distribution using the DataDrift preset designed for this purpose.,intermediate,code,lines 249-250,docs/library/overview.mdx
visualization capabilities of evidently,"The library enables generating visual reports to summarize evaluation results, which can be presented in user-friendly formats like HTML.",intermediate,code,lines 35-35,docs/library/overview.mdx
evidently report generation overview,"The article introduces how to generate reports using Evidently, emphasizing dataset evaluations and providing links to core concepts and prerequisites.",beginner,text,1-1,docs/library/report.mdx
python code to create report in evidently,"The article includes example Python code for creating a report with various metrics, showcasing how to implement it using the Evidently library.",beginner,code,19-21,docs/library/report.mdx
using metrics in evidently report,"You can customize reports in Evidently by selecting different metrics, with the article detailing how to use both pre-built presets and custom metrics for report generation.",intermediate,code,25-27,docs/library/report.mdx
group by metric in evidently,"The article explains how to use the GroupBy metric in Evidently for calculating metrics separately across categories, demonstrated through a code snippet.",advanced,code,149-161,docs/library/report.mdx
adding tags to reports in Evidently,"The article discusses how to add tags to reports to enhance searching and filtering, including examples of tagging by model version and status.",beginner,code,lines 19-28,docs/library/tags_metadata.mdx
custom metadata in Evidently,"It explains how to pass custom metadata as a Python dictionary in key:value pairs when creating a report, with code examples provided.",intermediate,code,lines 39-48,docs/library/tags_metadata.mdx
how to validate conditions with tests,"The article explains that tests allow users to validate data conditions and receive Pass/Fail results, integrated into the reporting process.",beginner,text,1-1,docs/library/tests.mdx
using report for tests in Python,The article provides import statements and the structure to create a report with tests in Python using the Evidently library.,beginner,code,13-19,docs/library/tests.mdx
difference between test presets and custom tests,"Test presets automatically generate checks, while custom tests allow users to define specific conditions for validating metrics manually.",intermediate,text,24-30,docs/library/tests.mdx
example of using reference datasets for tests,"The article illustrates how to use a reference dataset to validate the current dataset against predefined conditions, including missing values checks.",intermediate,code,53-59,docs/library/tests.mdx
setting custom test conditions,"You can define specific pass/fail conditions for metrics in tests, such as setting minimum values or checking for completeness.",intermediate,code,109-111,docs/library/tests.mdx
how to exclude tests in report,The article details how to exclude specific tests by setting the tests option to None or an empty list in the Report configuration.,intermediate,code,95-105,docs/library/tests.mdx
Test parameter examples with conditions,"Examples of test parameters like `eq()`, `gt()`, and `is_in` are provided, showing how to set conditions for different validation needs.",advanced,code,134-146,docs/library/tests.mdx
using multiple checks for the same metric,"You can add multiple conditions to a single metric in tests to validate against various criteria, enhancing the evaluation robustness.",advanced,code,169-171,docs/library/tests.mdx
setting test criticality in conditions,"The article explains how to control the criticality of test results, allowing users to receive warnings for non-critical issues instead of failures.",advanced,text,222-230,docs/library/tests.mdx
setting up alerts in Evidently,"The article details how to set up alerts in Evidently by configuring notification channels like email or Slack, and defining alert conditions based on failed tests or custom metrics.",beginner,code,lines 7-37,docs/platform/alerts.mdx
add panel to dashboard python,The article explains how to add panels to a dashboard using the Python API by providing relevant code examples and parameters.,beginner,code,lines 43-68,docs/platform/dashboard_add_panels.mdx
delete a tab from dashboard python,Instructions for deleting a tab from the dashboard using the Python API are provided along with the corresponding code.,beginner,code,lines 21-24,docs/platform/dashboard_add_panels.mdx
setup project in evidently cloud,The article mentions that users must connect to Evidently Cloud and create a project before adding panels to the dashboard.,beginner,text,lines 9-10,docs/platform/dashboard_add_panels.mdx
add multiple panels at once,"You can add multiple panels at once in a specified order, as detailed in the article with examples.",intermediate,code,lines 52-52,docs/platform/dashboard_add_panels.mdx
panel options explained,"The article provides a comprehensive summary of all parameters available for customizing panels, such as title, size, and plot types.",intermediate,text,lines 211-222,docs/platform/dashboard_add_panels.mdx
clear dashboard python,"To delete all tabs and panels on the dashboard, the article includes the relevant Python API command for clearing the dashboard.",beginner,code,lines 35-38,docs/platform/dashboard_add_panels.mdx
using text-only panels,Instructions on how to add text-only panels suitable for titles are provided with a code example in the article.,beginner,code,lines 56-70,docs/platform/dashboard_add_panels.mdx
panel metric options,"The article discusses various options for defining metrics in panels, including how to reference values correctly.",intermediate,text,lines 227-320,docs/platform/dashboard_add_panels.mdx
create a new tab in dashboard,"You can create a new tab while adding a panel, and the article explains how this works with examples.",beginner,code,lines 19-19,docs/platform/dashboard_add_panels.mdx
delete a specific panel code,How to delete a specific panel from a dashboard is described along with code snippets.,beginner,code,lines 27-31,docs/platform/dashboard_add_panels.mdx
add counters panel example,The article provides code examples on how to add counter panels that show values with optional text.,intermediate,code,lines 85-122,docs/platform/dashboard_add_panels.mdx
difference between plot types,"The article details different plot types available for panels, such as line and bar charts, including how to implement them.",advanced,text,lines 140-150,docs/platform/dashboard_add_panels.mdx
using metric labels with panels,"How to use metric labels for specific metrics when adding panels is clearly laid out, providing useful code examples.",advanced,text,lines 259-320,docs/platform/dashboard_add_panels.mdx
how to add tabs in dashboards,"The article explains that you can add tabs by entering 'Edit' mode on the Dashboard and clicking the plus sign to add a new tab, providing steps on how to create a custom tab.",beginner,code,13-15,docs/platform/dashboard_add_panels_ui.mdx
types of panels in Evidently dashboards,"It describes various panel types you can add to the dashboard, including text panels, counters, pie charts, and line plots, allowing users to visualize evaluation results.",beginner,text,33-33,docs/platform/dashboard_add_panels_ui.mdx
configuring metrics for dashboard panels,"The article covers how to select and filter metrics for panels, detailing the steps to ensure the selected metrics match those in the Reports for the Project.",intermediate,code,51-53,docs/platform/dashboard_add_panels_ui.mdx
deleting and editing dashboard panels,It provides instructions on how to delete or edit existing panels by entering Edit mode and selecting the action for the specific panel you wish to modify.,intermediate,code,65-67,docs/platform/dashboard_add_panels_ui.mdx
dashboard features in Evidently,"The article explains the various features of the Dashboard, including available tabs, panels, and the importance of Reports as a data source.",beginner,text,lines 1-21,docs/platform/dashboard_overview.mdx
how to add panels in Evidently dashboard,It describes two methods for adding Panels to the Dashboard: using the Python API for coding or directly through the UI in Cloud and Enterprise versions.,intermediate,code,lines 32-34,docs/platform/dashboard_overview.mdx
dataset creation methods in Evidently,"The article outlines several methods to create datasets in Evidently, including uploading CSV files, attaching datasets to reports, generating synthetic data, and creating datasets from tracing.",beginner,code,11-21,docs/platform/datasets_overview.mdx
definition of dataset in Evidently,"A dataset in Evidently is defined as a collection of data from your application used for analysis and automated checks, allowing for both existing and synthetic datasets.",beginner,text,5-7,docs/platform/datasets_overview.mdx
how to upload a dataset in Evidently,"To upload a dataset in Evidently, use the `add_dataset` method after preparing your dataset as an Evidently Dataset with the necessary data definition.",beginner,code,lines 9-20,docs/platform/datasets_workflow.mdx
Evidently dataset data definition example,"A data definition in Evidently helps specify the purpose of each column in your dataset, which is crucial for its evaluation and management.",intermediate,text,lines 26-26,docs/platform/datasets_workflow.mdx
evidently run evals example,"The article provides a simple code example for running an eval using the Evidently Python library, including the necessary setup for creating evaluation data and uploading results.",beginner,code,lines 11-24,docs/platform/evals_api.mdx
uploading raw data to evidently,"The document explains the process of uploading raw data or evaluation results to the Evidently platform, detailing the options available during this upload process.",intermediate,text,lines 53-63,docs/platform/evals_api.mdx
how to access evaluation reports,"To access your evaluation reports, navigate to your Project and go to the 'Reports' section in the left menu where you can view and download evaluation artifacts.",beginner,text,lines 3-3,docs/platform/evals_explore.mdx
create dataset from CSV no code,"The article explains how to create a dataset by uploading a CSV directly through the user interface, including specifying data definitions upon upload.",beginner,code,lines 5-8,docs/platform/evals_no_code.mdx
starting evaluation with descriptors,It describes how to start an evaluation by adding descriptors from the dataset view and optional configuration with LLM providers.,beginner,code,lines 18-24,docs/platform/evals_no_code.mdx
model-based evaluation methods,"The article details different evaluation methods available, including model-based, regex, text stats, and LLM-based evaluations.",intermediate,text,lines 28-33,docs/platform/evals_no_code.mdx
configuring LLM evaluator,"It provides a guide on how to set up a custom LLM evaluator, including the parameters needed for the evaluation process.",advanced,code,lines 69-84,docs/platform/evals_no_code.mdx
AI product evaluation methods,"The article outlines various evaluation methods such as ad hoc analysis, experiments, safety testing, regression testing, and monitoring for AI product development.",beginner,text,lines 1-11,docs/platform/evals_overview.mdx
running evals on Evidently platform,"The article explains how to run evaluations via API or through a no-code interface on the Evidently platform, detailing the steps for both methods.",intermediate,code,lines 13-51,docs/platform/evals_overview.mdx
batch monitoring examples with Evidently,The article offers a simple example showcasing how to run batch monitoring jobs by connecting to Evidently Cloud and uploading dataset statistics to a workspace.,beginner,code,"lines 5, 13-26",docs/platform/monitoring_local_batch.mdx
how to configure reports in Evidently,"It describes the steps to define and configure an Evidently Report, including setting metrics and uploading results effectively as part of batch monitoring.",intermediate,text,lines 35-50,docs/platform/monitoring_local_batch.mdx
AI observability monitoring overview,"The article explains how AI observability enables tracking input/output quality of AI applications in production, helping to identify and fix issues.",beginner,text,lines 1-1,docs/platform/monitoring_overview.mdx
setting up batch monitoring in Evidently,"Evidently offers a streamlined way to set up batch monitoring jobs, including creating evaluation pipelines and running metric calculations.",intermediate,code,lines 15-29,docs/platform/monitoring_overview.mdx
using Tracely library for data capture,The article outlines how to use the Tracely library to instrument applications for capturing detailed input and output data for LLM-powered applications.,advanced,code,lines 45-57,docs/platform/monitoring_overview.mdx
Evidently Platform features overview,"The article outlines key features of the Evidently Platform, including evaluations, dataset management, synthetic data generation, regression testing, monitoring, and tracing capabilities.",beginner,text,lines 5-5,docs/platform/overview.mdx
How to run evaluations with Evidently,"The Evidently Platform allows users to run evaluations locally or via a no-code interface using 100+ built-in evals and templates, enhancing experimentation tracking and debugging.",intermediate,code,lines 13-13,docs/platform/overview.mdx
create project in Evidently Python example,"The article provides a code example for creating a project in Evidently using Python, specifying organization ID or without it for self-hosting.",beginner,code,lines 9-22,docs/platform/projects_manage.mdx
how to connect to an existing project in Python,"Instructions for connecting to an existing project using Python's `get_project` method are included in the article, including a code snippet.",beginner,code,lines 39-43,docs/platform/projects_manage.mdx
delete project in Evidently warning,The article warns that deleting a project will remove all data inside it and provides the necessary steps to delete a project via both Python and UI.,intermediate,text,lines 69-86,docs/platform/projects_manage.mdx
project parameters Evidently,"Details on the parameters for each project are listed, including examples for attributes like name, ID, description, and time range.",advanced,text,lines 91-99,docs/platform/projects_manage.mdx
overview of projects in Evidently,"The article provides an overview of Projects in Evidently, explaining their purpose in organizing data, reports, and evaluations across different platforms like Evidently OSS, Cloud, and Enterprise.",beginner,text,lines 2-7,docs/platform/projects_overview.mdx
tracing in Evidently,"The article explains that tracing in Evidently captures detailed records of AI application operations, using the open-source `Tracely` library for data collection and evaluation.",beginner,text,lines 5-11,docs/platform/tracing_overview.mdx
pip install tracely example,"The article provides a quick command to install the `tracely` package using pip, which is essential for setting up tracing.",beginner,code,lines 7-11,docs/platform/tracing_setup.mdx
how to initialize tracing with tracely,"To initialize tracing, use the `init_tracing` function along with required parameters like address, API key, and project ID.",beginner,code,lines 19-29,docs/platform/tracing_setup.mdx
get export_id for tracing dataset,The `export_id` of the tracing dataset can be obtained by calling the `get_info()` function after initializing tracely.,intermediate,code,lines 48-54,docs/platform/tracing_setup.mdx
tracing dataset id example,"The export ID serves as a dataset ID for downloading traces, which is mentioned in relation to the datasets API.",intermediate,text,lines 56-56,docs/platform/tracing_setup.mdx
using trace_event decorator,The article explains how to use the `@trace_event` decorator to collect traces for specific functions in your application.,beginner,code,lines 61-71,docs/platform/tracing_setup.mdx
trace function arguments example,You can specify which function arguments to log in the trace by using parameters like `track_args` in the `@trace_event` decorator.,intermediate,code,lines 73-91,docs/platform/tracing_setup.mdx
multi-step tracing in LLM workflows,The article discusses how to trace multi-step workflows by nesting functions with `@trace_event` for detailed tracing.,advanced,text,lines 105-107,docs/platform/tracing_setup.mdx
context manager for tracing,You can create trace events using a context manager for specific pieces of code to gain fine-grained control over tracing.,intermediate,code,lines 139-159,docs/platform/tracing_setup.mdx
set attributes for trace events,You can add custom attributes to active span events using the `get_current_span()` method.,intermediate,code,lines 194-210,docs/platform/tracing_setup.mdx
linking events to a trace,"Using the `tracely.bind_to_trace` function, you can link different events into a single trace for better tracking across systems.",advanced,text,lines 222-240,docs/platform/tracing_setup.mdx
Evidently Cloud account setup steps,"The article outlines how to create a free account on Evidently Cloud, set up an organization, generate an API token, and connect using the Evidently Python library.",beginner,text,"lines 1-4, 12-24",docs/setup/cloud.mdx
create local workspace evidently,"To create a local workspace in Evidently, you can use the command `ws = Workspace.create(""evidently_ui_workspace"")` after installing the Evidently library.",beginner,code,lines 12-41,docs/setup/self-hosting.mdx
launch evidently ui service,"To launch the Evidently UI service, you can use the command `evidently ui` in your terminal from the directory containing your workspace.",beginner,code,lines 103-125,docs/setup/self-hosting.mdx
evidently workspace parameters,"When creating a remote workspace, you can specify parameters like `base_url` for the remote service and `secret` for access protection.",intermediate,text,lines 61-72,docs/setup/self-hosting.mdx
evidently remote data storage,You can store snapshots in a remote data store (like S3) and access them using the `fsspec` library in Evidently.,intermediate,code,lines 74-87,docs/setup/self-hosting.mdx
delete workspace command evidently,"To delete a workspace in Evidently, use the terminal command `rm -r workspace`, but be aware that all data will be permanently deleted.",advanced,text,lines 90-100,docs/setup/self-hosting.mdx
automated testing LLM outputs GitHub Actions,"The article outlines how to use Evidently with GitHub Actions to automatically test LLM outputs with every code push or pull request, ensuring feedback through detailed reports.",beginner,code,lines 1-2,examples/GitHub_actions.mdx
basic LLM evaluation setup guide,"The article details the setup and basic API for LLM evaluations, providing a structured introduction to evaluation methods.",beginner,text,lines 1-5,examples/LLM_evals.mdx
examples of reference-free evaluation methods,The document lists various reference-free evaluation techniques such as text statistics and session-level evaluators along with accompanying code examples.,intermediate,code,lines 5-5,examples/LLM_evals.mdx
LLM judge tutorial example,The article provides a detailed tutorial on creating and evaluating an LLM judge using Python code examples.,beginner,code,lines 1-4,examples/LLM_judge.mdx
Steps to create evaluation dataset for LLM,"It outlines the steps needed to create a toy Q&A dataset for testing the LLM judge, including defining questions and target responses.",beginner,code,lines 25-27,examples/LLM_judge.mdx
How to configure LLM judge prompt,"The article describes how to configure the LLM judge's prompt, focusing on criteria for correctness evaluation.",intermediate,code,lines 231-246,examples/LLM_judge.mdx
Using OpenAI API for LLM,Instructions are provided to set up and use the OpenAI API key for embedding in the LLM evaluator within the Python code.,beginner,code,lines 67-71,examples/LLM_judge.mdx
Evaluate LLM judge accuracy,It explains how to compare LLM judge evaluations against manual labels to assess correctness and accuracy of responses.,intermediate,code,lines 227-268,examples/LLM_judge.mdx
Difference between reference-based and open-ended LLM evaluation,"The article distinguishes between two evaluation methods: reference-based and open-ended, tailored for different use cases.",beginner,text,lines 10-13,examples/LLM_judge.mdx
How to preview dataset in Python,It gives code examples on how to preview the created dataset in Python to review the data structure and contents.,beginner,code,lines 200-203,examples/LLM_judge.mdx
Adding descriptors to the evaluation dataset,The article demonstrates how to enhance the dataset by adding descriptors to evaluate the quality of responses.,intermediate,code,lines 268-267,examples/LLM_judge.mdx
Setting up binary classification for LLM,Instructions are provided on how to set up a binary classification system to judge the quality of responses from the LLM.,intermediate,code,lines 314-317,examples/LLM_judge.mdx
Generating report for LLM evaluations,It describes how to create a report summarizing the results of LLM evaluations and interpret key metrics.,intermediate,code,lines 288-298,examples/LLM_judge.mdx
What makes a good LLM judge prompt,"The article discusses factors that contribute to an effective prompt for LLM judging, such as specificity and clarity.",intermediate,text,lines 253-253,examples/LLM_judge.mdx
Creating a verbosity evaluator for responses,It outlines how to create a verbosity evaluator in LLM that assesses the conciseness and clarity of generated outputs.,intermediate,code,lines 346-375,examples/LLM_judge.mdx
Advantages of using LLM as a judge,The article highlights the flexibility and customization options available when using LLM as a judgment tool in evaluation workflows.,beginner,text,lines 400-400,examples/LLM_judge.mdx
Sample code for LLM evaluation,"Full code snippets are provided for evaluating LLM responses, which can be directly used in Python.",beginner,code,lines 46-62,examples/LLM_judge.mdx
Setting environment variables for LLM,It instructs users on how to specify their OpenAI API key as an environment variable for lLM integration.,beginner,code,lines 67-71,examples/LLM_judge.mdx
Run a sample LLM notebook,The article references how to run example notebooks for practical understanding and step-by-step learning.,beginner,code,lines 38-39,examples/LLM_judge.mdx
Explore results from Evidently Cloud,It provides steps for integrating evaluation workflows with Evidently Cloud for data exploration and management.,intermediate,code,lines 402-426,examples/LLM_judge.mdx
Define custom criteria for LLM judging,It explains how to set custom evaluation criteria that gives flexibility in how the LLM judges outputs.,intermediate,text,lines 354-358,examples/LLM_judge.mdx
Integration options for different evaluator LLMs,The article discusses options to choose different LLMs for evaluating responses based on specific evaluation needs.,advanced,text,lines 74-74,examples/LLM_judge.mdx
Confusion matrix evaluation in LLM judging,It explains how to evaluate the performance of the LLM judge using a confusion matrix for classification errors.,advanced,code,lines 342-342,examples/LLM_judge.mdx
using multiple LLMs for evaluation,"This approach involves utilizing several LLMs to aggregate their evaluations, considering an output as a 'pass' only if the majority approves.",beginner,text,lines 1-1,examples/LLM_jury.mdx
how to install Evidently for LLM evaluations,Install the Evidently package using pip to get started with the setup for evaluating language model outputs.,beginner,code,lines 9-13,examples/LLM_jury.mdx
define evaluation criteria with LLM,Use the `BinaryClassificationPromptTemplate` for setting appropriate judging criteria for the emails generated by the model.,intermediate,code,lines 94-114,examples/LLM_jury.mdx
set up API keys for LLM judges,Pass your API keys for different LLM providers in your environment setup to allow for evaluation by multiple judges.,beginner,code,lines 36-42,examples/LLM_jury.mdx
viewing the results of LLM evaluations,You can export evaluation results to a DataFrame or generate summary reports to inspect the judgments made by the LLMs.,intermediate,code,lines 175-199,examples/LLM_jury.mdx
create a project for LLM evaluation in Evidently,"To store your evaluation results, you can create a project within the Evidently Cloud workspace using specific parameters.",beginner,code,lines 55-61,examples/LLM_jury.mdx
evaluate email tone appropriateness with LLM,The process measures the appropriateness of email output by setting up evaluators with specific criteria for judging tone and content.,intermediate,text,lines 96-108,examples/LLM_jury.mdx
custom descriptor for LLM disagreements,A custom function can be defined to identify and flag disagreements among the LLM judges based on their evaluation outcomes.,advanced,code,lines 156-170,examples/LLM_jury.mdx
exporting evaluation results to Evidently Cloud,"After evaluating, you can upload your results to the Evidently Cloud for easier access and display of metrics.",intermediate,code,lines 191-195,examples/LLM_jury.mdx
evaluate RAG system metrics,"The article guides users through evaluating various metrics for RAG systems, focusing on both retrieval and generation quality.",beginner,text,line 4,examples/LLM_rag_evals.mdx
installation steps for Evidently,"Clear instructions are provided for installing the Evidently library necessary for RAG evaluations, including a simple command for installation.",beginner,code,lines 28-32,examples/LLM_rag_evals.mdx
data evaluation with Pandas,The article demonstrates how to use Pandas to manage and evaluate data by transforming raw data into a structured format for analysis.,beginner,code,lines 86-87,examples/LLM_rag_evals.mdx
context quality assessment methods,"Different methods for assessing the quality of retrieved contexts are explained, including the ContextQualityLLMEval and ContextRelevance metrics.",intermediate,text,lines 96-138,examples/LLM_rag_evals.mdx
generate synthetic datasets for evaluation,"It shows how to create synthetic datasets for testing retrieval and generation functionalities, suitable for evaluating RAG system performance.",beginner,code,lines 64-84,examples/LLM_rag_evals.mdx
using ContextRelevance metrics,Guidelines are provided on using the ContextRelevance metric to evaluate the relevance of chunked data inputs against questions.,intermediate,code,lines 162-171,examples/LLM_rag_evals.mdx
how to run a correctness evaluation,"The article explains how to run correctness evaluations using LLM-based matching techniques, comparing responses to expected 'target' outputs.",intermediate,code,lines 201-230,examples/LLM_rag_evals.mdx
visual reports for RAG evaluations,"Instructions on creating visual reports with Evidently to summarize RAG evaluation results are extensively covered, helping to visualize data insights.",beginner,code,lines 274-305,examples/LLM_rag_evals.mdx
features of Evidently Cloud,"A summary of the Evidently Cloud features for systematic tracking and easy comparison of evaluation results is provided, emphasizing its utility.",beginner,text,lines 328-360,examples/LLM_rag_evals.mdx
numerical metrics for performance tracking,The article discusses leveraging numerical metrics derived from evaluations to track the performance of a RAG system over time.,intermediate,text,lines 311-324,examples/LLM_rag_evals.mdx
uploading evaluations to Cloud,Details are given on how to upload evaluation results to the Evidently Cloud for detailed analysis and ongoing monitoring.,intermediate,code,lines 352-358,examples/LLM_rag_evals.mdx
improving RAG response quality,"Strategies for enhancing RAG response quality are discussed, such as setting conditions for expected outcomes during evaluations.",advanced,text,lines 309-324,examples/LLM_rag_evals.mdx
setting up tests in reports,Learn how to set up specific tests within reports to ensure context validity and response fidelity during evaluations.,advanced,code,lines 310-320,examples/LLM_rag_evals.mdx
LLM regression testing tutorial,"This tutorial guides you on how to perform regression testing specifically for LLM outputs, detailing each step involved.",beginner,text,1-1,examples/LLM_regression_testing.mdx
compare old and new LLM responses,You can compare new and old responses to identify any significant changes after altering prompts or models.,beginner,text,3-3,examples/LLM_regression_testing.mdx
steps for regression testing LLM,"The tutorial outlines several steps including creating a dataset, generating new answers, and evaluating them using LLM as a judge.",beginner,text,11-19,examples/LLM_regression_testing.mdx
install evidently library for LLM,"Instructions for installing the Evidently library for LLM regression testing are provided, using the command `pip install evidently[llm]`.",beginner,code,39-43,examples/LLM_regression_testing.mdx
how to create a toy dataset for LLM,You will create a small Q&A dataset with reference answers as part of the tutorial to facilitate testing.,beginner,code,13-13,examples/LLM_regression_testing.mdx
generate new outputs from LLM,The article explains how to simulate generating new outputs by adding a new column to your dataset with the LLM's responses.,intermediate,code,178-178,examples/LLM_regression_testing.mdx
evaluate LLM outputs using tests,"You will learn to create and run tests to evaluate length, correctness, and style consistency of LLM outputs against references.",intermediate,code,17-17,examples/LLM_regression_testing.mdx
using observability dashboards for LLM results,The tutorial covers how to build a monitoring dashboard to track results over time and analyze the effectiveness of tests.,intermediate,code,19-19,examples/LLM_regression_testing.mdx
Python code for LLM correctness check,A binary classification prompt template is provided to evaluate the correctness of responses from the LLM.,advanced,code,243-256,examples/LLM_regression_testing.mdx
check LLM style consistency in outputs,Guidance is given on how to implement a judge for style matching of LLM outputs using a classification approach.,advanced,code,273-292,examples/LLM_regression_testing.mdx
set up a monitoring dashboard in Evidently,Instructions for setting up a monitoring dashboard to visualize LLM test results are available in the article.,intermediate,code,464-482,examples/LLM_regression_testing.mdx
run regression tests for LLM outputs,You can define and run regression tests on your LLM outputs using the setup explained in the article.,advanced,code,300-300,examples/LLM_regression_testing.mdx
interpret results from LLM tests,"The article explains how to interpret the results from running regression tests, including pass/fail scenarios.",intermediate,text,386-386,examples/LLM_regression_testing.mdx
requirements for LLM regression testing,"To complete the tutorial, basic Python knowledge and an OpenAI API key are required.",beginner,text,25-29,examples/LLM_regression_testing.mdx
Evidently Cloud installation steps,"The tutorial includes setup steps for connecting to Evidently Cloud, including API token usage.",beginner,code,61-94,examples/LLM_regression_testing.mdx
custom metrics in LLM evaluation,"You can define custom metrics to evaluate LLM responses beyond standard checks, enhancing testing fidelity.",advanced,code,222-232,examples/LLM_regression_testing.mdx
LLM evaluation dashboard features,"The tutorial discusses the features of the dashboard created for LLM evaluations, such as tracking test results historically.",intermediate,text,496-501,examples/LLM_regression_testing.mdx
testing against reference responses in LLM,The process outlined in the article allows testing LLM outputs against pre-defined reference responses for accuracy.,intermediate,text,228-228,examples/LLM_regression_testing.mdx
using Jupyter notebook for LLM testing,You can run the entire LLM regression testing example in a Jupyter notebook as demonstrated in the tutorial.,beginner,code,34-34,examples/LLM_regression_testing.mdx
LLM evaluation tutorial,"The article offers a comprehensive overview of LLM evaluation methods, including code examples and video tutorials to guide users through the process.",beginner,text,lines 22-23,examples/introduction.mdx
how to use LLM judge,It includes a tutorial on creating and tuning LLM judges suitable for aligning outputs with human preferences.,intermediate,code,lines 99-99,examples/introduction.mdx
Evidently GitHub actions example,There is a practical example showing how to implement Evidently evaluations within a CI/CD pipeline using GitHub Actions.,intermediate,code,lines 64-66,examples/introduction.mdx
metrics cookbook ML,"The metrics cookbook section provides various data and ML metrics for regression, classification, and data drift evaluation with examples.",beginner,code,lines 54-56,examples/introduction.mdx
RAG evaluation metrics,"The article features a walkthrough detailing different RAG evaluation metrics, aimed at validating retrieval-augmented generation systems.",intermediate,text,lines 29-30,examples/introduction.mdx
tracing AI application,There’s a quickstart guide on how to collect inputs and outputs from an AI application using tracing techniques.,beginner,code,lines 17-17,examples/introduction.mdx
deploy LLM evaluation,The deployment section provides guidance on how to create a workspace and run reports for LLM evaluations in an open-source UI.,intermediate,code,lines 83-83,examples/introduction.mdx
using multiple LLMs for evaluation,"One tutorial explains how to use multiple LLMs to evaluate the same output, offering insights into comparative analysis.",intermediate,text,lines 32-34,examples/introduction.mdx
LLM content generation,"A tutorial on the article explains how to utilize LLMs to generate content, like tweets, and evaluate engagement levels.",beginner,code,lines 101-101,examples/introduction.mdx
LLM evaluation methods overview,"The article outlines different methods for LLM evaluation, providing clarity on both reference-based and reference-free evaluation techniques.",advanced,text,lines 97-98,examples/introduction.mdx
how to visualize LLM metrics,"There are examples detailing how to visualize LLM evaluation metrics in Grafana, using PostgreSQL as the database.",advanced,code,lines 70-71,examples/introduction.mdx
Evidently Cloud v2 breaking changes,"The article highlights major breaking changes in Evidently Cloud v2, including compatibility issues with library versions below 0.7.0 and the recommendation to switch for new users.",beginner,text,lines 3-17,faq/cloud_v2.mdx
evidently migration guide 0.6,"This guide explains key changes for users migrating from earlier versions to Evidently 0.6 and above, highlighting major updates and what to expect.",beginner,text,lines 1-1,faq/migration.mdx
using new Report object Evidently,The new Report object in Evidently 0.6 enables users to generate reports using updated APIs for better functionality and ease of use.,beginner,code,lines 13-13,faq/migration.mdx
legacy API coexistence Evidently,"During the transition from versions 0.6 to 0.6.7, both the legacy and new APIs exist, allowing users to choose their preferred option until fully migrating.",intermediate,text,lines 14-14,faq/migration.mdx
data_definition instead of column_mapping,"Evidently 0.6 replaces column_mapping with data_definition, requiring users to create a Dataset object, significantly enhancing data handling flexibility.",intermediate,text,lines 24-26,faq/migration.mdx
aggregate descriptor results in Evidently,"Users can aggregate descriptor results in Evidently by running conditional checks, allowing efficient summary and analysis without recomputation.",intermediate,code,lines 42-68,faq/migration.mdx
presets for reports Evidently,"The new Report API in Evidently allows the use of presets to generate reports and summaries easily, improving reporting workflows.",beginner,code,lines 84-86,faq/migration.mdx
custom metrics dashboard API Evidently,"With the new Dashboard API, custom metrics can now be integrated and visualized in the UI, enhancing the dashboard's functionality.",advanced,code,lines 145-145,faq/migration.mdx
Evidently library features,"The Evidently library is an open-source Python library for AI evaluations and reporting, suited for data scientists and ML engineers.",beginner,text,lines 5-13,faq/oss_vs_cloud.mdx
Open-source vs cloud features comparison,"The article outlines the differences in features between the open-source and cloud/enterprise editions, highlighting core and premium features.",intermediate,text,lines 31-68,faq/oss_vs_cloud.mdx
Evidently enterprise self-hosted benefits,Evidently Enterprise offers self-hosted options with full features and dedicated support for teams with strict security requirements.,intermediate,text,"lines 29, 75-79",faq/oss_vs_cloud.mdx
How to deploy Evidently OSS,"The open-source version of Evidently requires users to handle their own deployment, maintenance, and scaling resources manually.",beginner,code,lines 81-83,faq/oss_vs_cloud.mdx
Installation of Evidently Python library,Installation instructions and usage examples for the Evidently library are crucial for data scientists looking to evaluate AI systems.,beginner,code,lines 5-13,faq/oss_vs_cloud.mdx
Premium features of Evidently cloud,The cloud version of Evidently offers advanced features like scheduled evaluations and dataset management that are not available in the OSS version.,advanced,text,lines 46-56,faq/oss_vs_cloud.mdx
data collected by Evidently telemetry,"Evidently collects anonymous usage data related to the environment and service interactions, such as user ID, OS version, and actions performed in the service.",beginner,text,lines 5-13,faq/telemetry.mdx
how to opt out of Evidently telemetry,"Users can opt out of telemetry by setting the environment variable DO_NOT_TRACK to any value, thus disabling data collection.",beginner,code,lines 85-99,faq/telemetry.mdx
telemetry data from Evidently Monitoring UI,"Telemetry is collected only when using the Evidently Monitoring UI, with no data collected when running as a library.",intermediate,text,lines 9-11,faq/telemetry.mdx
Evidently telemetry environment data,"The environment data collected includes the OS name, version, Python version, and other system details pertinent to how Evidently is run.",intermediate,text,lines 17-33,faq/telemetry.mdx
list of actions tracked by telemetry,"Evidently tracks several actions like Startup, Index, List_projects, Get_project_info, among others, to understand feature usage.",beginner,text,lines 43-69,faq/telemetry.mdx
Evidently's anonymous data collection,"Evidently only collects anonymous usage data, ensuring no personal or sensitive information is captured during telemetry.",beginner,text,lines 11-13,faq/telemetry.mdx
how to enable telemetry in Evidently,"Telemetry is enabled by default, but can be turned back on by unsetting the DO_NOT_TRACK environment variable.",intermediate,code,lines 99-102,faq/telemetry.mdx
configuring telemetry settings for Evidently,"Users can control telemetry settings directly through environment variables, allowing customization based on personal preferences.",intermediate,code,lines 81-91,faq/telemetry.mdx
version requirements for telemetry data collection,"Telemetry collection in Evidently starts from version 0.4.0, ensuring users are using the appropriate version for data tracking.",beginner,text,line 7,faq/telemetry.mdx
remote IP address handling in Evidently telemetry,"Evidently uses an obscured source IP address for telemetry, ensuring user anonymity while collecting essential data.",advanced,text,lines 35-41,faq/telemetry.mdx
Evidently AI capabilities and features,"The article outlines that Evidently AI supports the development of reliable AI products and offers a comprehensive feature set, including built-in evaluations and collaborative tools.",beginner,text,"1, 37-37",faq/why_evidently.mdx
Open source benefits of Evidently,"Evidently is an open-source library with a transparent implementation, numerous community resources, and a user-friendly API, which enhances trust and collaboration.",beginner,text,5-7,faq/why_evidently.mdx
How to integrate Evidently into my workflow,"The article mentions that Evidently is modular and can easily integrate into existing workflows, allowing users to add components like monitoring and custom metrics as needed.",intermediate,code,11-25,faq/why_evidently.mdx
Built-in evaluations in Evidently,"Evidently features over 100 built-in evaluations tailored for various ML and LLM use cases, simplifying the process of measuring performance without extensive setup.",intermediate,code,27-31,faq/why_evidently.mdx
Evidently Python library features,"The Evidently Python library provides over 100 evaluation metrics and a declarative testing API, making it an essential tool for monitoring data and AI systems.",beginner,text,lines 3-3,introduction.mdx
descriptor types in evaluation,"The article outlines various descriptor types for evaluations, including deterministic evals, content checks, syntax validation, and ML-based evals.",beginner,text,lines 5-7,metrics/all_descriptors.mdx
exactmatch example,"An example of the ExactMatch descriptor is `ExactMatch(columns=[""answer"", ""target""])` which checks if contents match between two columns.",beginner,code,line 15,metrics/all_descriptors.mdx
regexp descriptor usage,"The RegExp descriptor checks whether text matches a specified regular expression, such as `RegExp(reg_exp=r""^I"")` which checks if text starts with 'I'.",beginner,code,line 16,metrics/all_descriptors.mdx
includeswords parameter,The IncludesWords descriptor checks if the text includes specified vocabulary words and accepts parameters like `words_list` and `mode` for additional control over the check.,intermediate,code,line 28,metrics/all_descriptors.mdx
how to validate JSON,"The IsValidJSON descriptor checks if the column contains a valid JSON, returning True/False for each input, and can optionally include an alias.",beginner,code,line 42,metrics/all_descriptors.mdx
text length measurement,"The TextLength descriptor measures the length of the text in symbols and returns an absolute number, useful for understanding text size.",beginner,code,line 54,metrics/all_descriptors.mdx
syntax validation examples,"Examples of syntax validations include IsValidPython() and IsValidSQL(), which check for valid Python code and SQL queries respectively.",intermediate,code,lines 45-46,metrics/all_descriptors.mdx
custom descriptors overview,"CustomDescriptor() allows the implementation of custom checks using Python functions for specific columns, enhancing the evaluation process.",intermediate,code,line 66,metrics/all_descriptors.mdx
containslink descriptor,The ContainsLink descriptor checks if the column contains at least one valid URL and returns True/False for each row.,intermediate,code,line 34,metrics/all_descriptors.mdx
faithfulness evaluation,"FaithfulnessLLMEval assesses if the response maintains fidelity to the provided context, returning labels like FAITHFUL or UNFAITHFUL.",advanced,code,line 89,metrics/all_descriptors.mdx
context relevance check,ContextRelevance() checks if the provided context is relevant to a given question and outputs a similarity score between 0 and 1.,advanced,code,line 88,metrics/all_descriptors.mdx
text stats analysis,The document details text statistics descriptors like OOVWordsPercentage() and SentenceCount() which help evaluate text quality.,intermediate,code,lines 55-57,metrics/all_descriptors.mdx
how to use the LLMEval descriptor,"LLMEval scores text based on user-defined criteria and requires parameters including provider, model, and template for the evaluation.",intermediate,code,line 79,metrics/all_descriptors.mdx
custom columns descriptor explanation,"CustomColumnsDescriptor allows for implementing checks on any column in the dataset, enhancing evaluation flexibility.",intermediate,code,line 67,metrics/all_descriptors.mdx
usage of DoesNotContain(),"The DoesNotContain() descriptor checks if the text lacks specified forbidden expressions, returning True/False per input.",beginner,code,line 27,metrics/all_descriptors.mdx
context quality evaluation,"The ContextQualityLLMEval assesses if the context provides adequate information to answer a question, returning a validity label.",advanced,code,line 87,metrics/all_descriptors.mdx
needing a valid SQL check,"Use IsValidSQL() to check if a column contains valid SQL queries without executing them, ensuring syntax correctness.",intermediate,code,line 46,metrics/all_descriptors.mdx
detecting toxicity in text,"ToxicityLLMEval detects toxic language in the text, returning a label indicating the level of toxicity.",advanced,code,line 103,metrics/all_descriptors.mdx
parameters for CustomDescriptor,CustomDescriptor function includes parameters like `alias` and `func` to implement specific checks as needed on columns.,intermediate,code,line 66,metrics/all_descriptors.mdx
negative text detection process,NegativityLLMEval identifies negative texts and returns a label indicating whether the text is deemed negative or positive.,advanced,code,line 101,metrics/all_descriptors.mdx
using the ItemMatch() descriptor,ItemMatch() checks if the text contains specified items from a provided list and returns True/False based on its findings.,beginner,code,line 30,metrics/all_descriptors.mdx
checking word inclusion,"The IncludesWords descriptor checks for the presence of specified vocabulary words within the text, allowing for modes of either 'any' or 'all'.",intermediate,code,line 28,metrics/all_descriptors.mdx
assessing semantic similarity,The SemanticSimilarity() descriptor calculates similarity scores between two columns based on cosine similarity using a sentence embeddings model.,intermediate,code,line 111,metrics/all_descriptors.mdx
json schema validation,"JSONSchemaMatch checks if a column's JSON object aligns with an expected schema, ensuring all keys are present and values are not None.",advanced,code,line 43,metrics/all_descriptors.mdx
method for sentiment analysis,"Sentiment() analyzes text sentiment and provides a score ranging from -1 (negative) to 1 (positive), useful for gauging opinion in text.",intermediate,code,line 113,metrics/all_descriptors.mdx
importance of non-letter character percentage,"NonLetterCharacterPercentage calculates the ratio of non-letter characters in the text, which can provide insights into text cleanliness and formatting.",beginner,code,line 56,metrics/all_descriptors.mdx
dataset-level evaluation metrics,"The article provides a comprehensive reference for all dataset-level evaluation metrics, including descriptions and parameters.",beginner,text,lines 1-3,metrics/all_metrics.mdx
text evaluations available,"It summarizes results of text or LLM evaluations, detailing how to score individual inputs and use descriptors.",beginner,text,lines 19-21,metrics/all_metrics.mdx
metric cookbook example,The article references a Metric cookbook where users can find code examples for implementing various metrics.,beginner,code,line 2,metrics/all_metrics.mdx
how to read metric tables,"It outlines how to interpret metric tables, including metric names, descriptions, parameters, and test defaults.",beginner,text,lines 5-16,metrics/all_metrics.mdx
parameters for metrics,Each metric has a specific set of available parameters which can be used to customize evaluations.,intermediate,text,lines 28-30,metrics/all_metrics.mdx
Column level data quality metrics,The article details various column-level data quality metrics to assess the integrity of individual columns.,beginner,text,lines 54-56,metrics/all_metrics.mdx
aggregate descriptor results,This section explains how to use metrics for aggregating descriptor results or checking data quality at the column level.,intermediate,text,lines 33-33,metrics/all_metrics.mdx
ValueStats() metrics,ValueStats() provides several statistical metrics like UniqueValueCount and MissingValueCount for assessing individual column quality.,intermediate,text,lines 43-45,metrics/all_metrics.mdx
example of ValueStats() implementation,An example metric is provided for using ValueStats() to compute descriptive statistics for a specified column.,intermediate,code,line 45,metrics/all_metrics.mdx
Data Drift detection methods,"The article discusses various methods for detecting data drift, including visualizing distributions and calculating drift scores.",intermediate,text,lines 111-119,metrics/all_metrics.mdx
how to map text columns,Guides on mapping text columns are included for using data definition effectively within metrics.,beginner,text,lines 24-24,metrics/all_metrics.mdx
how to compute UniqueValueCount,"It provides the requirement and use case for the UniqueValueCount metric, essential for data analysis.",intermediate,code,line 69,metrics/all_metrics.mdx
standard deviation metric,"Details about the StdValue() metric, which computes the standard deviation for a numerical column, are provided.",intermediate,text,lines 46-46,metrics/all_metrics.mdx
MinValue() column metric,MinValue() computes the minimum value for a specified numerical column and is crucial for data validation.,intermediate,text,lines 46-47,metrics/all_metrics.mdx
MaxValue() usage,"The MaxValue() metric details how to compute the maximum value for a given column, with relevant implementation details.",intermediate,code,lines 49-49,metrics/all_metrics.mdx
importance of data quality metrics,Data quality metrics are crucial for ensuring the integrity and reliability of datasets before model training and evaluation.,beginner,text,lines 56-56,metrics/all_metrics.mdx
what is RecallTopK(),RecallTopK() calculates the recall at the top K retrieved items and is useful in ranking tasks.,intermediate,text,lines 228-229,metrics/all_metrics.mdx
Dummy model metrics overview,An overview of dummy model quality metrics that help assess baseline performance compared to a random model is provided.,beginner,text,lines 157-159,metrics/all_metrics.mdx
FNR() metric description,"FNR() calculates the False Negative Rate, essential for understanding the performance of classification models.",advanced,text,lines 150-150,metrics/all_metrics.mdx
calculate LogLoss(),It details how to implement the LogLoss() metric for evaluating the performance of classification models.,intermediate,code,lines 152-152,metrics/all_metrics.mdx
MeanAbsolutePercentageError (MAPE),"The MAPE metric is discussed, including its role in evaluating regression model performance.",intermediate,text,lines 195-195,metrics/all_metrics.mdx
DataSummaryPreset() implementation,"DataSummaryPreset() combines dataset statistics and value statistics for overall analysis, along with example usage.",intermediate,code,lines 85-85,metrics/all_metrics.mdx
DummyRecall() metric,"DummyRecall() measures recall for a dummy model, providing insight into baseline evaluation methods.",beginner,code,lines 163-163,metrics/all_metrics.mdx
Effect of reference in metrics,Explanations on how providing a reference impacts the results of various metrics are included in the article.,intermediate,text,lines 14-16,metrics/all_metrics.mdx
example of Precision() in metric evaluation,Precision() metric with visualizations like confusion matrix is described for evaluating classification quality.,intermediate,code,lines 145-145,metrics/all_metrics.mdx
usage of classification quality metrics,The article outlines usage scenarios for metrics related to the quality of classification tasks.,intermediate,text,lines 130-132,metrics/all_metrics.mdx
what is R2Score() in regression,The R2Score() metric assesses the performance of regression models by explaining the variance accounted for by the model.,intermediate,text,lines 196-196,metrics/all_metrics.mdx
Ranking metrics explanation,"The article discusses metrics for evaluating ranking and retrieval tasks, essential for recommendation systems.",beginner,text,lines 216-223,metrics/all_metrics.mdx
first test condition in metrics,"In metrics, the first test conditions determine the evaluation context, influencing how metrics are computed.",intermediate,text,lines 10-10,metrics/all_metrics.mdx
basic accuracy metric importance,"The accuracy metric reflects the proportion of correctly predicted instances in classification tasks, serving as a fundamental performance measure.",beginner,text,lines 144-144,metrics/all_metrics.mdx
use of MeanError() in analysis,"MeanError() calculates the mean error in predictions, vital for understanding overall model performance in regression.",intermediate,text,lines 192-192,metrics/all_metrics.mdx
how to implement InListValueCount(),"InListValueCount() counts the number and share of specified values in a column, providing valuable insights into data distributions.",intermediate,code,lines 67-67,metrics/all_metrics.mdx
how to calculate False Positive Rate (FPR),"FPR() calculates the rate of false positives for classification models, which is vital for evaluating model performance.",intermediate,text,lines 150-150,metrics/all_metrics.mdx
need for validation metrics,Validation metrics ensure that models not only perform well on training data but also generalize effectively to unseen data.,beginner,text,lines 73-73,metrics/all_metrics.mdx
dataset statistical methods,"Various metrics for statistical evaluation of datasets, including visualizations for detailed insights, are discussed in the article.",intermediate,text,lines 81-81,metrics/all_metrics.mdx
what is DatasetMissingValueCount(),DatasetMissingValueCount() assesses missing values in a dataset and is crucial for maintaining data quality.,intermediate,text,lines 105-105,metrics/all_metrics.mdx
usage of DriftedColumnsCount(),"DriftedColumnsCount() calculates the number of columns with detected data drift, crucial for monitoring data quality over time.",intermediate,code,lines 124-124,metrics/all_metrics.mdx
understanding ranking quality metrics,"The article introduces ranking quality metrics for evaluating performance in ranking tasks, pivotal for recommendations.",intermediate,text,lines 216-218,metrics/all_metrics.mdx
pre-built evaluation templates explained,"The article describes various pre-built evaluation templates, known as Presets, that can be easily used on a dataset level, allowing users to run evaluations without setup.",beginner,text,1-1,metrics/all_presets.mdx
data drift detection methods examples,"The article provides examples of how to implement data drift detection methods such as PSI, K-S test, and Chi-square test.",beginner,code,lines 81-92,metrics/customize_data_drift.mdx
how to set custom thresholds for data drift,You can set custom thresholds for drift detection methods in the Metrics or Presets by using parameters like `cat_threshold` or `num_threshold`.,beginner,code,lines 45-49,metrics/customize_data_drift.mdx
override default data drift metrics,"To override default data drift metrics, pass custom parameters to the chosen Metric or Preset, including modifying methods, thresholds, and conditions.",beginner,code,lines 3-5,metrics/customize_data_drift.mdx
dataset-level drift detection settings,"You can set the dataset-level drift detection parameters, such as the share of drifting columns indicating dataset drift, with examples provided for implementation.",intermediate,code,lines 21-39,metrics/customize_data_drift.mdx
drift detection method for categorical columns,"For categorical columns, you can use methods like Chi-square and specify thresholds to detect drift effectively as shown in the examples.",intermediate,code,lines 88-90,metrics/customize_data_drift.mdx
custom data drift method implementation,The article outlines how to implement a custom data drift detection method using the StatTest class and registering it for use in reports.,advanced,code,lines 119-169,metrics/customize_data_drift.mdx
understanding drift parameters explanation,"Drift parameters such as `drift_share`, `method`, and `threshold` are explained in detail with their applications and significance in data drift detection.",beginner,text,lines 67-75,metrics/customize_data_drift.mdx
detection methods for numerical data,"Numerical data can be analyzed using various drift detection methods including K-S test and Anderson-Darling, with specific implementations detailed.",intermediate,code,lines 87-95,metrics/customize_data_drift.mdx
using PSI for drift detection,PSI (Population Stability Index) can be used for detecting drift in both categorical and numerical data with specific implementations shown in Python.,intermediate,code,lines 31-39,metrics/customize_data_drift.mdx
API differences in data drift conditions,"The article highlights that setting conditions for data drift can differ from the standard Test API usage, emphasizing nuance in threshold roles.",intermediate,text,lines 15-16,metrics/customize_data_drift.mdx
text data drift detection examples,"For text data, methods such as `perc_text_content_drift` and `abs_text_content_drift` are explained along with thresholds for detection.",intermediate,code,lines 108-112,metrics/customize_data_drift.mdx
examples of fully custom drift methods,Examples of fully custom drift detection methods can be implemented using custom functions and the StatTest class as illustrated in the article.,advanced,code,lines 119-182,metrics/customize_data_drift.mdx
data drift relevance of thresholds,"Thresholds play a crucial role in evaluating data drift, where their significance and values can vary based on drift detection methods used.",intermediate,text,lines 72-75,metrics/customize_data_drift.mdx
setting drift parameters for all columns,"You can set drift parameters like methods and thresholds for all columns at once, simplifying the configuration for dataset evaluations.",beginner,code,lines 21-27,metrics/customize_data_drift.mdx
reporting drift detection results,"The results of drift detection can be captured in reports, with examples provided for how to generate and run reports effectively using the library.",beginner,code,lines 24-26,metrics/customize_data_drift.mdx
how to detect drift using classifiers,"The article describes using classifiers to detect drift in text data, with specific models and statistical methods explained for implementation.",advanced,code,lines 110-112,metrics/customize_data_drift.mdx
custom function for row-level evaluator,"The article describes how to implement a custom function for checks not available in Evidently, allowing you to build your own programmatic evaluators.",beginner,code,lines 1-1,metrics/customize_descriptor.mdx
how to use CustomColumnDescriptor,It details how to use `CustomColumnDescriptor` for evaluating values within a dataset's column and returning transformed scores or labels.,intermediate,code,lines 41-49,metrics/customize_descriptor.mdx
difference between CustomColumnDescriptor and CustomDescriptor,"The article outlines the differences, highlighting that `CustomColumnDescriptor` applies to a single column while `CustomDescriptor` can handle multiple columns and return multiple transformed scores.",advanced,text,lines 74-102,metrics/customize_descriptor.mdx
HuggingFace model scoring examples,The article provides code examples of how to use HuggingFace models to score and classify text based on emotions.,beginner,code,lines 1-5,metrics/customize_hf_descriptor.mdx
Installing HuggingFace dependencies,It assumes you have knowledge of Python and how to install necessary packages like `evidently` for using HuggingFace models.,beginner,text,lines 7-9,metrics/customize_hf_descriptor.mdx
HuggingFace models list,"The article mentions various models available through the `HuggingFace()` descriptor, such as emotion classifiers and zero-shot classifiers.",intermediate,text,lines 103-119,metrics/customize_hf_descriptor.mdx
How to add custom ML evaluations with HuggingFace,Users can add their own custom evaluation checks for text data using the general `HuggingFace()` descriptor in Python.,intermediate,code,lines 71-80,metrics/customize_hf_descriptor.mdx
Using HuggingFace for emotion classification,"Detailed examples of using HuggingFace models to classify text into various emotions are presented, including required parameters.",intermediate,code,lines 105-107,metrics/customize_hf_descriptor.mdx
Example of Toxicity evaluation with HuggingFace,The article provides a specific example of how to use the `HuggingFaceToxicity` descriptor for toxicity evaluation in text data.,beginner,code,lines 63-68,metrics/customize_hf_descriptor.mdx
Output format of HuggingFace evaluations,It discusses how to call and view results from evaluations using the `as_dataframe()` method in the `eval_df` object.,intermediate,code,lines 91-95,metrics/customize_hf_descriptor.mdx
Data structure for HuggingFace models,The article explains how to structure your data using a pandas DataFrame for compatibility with HuggingFace model evaluations.,intermediate,code,lines 49-54,metrics/customize_hf_descriptor.mdx
Setting thresholds for HuggingFace evaluations,"It explains how to set classification thresholds for some models, which helps in determining the output labels based on prediction probabilities.",advanced,text,lines 108-108,metrics/customize_hf_descriptor.mdx
HuggingFace integration with evidently,"The document details how HuggingFace integrates with the evidently library for model evaluations, emphasizing the implementation flexibility and model compatibility.",advanced,text,lines 111-119,metrics/customize_hf_descriptor.mdx
configure LLM judges with examples,The article outlines how to configure LLM judges using built-in or custom evaluators with examples provided throughout.,beginner,code,lines 1-5,metrics/customize_llm_judge.mdx
how to create LLM descriptors,"It provides steps for creating LLM descriptors, including using templates for binary and multi-class evaluations.",beginner,code,lines 176-183,metrics/customize_llm_judge.mdx
setup OpenAI API key for evaluators,Instructions on setting up the OpenAI API key as an environment variable are included to enable evaluation.,intermediate,code,lines 67-71,metrics/customize_llm_judge.mdx
examples of built-in LLM judges,The article lists built-in LLM judges like ToxicityLLMEval and ContextQualityLLMEval as examples of configurable evaluators.,intermediate,text,lines 61-65,metrics/customize_llm_judge.mdx
run single-column eval for toxicity,It details how to conduct a single-column evaluation for toxicity using the ToxicityLLMEval descriptor.,beginner,code,lines 74-79,metrics/customize_llm_judge.mdx
multi-column evaluation with context,"Instructions for running a multi-column evaluation related to context quality are provided, requiring parameters for both context and question columns.",intermediate,code,lines 92-97,metrics/customize_llm_judge.mdx
change LLM model and provider,"The process for changing the LLM model and provider is explained, allowing users to specify different models from OpenAI or other sources.",intermediate,code,lines 126-146,metrics/customize_llm_judge.mdx
parameters for LLMEval descriptors,"A detailed description of available parameters for LLMEval descriptors, including template, provider, and additional columns.",advanced,text,lines 442-447,metrics/customize_llm_judge.mdx
create binary classification evaluator,Guidance on setting up a binary classification evaluator with custom criteria and its implementation in code is provided.,beginner,code,lines 185-213,metrics/customize_llm_judge.mdx
evaluate question appropriateness as LLM,An example of evaluating if a question is appropriate using a binary classification template is discussed.,intermediate,code,lines 231-259,metrics/customize_llm_judge.mdx
options using evaluator parameters,Describes how to use Options with various providers for passing API keys and additional parameters directly.,advanced,text,lines 153-168,metrics/customize_llm_judge.mdx
how to customize LLM evaluator templates,"Information on customizing LLM evaluator templates is provided, detailing criteria and categories for classification.",intermediate,text,lines 178-183,metrics/customize_llm_judge.mdx
running multi-class classifier evaluations,"Instructions on running evaluations with multi-class classifiers are included, explaining category definitions and implementation.",intermediate,code,lines 326-372,metrics/customize_llm_judge.mdx
examples of toxicity evaluation output,"The article describes the expected output of toxicity evaluations, showing examples of results from built-in LLM judges.",beginner,text,lines 90-90,metrics/customize_llm_judge.mdx
using additional columns in evaluations,"It discusses how to use additional columns in evaluations for more complex descriptors, enhancing contextual accuracy.",intermediate,code,lines 301-311,metrics/customize_llm_judge.mdx
pip installing Evidently for LLM,Basic guidance for installing the Evidently library to access LLM functionalities is hinted at in the prerequisites.,beginner,text,lines 6-8,metrics/customize_llm_judge.mdx
specifying context in LLMEval,Instructions on how to specify context in evaluations when using LLMEval descriptors are provided.,intermediate,code,lines 182-182,metrics/customize_llm_judge.mdx
metrics for evaluating LLM performance,Insights on what metrics to consider when evaluating LLM performance using descriptors are discussed towards the end.,advanced,text,lines 438-470,metrics/customize_llm_judge.mdx
define criteria for binary classifiers,It mentions how to define criteria for binary classifiers and provides a sample implementation in Python code.,intermediate,code,lines 453-468,metrics/customize_llm_judge.mdx
different providers for LLMs in Evidently,"Discusses the various providers available for LLMs within the Evidently framework, highlighting versatility.",intermediate,text,lines 120-121,metrics/customize_llm_judge.mdx
creating multi-class classification evaluator,Detailed guidance on creating multi-class evaluators with custom category definitions and templates is provided.,advanced,code,lines 324-410,metrics/customize_llm_judge.mdx
debugging LLM outputs for evaluations,"Notes strategies for debugging and validating LLM outputs, especially using Edge cases and performance metrics.",advanced,text,lines 170-171,metrics/customize_llm_judge.mdx
visualizing results from LLM evaluations,The article hints at methods for visualizing results obtained from LLM evaluations using standard data viewing methods.,beginner,text,lines 82-82,metrics/customize_llm_judge.mdx
create custom metric column-level implementation,"The article explains the steps to create a custom metric by implementing a calculation method, setting test conditions, and customizing visualizations.",beginner,code,lines 13-21,metrics/customize_metric.mdx
custom metric default test conditions,"The article describes optional default test conditions that can be set when creating a custom metric, which help evaluate the metric against specified criteria.",intermediate,text,lines 17-19,metrics/customize_metric.mdx
example of MyMaxMetric implementation,"An example implementation of a custom metric, `MyMaxMetric`, is provided, which calculates the maximum value in a column and visualizes it using Plotly.",intermediate,code,lines 29-82,metrics/customize_metric.mdx
plotly in custom metric visualization,"The article highlights how to use Plotly for creating custom visualizations in metrics, noting that skipping visualization results in a simple counter display.",advanced,text,lines 19-21,metrics/customize_metric.mdx
classification metrics explained,"The article discusses various classification metrics such as accuracy, precision, and recall, providing definitions and the importance of each in model evaluation.",beginner,text,lines 3-5,metrics/explainer_classification.mdx
interactive visualizations in model evaluation,"Evidently generates interactive visualizations that help analyze model mistakes and suggest improvement ideas, enhancing model performance analysis.",beginner,text,lines 5-5,metrics/explainer_classification.mdx
confusion matrix usage,"The article illustrates how a confusion matrix visualizes classification errors, helping users understand model performance and areas of error distribution.",beginner,text,lines 13-15,metrics/explainer_classification.mdx
how to calculate accuracy and precision,"It details the calculation of standard metrics including accuracy and precision, which are crucial for assessing model performance in classification tasks.",intermediate,code,lines 3-3,metrics/explainer_classification.mdx
ROC AUC interpretation,"The article provides insights into the ROC AUC metric, explaining how it reflects the model's ability to distinguish between classes under different thresholds.",intermediate,text,lines 21-21,metrics/explainer_classification.mdx
how to visualize model quality by class,"It explains how to visualize model quality metrics by class, which is especially important in multi-class classification problems, allowing for targeted improvements.",intermediate,code,lines 19-21,metrics/explainer_classification.mdx
using precision-recall curve,"Users can refer to the precision-recall curve to understand the trade-off between precision and recall at various thresholds, aiding in model optimization.",advanced,code,lines 45-47,metrics/explainer_classification.mdx
class separation quality visualization,"The article describes how to create scatter plots visualizing class separation quality, helping to select the optimal probability threshold for model predictions.",advanced,code,lines 25-29,metrics/explainer_classification.mdx
overall dataset summary features,"The article discusses a summary widget that provides an overview of the dataset, focusing on missing and empty features.",beginner,text,3-3,metrics/explainer_data_stats.mdx
visualizations for feature types,"The features widget generates various visualizations based on feature types like categorical, numerical, datetime, and text.",beginner,code,9-9,metrics/explainer_data_stats.mdx
explaining feature distribution,The article details how the feature overview table presents statistical summaries and distribution visualizations for each feature type.,intermediate,text,13-13,metrics/explainer_data_stats.mdx
examples of categorical feature analysis,"For categorical features, the article provides visual examples of how to analyze and visualize feature distributions over time and by target.",intermediate,code,35-49,metrics/explainer_data_stats.mdx
missing and constant features criteria,"The article defines almost empty and almost constant features as those where 95% or more of the data is missing or constant, which is highlighted in the dataset overview.",beginner,text,3-3,metrics/explainer_data_stats.mdx
feature correlations insights,"The correlation widget summarizes feature correlations, listing highly correlated variables and changes in correlation between datasets, which is essential for dataset analysis.",advanced,text,67-69,metrics/explainer_data_stats.mdx
how to create a correlation heatmap,"The article explains that correlation heatmaps can be generated from categorical and numerical features using different correlation matrix methods, although this specific widget has been removed in later versions.",advanced,code,75-79,metrics/explainer_data_stats.mdx
default data drift detection algorithm,The article explains how Evidently utilizes the default Data Drift Detection algorithm to identify distribution changes in datasets by comparing reference and current datasets.,beginner,text,1-11,metrics/explainer_drift.mdx
how to set drift detection parameters,"It mentions that users can customize data drift parameters and methods in the Evidently library, providing guidelines in the API reference section.",intermediate,code,"5, 69-69",metrics/explainer_drift.mdx
requirements for data drift detection,"The article outlines the requirements for conducting data drift detection, including the need for two datasets and non-empty columns for evaluation.",beginner,text,23-27,metrics/explainer_drift.mdx
tests for numerical columns,"It describes the specific statistical tests to apply for numerical columns based on the dataset size, including the Kolmogorov-Smirnov test and Wasserstein Distance.",intermediate,text,49-61,metrics/explainer_drift.mdx
drift detection for categorical features,The article notes that categorical features require different tests like the chi-squared test and how to interpret their results when detecting drift.,intermediate,code,51-63,metrics/explainer_drift.mdx
calculating drift score in Evidently,"Drift scores in Evidently are calculated using P-values or distances/divergences, with thresholds set for detecting significant drift.",advanced,text,"56, 66, 104-104",metrics/explainer_drift.mdx
dataset-level drift detection rules,"It explains how users can establish dataset-level drift detection rules, setting conditions based on the percentage of columns that have drifted.",beginner,code,"21, 37-39",metrics/explainer_drift.mdx
ROC AUC in text data drift,The article details how ROC AUC scores are used to detect text data drift and the specific thresholds needed for different dataset sizes.,advanced,text,97-101,metrics/explainer_drift.mdx
recall at K metrics,"The article explains recall at K as a measure of how many relevant items are retrieved within the top K results, detailing how to compute it by user and overall.",beginner,text,9-22,metrics/explainer_recsys.mdx
how to calculate precision at K,"It outlines the steps to compute precision at K, which assesses the proportion of relevant results among the top K items, including calculations by user and overall.",beginner,code,28-40,metrics/explainer_recsys.mdx
F Beta score explanation,"The article describes the F Beta score as a combined measure of precision and recall at K, adjusting the weight of recall relative to precision based on the Beta parameter.",intermediate,text,45-56,metrics/explainer_recsys.mdx
Mean Average Precision implementation steps,"It provides a detailed method for computing Mean Average Precision at K, emphasizing its rank-awareness and how it differs from basic precision.",intermediate,code,59-80,metrics/explainer_recsys.mdx
how to compute NDCG,"The article describes how to compute the Normalized Discounted Cumulative Gain (NDCG) at K, including how to determine relevance scores and the calculation of DCG and IDCG.",intermediate,code,113-128,metrics/explainer_recsys.mdx
mean reciprocal rank usage,It explains the concept of Mean Reciprocal Rank (MRR) and how it is used to measure ranking quality based on the position of the first relevant item.,beginner,text,154-168,metrics/explainer_recsys.mdx
Hitrate in recommendation systems,The article outlines Hit Rate as a metric to measure the share of users with at least one relevant item in their top K recommendations.,beginner,text,138-146,metrics/explainer_recsys.mdx
compute recall and precision at K,"It provides a combined approach to compute both recall and precision at K, elaborating on their formulas and implementations.",intermediate,code,14-35,metrics/explainer_recsys.mdx
difference between MAP and precision,"The article distinguishes MAP from precision, noting that MAP penalizes lower-ranked relevant items, making it rank-aware.",advanced,text,61-63,metrics/explainer_recsys.mdx
score distribution in recommendation metrics,"It discusses how to compute score distribution entropy in recommendations, including using softmax transformation and KL divergence.",advanced,code,176-182,metrics/explainer_recsys.mdx
model quality metrics definitions,"This article explains standard model quality metrics such as Mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE).",beginner,text,lines 3-3,metrics/explainer_regression.mdx
how to calculate MAE,The article provides an overview of how to calculate metrics like Mean Absolute Error (MAE) as part of model quality analysis.,beginner,code,lines 3-51,metrics/explainer_regression.mdx
interactive visualizations for model performance,The article introduces interactive visualizations to analyze model predictions and understand performance issues and errors.,beginner,text,lines 9-9,metrics/explainer_regression.mdx
scatter plot for predicted vs actual values,"It describes how to generate a scatter plot for predicted versus actual values, enabling visual analysis of model performance.",intermediate,code,lines 11-13,metrics/explainer_regression.mdx
error distribution analysis,The article covers how to analyze error distribution to better understand model performance and where it makes mistakes.,intermediate,text,lines 35-39,metrics/explainer_regression.mdx
how to visualize predicted vs actual over time,It details generating visualizations of predicted and actual values over time or by index for performance tracking.,intermediate,code,lines 19-19,metrics/explainer_regression.mdx
importance of standard deviation in metrics,The article mentions that it shows the standard deviation of model metrics to estimate the stability of performance.,intermediate,text,lines 7-7,metrics/explainer_regression.mdx
mean error for underestimation and overestimation,The model quality summary for groups with high positive or negative errors provides insights into mean errors per group.,advanced,text,lines 51-51,metrics/explainer_regression.mdx
histogram for error bias per feature,The article explains using histograms to visualize the distribution of feature values associated with high error predictions.,advanced,code,lines 89-89,metrics/explainer_regression.mdx
using ClassificationPreset python example,"The article provides a Python example showing how to implement the ClassificationPreset in a report, allowing users to run evaluations on their dataset.",beginner,code,lines 7-12,metrics/preset_classification.mdx
Classification Quality Tests auto-generated,"It explains how tests are auto-generated from the reference dataset to assess model performance, detailing conditions derived from the data.",intermediate,text,lines 40-48,metrics/preset_classification.mdx
customize report conditions Classification,"The article outlines how to customize report conditions, allowing users to modify auto-generated tests or add new metrics for improved evaluation.",advanced,code,lines 83-85,metrics/preset_classification.mdx
data drift preset usage example,The article provides example code snippets for using the DataDriftPreset within a Report to compare current and reference datasets.,beginner,code,lines 5-12,metrics/preset_data_drift.mdx
how to set up data drift tests,It explains how to include explicit tests for each column using the DataDriftPreset and the Report's configuration options.,beginner,code,lines 15-23,metrics/preset_data_drift.mdx
understanding overall dataset drift,The article details how to detect overall dataset drift based on the proportion of drifting columns identified during evaluation.,intermediate,text,lines 36-38,metrics/preset_data_drift.mdx
report customization options for data drift,"Customization options for data drift reporting, such as selecting specific columns and changing drift parameters, are discussed in detail to tailor evaluations.",intermediate,text,lines 78-90,metrics/preset_data_drift.mdx
advanced data drift detection methods,"For advanced users, the article outlines various drift detection methods available for customization, such as K-L divergence and PSI.",advanced,text,lines 86-93,metrics/preset_data_drift.mdx
data summary preset example,"The article provides examples of how to use the Data Summary Preset to generate reports on datasets, both with and without tests.",beginner,code,lines 5-12,metrics/preset_data_summary.mdx
how to run data summary report,"Instructions for running a Data Summary report on a current dataset are clearly outlined in the article, including example code snippets.",beginner,code,lines 5-12,metrics/preset_data_summary.mdx
test suite in data summary,"The article discusses how a Test Suite can be implemented using the Data Summary Preset, including auto-generated tests based on reference datasets.",intermediate,text,lines 15-44,metrics/preset_data_summary.mdx
how to run a report on recommender system,The article provides a code snippet showing how to create and run a Report for a recommender system using the RecSysPreset with a specified 'k' value for top-k recommendations.,beginner,code,lines 5-12,metrics/preset_recsys.mdx
RecsysPreset metrics explained,"The article describes that RecsysPreset evaluates the recommender system through various metrics, including NDCG at K and diversity, with metric selection depending on the provided data.",intermediate,text,lines 28-32,metrics/preset_recsys.mdx
customizing report for recommender systems,"This section details how to modify the Report's test conditions and composition, such as adding metrics to assess data drift and creating custom reports.",advanced,code,lines 78-85,metrics/preset_recsys.mdx
how to prepare data for regression preset,The article mentions that users should know how to use the Data Definition to prepare the data before running the Regression Preset.,beginner,text,lines 1-3,metrics/preset_regression.mdx
python code for running regression report,The document provides Python code snippets for generating a regression report using the RegressionPreset for evaluating a dataset.,beginner,code,lines 7-12,metrics/preset_regression.mdx
customizing regression report conditions,The article outlines how users can modify test conditions and report composition in the Regression Quality Preset for more tailored evaluations.,intermediate,code,lines 80-82,metrics/preset_regression.mdx
compute descriptors before running Text Evals,"To run the Text Evals Report, you need to compute descriptors and add them to your dataset first.",beginner,text,lines 1-1,metrics/preset_text_evals.mdx
how to run Report with TextEvals in Python,"You can run a Report in Python using `Report(metrics=[TextEvals()])` and calling `report.run(current, None)` for a single current dataset.",beginner,code,lines 6-10,metrics/preset_text_evals.mdx
customize Text Evals report conditions,You can customize your Text Evals report by selecting specific descriptors and setting custom test conditions using the `columns` parameter.,intermediate,code,lines 68-70,metrics/preset_text_evals.mdx
evidently llm evaluation setup,"The article provides detailed steps on setting up the Evidently environment for LLM evaluation, including how to install the library and create a project.",beginner,code,lines 20-56,quickstart_llm.mdx
how to create a dataset for llm,"It explains how to prepare a toy demo chatbot dataset with questions and answers, essential for evaluating LLM outputs.",beginner,code,lines 57-75,quickstart_llm.mdx
python code to run llm evaluations,"The article includes code snippets for running evaluations using the Evidently dataset, adding descriptors like sentiment and text length.",intermediate,code,lines 111-129,quickstart_llm.mdx
evidently llm descriptor examples,"It outlines various descriptors you can use for evaluations, such as Sentiment, TextLength, and others that enhance LLM output assessments.",intermediate,text,lines 98-110,quickstart_llm.mdx
how to create a report in evidently,"Instructions are provided on creating a report that summarizes evaluation results, which can be previewed locally or uploaded to the Evidently Cloud.",beginner,code,lines 137-171,quickstart_llm.mdx
using openai key for evaluations,The article specifies how to set the OpenAI API key as an environment variable for evaluating LLM outputs with the OpenAI model.,intermediate,code,lines 100-108,quickstart_llm.mdx
define custom llm judge template,It shows how to create a custom LLM judge template for evaluating question appropriateness using the BinaryClassificationPromptTemplate.,advanced,code,lines 238-278,quickstart_llm.mdx
track llm evaluation results,The document describes how to create a dashboard for tracking evaluation results over time within the Evidently platform.,intermediate,text,lines 175-181,quickstart_llm.mdx
conditional tests in llm evaluations,"The article explains how to implement conditions for evaluation results, such as checks for sentiment and text length, enhancing the evaluation process.",advanced,code,lines 185-210,quickstart_llm.mdx
evaluate prediction accuracy in Python,"The article explains how to evaluate prediction accuracy for ML models, including methods to check classification or regression accuracy.",beginner,text,lines 8-9,quickstart_ml.mdx
set up Evidently for local use,It details the installation steps for the Evidently library and setting up the local environment for conducting data evaluations.,beginner,code,lines 27-29,quickstart_ml.mdx
data drift Python example,"The article provides a step-by-step example of generating a data drift report using Python, including necessary code snippets.",intermediate,code,lines 110-117,quickstart_ml.mdx
import datasets from OpenML,"It includes instructions on how to fetch and import datasets from OpenML, useful for preparing data for evaluation.",beginner,code,lines 58-61,quickstart_ml.mdx
create a dashboard in Evidently,"The article describes how to create a dashboard for monitoring data drift over time in Evidently, with example code for setting up visualizations.",advanced,code,lines 154-200,quickstart_ml.mdx
difference between data drift and prediction drift,"It explains the differences between data drift and prediction drift, and why both metrics are important for monitoring ML model performance.",intermediate,text,lines 11-12,quickstart_ml.mdx
setting up tracing for LLM app,"The article provides a step-by-step guide on how to set up tracing for an LLM app, detailing the tools needed and installation steps.",beginner,text,4-4,quickstart_tracing.mdx
installing Evidently and Tracely libraries,It includes Python commands to install the Evidently and Tracely libraries necessary for tracing in LLM applications.,beginner,code,20-25,quickstart_tracing.mdx
how to configure tracing in OpenAI app,"The tutorial outlines how to configure tracing in an OpenAI application, including setting up the tracing parameters and initializing the tracing library.",intermediate,code,72-91,quickstart_tracing.mdx
viewing traces in Evidently Cloud,"Instructions are provided on how to navigate to the 'Traces' section in Evidently Cloud to view, sort, and manage your traced dataset.",beginner,text,138-140,quickstart_tracing.mdx
running evaluations on traced dataset,"The article describes how to run evaluations on the traced dataset, both in the Cloud and locally, with code examples for loading and analyzing data.",advanced,code,156-188,quickstart_tracing.mdx
adversarial tests examples,"The article discusses how adversarial tests are designed to exploit model weaknesses, such as harmful content and prompt leakage, and how to set up these tests using Evidently Cloud.",beginner,text,lines 1-5,synthetic-data/adversarial_data.mdx
creating adversarial dataset Evidently,"The article provides a step-by-step guide on creating an adversarial dataset in the Evidently UI, including selecting test scenarios and configuring input numbers.",intermediate,code,lines 11-58,synthetic-data/adversarial_data.mdx
generate synthetic inputs for test cases,"The article details the process for generating synthetic input test cases, including defining scenarios and generating example inputs specific to your use case.",beginner,code,lines 1-5,synthetic-data/input_data.mdx
synthetic data generation examples,"The article discusses how Evidently Cloud can generate synthetic data for experiments, regression testing, and adversarial testing, providing practical applications and examples of input generation.",beginner,code,lines 5-13,synthetic-data/introduction.mdx
generate RAG test dataset from knowledge source,The article explains how to generate a RAG test dataset directly from your knowledge source by uploading your data and specifying parameters for input generation.,beginner,code,lines 7-21,synthetic-data/rag_data.mdx
using synthetic data for AI testing,"The article explains that synthetic data helps generate test datasets quickly and efficiently, especially when real data is scarce or inadequate for certain evaluations.",beginner,code,lines 16-21,synthetic-data/why_synthetic.mdx
how to create test datasets for LLM evaluations,"It outlines methods for creating test datasets, including manually, from real data, or synthetically, emphasizing the advantages of synthetic data for scaling and covering edge cases.",intermediate,text,lines 14-20,synthetic-data/why_synthetic.mdx
