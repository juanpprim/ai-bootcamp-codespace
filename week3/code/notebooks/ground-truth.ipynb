{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dd1485-e094-4189-ab7a-f5af32317d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f217dbed-9bda-4aac-bc66-b9b316c7ddd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docs\n",
    "\n",
    "raw_documents = docs.read_github_data()\n",
    "documents = docs.parse_data(raw_documents)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d10370-1423-4d7c-9613-e1b90605b3ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data definition 11\n",
      "descriptors 12\n",
      "overview 3\n",
      "metric generators 2\n",
      "output formats 1\n",
      "introduction 22\n",
      "report 4\n",
      "add tags and metadata 2\n",
      "tests 9\n",
      "alerts 1\n",
      "add dashboard panels (api) 13\n",
      "add dashboard panels (ui) 4\n",
      "overview 2\n",
      "overview 2\n",
      "work with datasets 2\n",
      "run evals via api 2\n",
      "explore view 1\n",
      "no code evals 4\n",
      "overview 2\n",
      "batch monitoring 2\n",
      "overview 3\n",
      "introduction 2\n",
      "manage projects 4\n",
      "overview 1\n",
      "overview 1\n",
      "set up tracing 10\n",
      "evidently cloud 1\n",
      "self-hosting 5\n",
      "evidently and github actions 1\n",
      "llm evaluations 2\n",
      "llm as a judge 21\n",
      "llm-as-a-jury 9\n",
      "rag evals 13\n",
      "llm regression testing 21\n",
      "tutorials and guides 12\n",
      "evidently cloud v2 1\n",
      "migration guide 7\n",
      "open-source vs. cloud 6\n",
      "telemetry 10\n",
      "why evidently? 4\n",
      "what is evidently? 1\n",
      "all descriptors 31\n",
      "all metrics 54\n",
      "overview 1\n",
      "customize data drift 17\n",
      "custom text descriptor 3\n",
      "use huggingface models 10\n",
      "configure llm judges 26\n",
      "custom metric 4\n",
      "classification metrics 8\n",
      "data stats and quality 7\n",
      "data drift 8\n",
      "ranking and recsys metrics 10\n",
      "regression metrics 9\n",
      "classification 3\n",
      "data drift 5\n",
      "data summary 3\n",
      "recommendations 3\n",
      "regression 3\n",
      "text evals 3\n",
      "llm evaluation 9\n",
      "data and ml checks 6\n",
      "tracing 5\n",
      "adversarial testing 2\n",
      "create synthetic inputs 1\n",
      "synthetic data 1\n",
      "rag evaluation dataset 1\n",
      "why synthetic data? 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_docs = []\n",
    "total_questions = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if 'title' not in doc:\n",
    "        continue\n",
    "\n",
    "    title = doc['title'].lower()\n",
    "\n",
    "    content = doc.get('content', '')\n",
    "\n",
    "    if len(content) < 1000:\n",
    "        continue\n",
    "    \n",
    "    if 'unpublished' in title:\n",
    "        continue\n",
    "\n",
    "    if 'legacy' in title:\n",
    "        continue\n",
    "\n",
    "    if 'leftovers' in title:\n",
    "        continue\n",
    "\n",
    "    if 'updates' in title:\n",
    "        continue\n",
    "\n",
    "    num_questions = len(content) // 1000\n",
    "    total_questions = total_questions + num_questions\n",
    "    print(title, num_questions)\n",
    "    selected_docs.append(doc)\n",
    "\n",
    "total_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "babdc647-46ea-4a83-8759-724f8da88ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cd98e8-bad1-4343-bb0b-796eb3bfc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea27637b-8cfe-4728-a32e-728a160d2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm_structured(instructions, user_prompt, output_format, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_format\n",
    "    )\n",
    "\n",
    "    return (response.output_parsed, response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ff88b3-e7f2-4096-9582-045603a4b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are given a technical article. Your task is to imagine what a person might type into a search engine \n",
    "before finding and reading this article.\n",
    "\n",
    "Generate realistic, human-like search queries — not formal questions. \n",
    "They should sound like what people actually type into Google or Stack Overflow \n",
    "when trying to solve a problem, learn a concept, or find code examples.\n",
    "\n",
    "Guidelines:\n",
    "- Avoid full-sentence questions with punctuation like \"What is...\" or \"How do I...\".\n",
    "- Use short, natural search phrases instead, such as:\n",
    "  - \"evidently data definition example\"\n",
    "  - \"map target and prediction columns evidently\"\n",
    "  - \"difference between timestamp and datetime evidently\"\n",
    "- Make queries varied and spontaneous, not repetitive or over-polished.\n",
    "- Assume users of different knowledge levels:\n",
    "  - beginner: broad or basic understanding\n",
    "  - intermediate: knows basic terms but seeks clarification or examples\n",
    "  - advanced: familiar with the tool, looking for details, edge cases, or integration options\n",
    "\n",
    "Distribution rules:\n",
    "- 60% of the queries should target beginner-level users\n",
    "- 30% should target intermediate-level users\n",
    "- 10% should target advanced-level users\n",
    "- 75% of queries should have an intent of \"code\" (looking for examples or implementation)\n",
    "- 25% should have an intent of \"text\" (looking for conceptual or theoretical explanations)\n",
    "\n",
    "For each generated query, include:\n",
    "- question: the natural, human-style search phrase\n",
    "- summary_answer: a short 1–2 sentence summary of how the article addresses it\n",
    "- difficulty: one of [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "- intent: one of [\"text\", \"code\"]\n",
    "\n",
    "Also include a description summarizing what kind of article the questions are about.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abce3a28-df8c-47d4-b12b-c1f8ee1c8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a realistic search-engine-style query a user might type before finding the article.\n",
    "    Each question captures the likely search phrase, a short summary answer,\n",
    "    the user's assumed skill level, and their intent (conceptual or code-focused).\n",
    "    \"\"\"\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"A natural, short search query — not a full-sentence question — phrased like something typed into Google.\"\n",
    "    )\n",
    "    summary_answer: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise 1–2 sentence summary of how the article addresses the query.\"\n",
    "    )\n",
    "    difficulty: Literal[\"beginner\", \"intermediate\", \"advanced\"] = Field(\n",
    "        ...,\n",
    "        description=\"The assumed knowledge level of the user making the query.\"\n",
    "    )\n",
    "    intent: Literal[\"text\", \"code\"] = Field(\n",
    "        ...,\n",
    "        description=\"Specifies if the user's intent is to get a theoretical explanation ('text') or an implementation example ('code').\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GeneratedQuestions(BaseModel):\n",
    "    \"\"\"\n",
    "    A structured collection of human-like search queries derived from a given article.\n",
    "    Includes a brief description of the article topic and a list of generated queries.\n",
    "    Difficulty distribution: 60% beginner, 30% intermediate, 10% advanced.\n",
    "    Intent distribution: 75% code-focused, 25% concept-focused.\n",
    "    \"\"\"\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"A summary of the article or topic these search-style questions were generated for.\"\n",
    "    )\n",
    "    questions: List[Question] = Field(\n",
    "        ...,\n",
    "        description=\"A list of realistic search queries with short summaries, difficulty levels, and user intent.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf5d34c-ed38-4e1d-9431-08010753f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def map_progress(pool, seq, f):\n",
    "    \"\"\"Map function f over seq using the provided executor pool while\n",
    "    displaying a tqdm progress bar. Returns a list of results in submission order.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "    \n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd90b4b-ea2e-498f-a09c-06e94b82ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(doc):\n",
    "    content = doc['content']\n",
    "    num_questions = len(content) // 1000\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    generate {num_questions} questions for this document:\n",
    "{json.dumps(doc)}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    output, usage = llm_structured(\n",
    "        instructions=instructions,\n",
    "        user_prompt=user_prompt,\n",
    "        output_format=GeneratedQuestions,\n",
    "    )\n",
    "\n",
    "    return {'doc': doc, 'questions': output, 'usage': usage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34deb7e2-cfd0-43aa-9cd5-b987e41cd70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 68/68 [01:13<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=6) as pool:\n",
    "    results = map_progress(pool, selected_docs, process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2a3ecf-d000-4c1f-a8de-8b3f57a750b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e23539e-20ff-4c45-9ad1-744b56407354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "\n",
    "pricing = PricingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ceb8cdf-d317-42e7-bc5f-0206a51df18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CostInfo(input_cost=0.026054099999999997, output_cost=0.014013, total_cost=0.040067099999999994)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = 0\n",
    "output_tokens = 0\n",
    "\n",
    "for r in results:\n",
    "    usage = r['usage']\n",
    "    input_tokens = input_tokens + usage.input_tokens\n",
    "    output_tokens = output_tokens + usage.output_tokens\n",
    "    \n",
    "pricing.calculate_cost('gpt-4o-mini', input_tokens, output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dac0a27-f8c8-4138-9f95-6733fdbd92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_questions = []\n",
    "\n",
    "for r in results:\n",
    "    doc = r['doc']\n",
    "    questions = r['questions']\n",
    "\n",
    "    for q in questions.questions:\n",
    "        final_question = q.model_dump()\n",
    "        final_question['filename'] = doc['filename']\n",
    "        final_questions.append(final_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "473ce08f-46a2-4291-9410-0dfeb1569951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3441d4fd-fe64-42ad-a500-86d3ee488c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_questions = pd.DataFrame(final_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5e93a01-dcf4-4ee4-aa07-ca3fbd73316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.to_csv('ground_truth_evidently.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b142077a-99dc-4756-9952-b8beba47b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question,summary_answer,difficulty,intent,filename\n",
      "how to create a Dataset object in Evidently,\"To create a `Dataset` object, use `Dataset.from_pandas()` and pass your data along with a `DataDefinition` object to specify column types and roles.\",beginner,code,docs/library/data_definition.mdx\n",
      "Evidently DataDefinition examples,\"The `DataDefinition` allows users to specify mappings for column types like numerical, categorical, and text to ensure proper data evaluations in Evidently.\",beginner,code,docs/library/data_definition.mdx\n",
      "difference between numerical and categorical columns in Evidently,\"Numerical columns contain numeric values used for calculations, while categorical columns represent discrete categories or groups, each serving different roles in data evaluations.\",intermediate,text,docs/library/data_definition.mdx\n",
      "how to manually map columns in DataDefinition,\"You can manually map columns in `DataDefinition` by specifying each column in its respective category, such as `numerical_columns` or `text_columns`, instead of relying on automatic mapping.\",intermediate,code,docs/library/data_definition.mdx\n",
      "example of mapping regression columns in Evidently,\"To map regression columns, define the target and prediction columns in `DataDefinition` using `Regression(target='y_true', prediction='y_pred')` for example.\",intermediate,code,docs/library/data_definition.mdx\n",
      "automatically mapping columns in Evidently,\"You can map columns automatically by passing an empty `DataDefinition()` to `Dataset.from_pandas()`, allowing Evidently to infer column types and roles based on their names and data types.\",beginner,code,docs/library/data_definition.mdx\n",
      "role of ID and timestamp columns in data definition,\"ID columns serve as unique identifiers ignored in calculations, while timestamp columns track when data entries are recorded, also ignored in some evaluations like drift detection.\",intermediate,text,docs/library/data_definition.mdx\n",
      "importance of text column mapping in Evidently,\"Mapping text columns in `DataDefinition` is crucial for analyses like text data drift detection. Without it, the system may not accurately process those columns during evaluations.\",intermediate,text,docs/library/data_definition.mdx\n",
      "Evidently DataDefinition for multiclass classification,\"For multiclass classification in `DataDefinition`, use `MulticlassClassification` to specify target, prediction labels, and optionally, prediction probabilities and display labels.\",advanced,code,docs/library/data_definition.mdx\n"
     ]
    }
   ],
   "source": [
    "!head ground_truth_evidently.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca7828-8282-4be7-91fb-26a3ac98b6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
