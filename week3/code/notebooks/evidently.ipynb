{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b77bc3f-ed72-4cf6-93e3-e4eb45979364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aaa31db-1a26-4ef6-bc1f-3a55f9de06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "effebce8-76b0-48a5-ab20-61a9d6f37101",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./eval-run-v2-2025-10-24-21-42.bin', 'rb') as f_in:\n",
    "    rows = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a664fd-e8a1-484d-9b33-93308b137a47",
   "metadata": {},
   "source": [
    "- match\n",
    "- partial match\n",
    "- mismatch\n",
    "- not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c05ba4c-bafb-4bb8-bf0b-81dbb6e7130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c67330d9-e4a8-479a-95e7-9393a4e30e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "583956d4-5358-439d-a99e-8ef3e76bbf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = {d['filename']: d['content'] for d in parsed_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "187f2959-8108-47ee-b6b0-2ff51df95abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b64ef18-244a-45c3-ba00-1439bb1d83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evals = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa9bf09a-6b39-4e7a-bdf2-ea52174f51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evals['filename'] = df_evals['original_question'].apply(lambda r: r['filename'])\n",
    "df_evals['reference'] = df_evals['filename'].apply(file_index.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e3b0e28-811e-475e-9084-361d6b0d5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently import Dataset, DataDefinition\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.templates import MulticlassClassificationPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ee516d5-117a-4f3e-b709-6e67b49d9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = MulticlassClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are a judge that evaluates the factual alignment of two chatbot answers.\")],\n",
    "    criteria=\"\"\"\n",
    "    You are given a new answer and a reference answer and also the question.\n",
    "    Classify the new answer based on how it compares to the reference.\n",
    "    ===\n",
    "    Question: {question}\n",
    "    Reference: {reference}\n",
    "    \"\"\",\n",
    "    category_criteria={\n",
    "        \"match\": \"The answer matches the reference in all factual and semantic details.\",\n",
    "        \"partial_match\": \"The answer is correct in what it says but leaves out details from the reference.\",\n",
    "        \"mismatch\": \"The answer doesn't match the reference answer.\",\n",
    "        \"not_available\": \"The answer says that information is not available.\",\n",
    "    },\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    "    include_scores=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ad48077-d94b-4be8-b8ed-cc75b685404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_dataset = Dataset.from_pandas(\n",
    "    data=df_evals,\n",
    "    data_definition=DataDefinition(),\n",
    "    descriptors=[\n",
    "        LLMEval(\"answer\",\n",
    "            template=matcher,\n",
    "            additional_columns={\"reference\": \"reference\", \"question\": \"question\"},\n",
    "            provider=\"openai\",\n",
    "            model=\"gpt-4o-mini\",\n",
    "            alias=\"eval\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81c885f3-7cc0-4904-a0bf-8a6e59d75eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = evals_dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25de6527-2486-4b8b-8def-1855322444a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The new answer provides an overview of dataset evaluation metrics, mentioning model quality, classification quality metrics, and regression metrics, which aligns with the reference's focus. However, it lacks specifics about dataset-level metrics mentioned in the reference, like examples of parameters used and different metric options. It also does not reference the contents of the accordion details present in the original reference, thus missing some details.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result['eval reasoning'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f1cfd01-be6d-44b7-8781-a6b8f5ce3329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval\n",
       "partial_match    0.653846\n",
       "match            0.346154\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result['eval'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cdbdb38-3bec-4db8-8a82-d4c8fb4b2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently import Report\n",
    "from evidently.presets import TextEvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ba31300-1aa2-4549-9cee-51d3660e2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report([\n",
    "    TextEvals()\n",
    "])\n",
    "\n",
    "my_eval = report.run(evals_dataset, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfacd5c-38aa-4593-aad5-fd2c6719f709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bcf472-44a4-4baf-8a40-4a7ec8c0c0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
