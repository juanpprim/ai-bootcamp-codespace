{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e39d05e9-35a5-4123-ab26-e36738b2caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "357c24e9-0f08-4e24-a8f1-f5370bbb904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import search_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0c3030e-fade-4206-a976-bdfd4cfb554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d692f4e1-708b-4a44-80c3-03f4d3b576f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval-run-v2-2025-10-22-22-25.bin', 'rb') as f_in:\n",
    "    rows = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "09a54d01-4596-4976-b424-76151e688497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3809953f-80f3-4701-a213-0e9b488a2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4dd2d669-ce2e-443a-95a7-e7eddb190e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "\n",
    "file_index = {d['filename']: d['content'] for d in parsed_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f1abfdf-afff-453f-8ea9-0937cfbe093d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metrics/all_metrics.mdx'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]['original_question']['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e7a48efd-4dd2-44ae-89e4-21fbb453ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cdd45eee-274b-409d-90c6-d69bf7c6c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6db9c90f-2f66-4b51-8594-cc20781dd331",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_instructions = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent’s answer\n",
    "(<ANSWER>) to a user question (<QUESTION>). We also include the\n",
    "entire log (<LOG>) for analysis. In <REFERENCE> you will see\n",
    "the file, from which the user question was generated. \n",
    "\n",
    "For each item of the checklist, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user’s instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user’s question  \n",
    "- answer_match: The ANSWER is similar to the REFERENCE\n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e42f90b0-624f-42c3-9745-7d3b102de037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7a1e1083-5e4a-4942-a999-6b6245e61144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    reasoning: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "315a0f22-10b8-47a3-9131-a73eb22304cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = Agent(\n",
    "    name=\"judge\",\n",
    "    instructions=judge_instructions,\n",
    "    model=\"gpt-5-nano\",\n",
    "    output_type=EvaluationChecklist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a7d59b3-4913-45df-ab60-ffebcbc9ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "async def map_progress(seq, f, max_concurrency=6):\n",
    "    \"\"\"Asynchronously map async function f over seq with progress bar.\"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    async def run(el):\n",
    "        async with semaphore:\n",
    "            return await f(el)\n",
    "\n",
    "    # create one coroutine per element\n",
    "    coros = [run(el) for el in seq]\n",
    "\n",
    "    # turn them into tasks that complete as they finish\n",
    "    completed = asyncio.as_completed(coros)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for coro in tqdm(completed, total=len(seq)):\n",
    "        result = await coro\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "36ec00dc-6a1a-44ea-a7a9-e7973d0c22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_eval(row):\n",
    "    original_filename = row['original_question']['filename']\n",
    "    reference = file_index[original_filename]\n",
    "    user_prompt = f\"\"\"\n",
    "<INSTRUCTIONS>{search_agent.search_instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{row['question']}</QUESTION>\n",
    "<ANSWER>{row['answer']}</ANSWER>\n",
    "<REFERENCE>{reference}</REFERENCE>\n",
    "<LOG>{json.dumps(row['messages'])}</LOG>\n",
    "\"\"\".strip()\n",
    "    output = await judge.run(user_prompt=user_prompt)\n",
    "    return row, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ffa28e65-077e-4e02-88a3-f5f83f94f7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e79ee6c0e1f4612af063c95f6beccb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = await map_progress(rows, run_eval, max_concurrency=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8aa4d8f0-a9f1-4151-a0ec-c114fb9fc966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "pricing = PricingConfig()\n",
    "\n",
    "def calculate_cost(model, all_results):\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    \n",
    "    for _, r in all_results:\n",
    "        usage = r.usage()\n",
    "        input_tokens = input_tokens + usage.input_tokens\n",
    "        output_tokens = output_tokens + usage.output_tokens\n",
    "\n",
    "    cost = pricing.calculate_cost(model, input_tokens, output_tokens)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45629b1f-55f0-4129-902f-e0f47503a976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CostInfo(input_cost=0.0016259, output_cost=0.030878000000000003, total_cost=0.0325039)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cost('gpt-5-nano', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a67e356c-b20c-4b12-a915-84f665fd59e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CostInfo(input_cost=0.0087156, output_cost=0.0351392, total_cost=0.0438548)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cost('gpt-5-nano', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2fe4750f-01c6-419a-8e73-1bf0e1f5870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checks = []\n",
    "\n",
    "for original_row, result in results[1:]:\n",
    "    checks = result.output.checklist\n",
    "    checks_formatted = {\n",
    "        'question': original_row['question']\n",
    "    }\n",
    "    for check in checks:\n",
    "        checks_formatted[check.check_name] = check.check_pass\n",
    "    all_checks.append(checks_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "74a4fefe-b700-44d0-8d67-2699d4914d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f7656b74-bb96-4d3a-8ed5-f3946b1a3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(all_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d3944ab9-b36d-4cc4-b131-e428785c010a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow       0.625\n",
       "instructions_avoid          1.0\n",
       "answer_relevant        0.958333\n",
       "answer_match           0.708333\n",
       "answer_clear           0.916667\n",
       "answer_citations       0.958333\n",
       "completeness           0.833333\n",
       "tool_call_search           0.64\n",
       "dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[df_eval.columns[1:]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfeae84-4d6a-4a5d-b09b-054fae2c77f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "880f2e5a-e600-4c88-a4ed-adb812e2554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CheckName(str, Enum):\n",
    "    instructions_follow = \"instructions_follow\"\n",
    "    instructions_avoid = \"instructions_avoid\" \n",
    "    answer_relevant = \"answer_relevant\"\n",
    "    answer_clear = \"answer_clear\"\n",
    "    answer_citations = \"answer_citations\"\n",
    "    completeness = \"completeness\"\n",
    "    tool_call_search = \"tool_call_search\"\n",
    "\n",
    "CHECK_DESCRIPTIONS = {\n",
    "    CheckName.instructions_follow: \"The agent followed the user's instructions (in <INSTRUCTIONS>)\",\n",
    "    CheckName.instructions_avoid: \"The agent avoided doing things it was told not to do\",\n",
    "    CheckName.answer_relevant: \"The response directly addresses the user's question\",\n",
    "    CheckName.answer_clear: \"The answer is clear and correct\",\n",
    "    CheckName.answer_citations: \"The response includes proper citations or sources when required\",\n",
    "    CheckName.completeness: \"The response is complete and covers all key aspects of the request\",\n",
    "    CheckName.tool_call_search: \"Is the search tool invoked?\"\n",
    "}\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: CheckName = Field(description=\"The type of evaluation check\")\n",
    "    reasoning: str = Field(description=\"The reasoning behind the check result\")\n",
    "    check_pass: bool = Field(description=\"Whether the check passed (True) or failed (False)\")\n",
    "    \n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck] = Field(description=\"List of all evaluation checks\")\n",
    "    summary: str = Field(description=\"Evaluation summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "60b28638-6e91-4af2-9fcf-2ecf688ec6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_checklist_text():\n",
    "    checklist_items = []\n",
    "    for check_name in CheckName:\n",
    "        description = CHECK_DESCRIPTIONS[check_name]\n",
    "        checklist_items.append(f\"- {check_name.value}: {description}\")\n",
    "    return \"\\n\".join(checklist_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2a7f276-2c40-425b-a285-257da6e4f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_instructions = f\"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent’s answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "{generate_checklist_text()}\n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1959395-eaf6-4262-97cf-b72292583c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3708b8-51b2-4025-b20f-9fb0f5ff8c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
