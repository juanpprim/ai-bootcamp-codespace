{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69291aa2-b47d-410d-9dd5-4701a0fe96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b77bc3f-ed72-4cf6-93e3-e4eb45979364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67330d9-e4a8-479a-95e7-9393a4e30e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "\n",
    "file_index = {d['filename']: d['content'] for d in parsed_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "effebce8-76b0-48a5-ab20-61a9d6f37101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./eval-run-v2-2025-10-24-21-42.bin', 'rb') as f_in:\n",
    "    rows = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b64ef18-244a-45c3-ba00-1439bb1d83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "\n",
    "df_evals['filename'] = df_evals['original_question'].apply(lambda r: r['filename'])\n",
    "df_evals['reference'] = df_evals['filename'].apply(file_index.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9bf09a-6b39-4e7a-bdf2-ea52174f51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dfacd5c-38aa-4593-aad5-fd2c6719f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\": \"run LLM as judge in Evidently\"})\n",
      "TOOL CALL (search): search({\"query\": \"LLM judge\"})\n",
      "TOOL CALL (search): search({\"query\": \"evaluate LLM performance Evidently\"})\n",
      "TOOL CALL (search): search({\"query\": \"Evidently LLM integration\"})\n",
      "TOOL CALL (search): search({\"query\": \"use LLM as evaluator\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"examples/LLM_judge.mdx\"})\n"
     ]
    }
   ],
   "source": [
    "result = await main.run_agent('how do I run llm as a judge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9bcf472-44a4-4baf-8a40-4a7ec8c0c0f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# How to Run LLM as a Judge in Evidently\n",
      "\n",
      "## Overview\n",
      "\n",
      "In this tutorial, you will learn how to use a Language Model (LLM) as a judge to evaluate text based on custom criteria. Two main approaches will be covered: reference-based evaluation against known accurate responses, and open-ended evaluation based on custom metrics.\n",
      "\n",
      "### References\n",
      "- [LLM as a judge](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx)\n",
      "## Tutorial Steps\n",
      "\n",
      "### 1. Environment Setup\n",
      "- Install the Evidently library:\n",
      "    ```bash\n",
      "    pip install evidently\n",
      "    ```\n",
      "- Import necessary modules in your Python script:\n",
      "    ```python\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    from evidently import Dataset, DataDefinition, Report\n",
      "    from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "    import os\n",
      "    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "    ```\n",
      "\n",
      "### 2. Create the Evaluation Dataset\n",
      "- Create a toy dataset containing questions, target responses, new responses, and their labels:\n",
      "    ```python\n",
      "    data = [\n",
      "        [\"Hi there, how do I reset my password?\", \n",
      "         \"To reset your password, click on 'Forgot Password'...],\n",
      "        ...\n",
      "    ]\n",
      "    columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\n",
      "    golden_dataset = pd.DataFrame(data, columns=columns)\n",
      "    \n",
      "    definition = DataDefinition(\n",
      "        text_columns=[\"question\", \"target_response\", \"new_response\"],\n",
      "        categorical_columns=[\"label\"]\n",
      "    )\n",
      "    eval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\n",
      "    ```\n",
      "\n",
      "### 3. Configure the LLM Judge\n",
      "- Create a prompt template for evaluating correctness:\n",
      "    ```python\n",
      "    correctness = BinaryClassificationPromptTemplate(\n",
      "        criteria = \"An ANSWER is correct when it matches the REFERENCE in all details...\",\n",
      "        target_category=\"incorrect\",\n",
      "        non_target_category=\"correct\",\n",
      "        uncertainty=\"unknown\",\n",
      "        include_reasoning=True,\n",
      "        pre_messages=[(\"system\", \"You are an expert evaluator...\")]\n",
      "    )\n",
      "    ```\n",
      "- Add the LLM evaluator to your dataset:\n",
      "    ```python\n",
      "    eval_dataset.add_descriptors(descriptors=[\n",
      "        LLMEval(\"new_response\",\n",
      "                template=correctness,\n",
      "                provider = \"openai\",\n",
      "                model = \"gpt-4o-mini\",\n",
      "                alias=\"Correctness\",\n",
      "                additional_columns={\"target_response\": \"target_response\"}),\n",
      "    ])\n",
      "    ```\n",
      "\n",
      "### 4. Generate and Review Report\n",
      "- Generate evaluation report:\n",
      "    ```python\n",
      "    report = Report([\n",
      "        TextEvals()\n",
      "    ])\n",
      "    my_eval = report.run(eval_dataset, None)\n",
      "    my_eval\n",
      "    ```\n",
      "- To visualize results, output the dataframe:\n",
      "    ```python\n",
      "    eval_dataset.as_dataframe()\n",
      "    ```\n",
      "\n",
      "### 5. Create Verbosity Evaluator\n",
      "- To check for conciseness in responses, set up a verbosity template and evaluate likewise.\n",
      "\n",
      "### References\n",
      "- [LLM as a judge](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx)\n",
      "## References\n",
      "- [LLM as a judge](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.output.format_article())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50cd3dd0-e5dc-409f-9e25-4a4b382bf1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently import Dataset, DataDefinition, Report\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40fe426c-7aef-494e-bac9-eb5a3aebb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = DataDefinition(\n",
    "    text_columns=[\"question\", \"reference\", \"answer\"],\n",
    ")\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(\n",
    "    df_evals,\n",
    "    data_definition=definition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef9293a-c308-44bb-89d5-f106df6b565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness = BinaryClassificationPromptTemplate(\n",
    "    criteria=\"An ANSWER is correct when it matches the REFERENCE in all details\",\n",
    "    target_category=\"incorrect\",\n",
    "    non_target_category=\"correct\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    "    pre_messages=[(\"system\", \"You are an expert evaluator\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "598b2b92-51c4-4e86-8fe8-013e9c4ac5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.descriptors import LLMEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8e10a7a-d138-4ee1-bdbd-2b7967000a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.add_descriptors(descriptors=[\n",
    "    LLMEval(\n",
    "        \"answer\",\n",
    "        template=correctness,\n",
    "        provider=\"openai\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        alias=\"Correctness\",\n",
    "        additional_columns={\"reference\": \"reference\", \"question\": \"question\"}\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65b829f8-e0f3-4be1-91db-d765769ad4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently import Report\n",
    "from evidently.presets import TextEvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95567105-e8df-4bd0-864f-873a5425e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eval_dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2048e7f8-4a16-4955-9662-2e115be585fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>messages</th>\n",
       "      <th>tool_call_number</th>\n",
       "      <th>requests</th>\n",
       "      <th>original_question</th>\n",
       "      <th>original_result</th>\n",
       "      <th>filename</th>\n",
       "      <th>reference</th>\n",
       "      <th>Correctness</th>\n",
       "      <th>Correctness reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>regexp descriptor usage</td>\n",
       "      <td># Evidently Regexp Descriptor Usage\\n\\n## Over...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'regexp de...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>{'question': 'regexp descriptor usage', 'summa...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>metrics/all_descriptors.mdx</td>\n",
       "      <td>&lt;Info&gt;\\n  For an intro, read about [Core Conce...</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The text contains duplicated references statin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset-level evaluation metrics</td>\n",
       "      <td># Dataset Level Evaluation Metrics\\n\\n## Overv...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'dataset-l...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>{'question': 'dataset-level evaluation metrics...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>metrics/all_metrics.mdx</td>\n",
       "      <td>&lt;Info&gt;\\n  For an intro, read [Core Concepts](/...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The provided text accurately describes the dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customize Text Evals report conditions</td>\n",
       "      <td># Text Evals Report Customization\\n\\n## Custom...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'customize...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'question': 'customize Text Evals report cond...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>metrics/preset_text_evals.mdx</td>\n",
       "      <td>To run this Report, first compute `descriptors...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The text provides accurate information on cust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>automatically map data columns</td>\n",
       "      <td># Automatic Data Column Mapping in Evidently\\n...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'automatic...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>{'question': 'automatically map data columns',...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>docs/library/data_definition.mdx</td>\n",
       "      <td>To run evaluations, you must create a `Dataset...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The text accurately describes the automatic da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>using synthetic data for AI testing</td>\n",
       "      <td># Using Synthetic Data for AI Testing\\n\\n## Ge...</td>\n",
       "      <td>[{'kind': 'user-prompt', 'content': 'using syn...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'question': 'using synthetic data for AI test...</td>\n",
       "      <td>AgentRunResult(output=SearchResultArticle(foun...</td>\n",
       "      <td>synthetic-data/why_synthetic.mdx</td>\n",
       "      <td>When working on an AI system, you need test da...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The text accurately describes the process of g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 question  \\\n",
       "0                 regexp descriptor usage   \n",
       "1        dataset-level evaluation metrics   \n",
       "2  customize Text Evals report conditions   \n",
       "3          automatically map data columns   \n",
       "4     using synthetic data for AI testing   \n",
       "\n",
       "                                              answer  \\\n",
       "0  # Evidently Regexp Descriptor Usage\\n\\n## Over...   \n",
       "1  # Dataset Level Evaluation Metrics\\n\\n## Overv...   \n",
       "2  # Text Evals Report Customization\\n\\n## Custom...   \n",
       "3  # Automatic Data Column Mapping in Evidently\\n...   \n",
       "4  # Using Synthetic Data for AI Testing\\n\\n## Ge...   \n",
       "\n",
       "                                            messages  tool_call_number  \\\n",
       "0  [{'kind': 'user-prompt', 'content': 'regexp de...                 8   \n",
       "1  [{'kind': 'user-prompt', 'content': 'dataset-l...                 4   \n",
       "2  [{'kind': 'user-prompt', 'content': 'customize...                 5   \n",
       "3  [{'kind': 'user-prompt', 'content': 'automatic...                 6   \n",
       "4  [{'kind': 'user-prompt', 'content': 'using syn...                 5   \n",
       "\n",
       "   requests                                  original_question  \\\n",
       "0         4  {'question': 'regexp descriptor usage', 'summa...   \n",
       "1         2  {'question': 'dataset-level evaluation metrics...   \n",
       "2         2  {'question': 'customize Text Evals report cond...   \n",
       "3         2  {'question': 'automatically map data columns',...   \n",
       "4         2  {'question': 'using synthetic data for AI test...   \n",
       "\n",
       "                                     original_result  \\\n",
       "0  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "1  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "2  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "3  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "4  AgentRunResult(output=SearchResultArticle(foun...   \n",
       "\n",
       "                           filename  \\\n",
       "0       metrics/all_descriptors.mdx   \n",
       "1           metrics/all_metrics.mdx   \n",
       "2     metrics/preset_text_evals.mdx   \n",
       "3  docs/library/data_definition.mdx   \n",
       "4  synthetic-data/why_synthetic.mdx   \n",
       "\n",
       "                                           reference Correctness  \\\n",
       "0  <Info>\\n  For an intro, read about [Core Conce...   incorrect   \n",
       "1  <Info>\\n  For an intro, read [Core Concepts](/...     correct   \n",
       "2  To run this Report, first compute `descriptors...     correct   \n",
       "3  To run evaluations, you must create a `Dataset...     correct   \n",
       "4  When working on an AI system, you need test da...     correct   \n",
       "\n",
       "                               Correctness reasoning  \n",
       "0  The text contains duplicated references statin...  \n",
       "1  The provided text accurately describes the dat...  \n",
       "2  The text provides accurate information on cust...  \n",
       "3  The text accurately describes the automatic da...  \n",
       "4  The text accurately describes the process of g...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "985c6265-ce27-4bfc-9f46-bc1ab7ca3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Dataset Level Evaluation Metrics\n",
      "\n",
      "## Overview of Dataset Evaluation Metrics\n",
      "\n",
      "Evidently provides several dataset-level evaluation metrics that help users understand the quality of their datasets and the performance of machine learning models. Key metrics include:**\n",
      "  - **Model Quality Summary Metrics**: These metrics consist of Mean Error (ME), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE), which give a quantitative assessment of model predictions relative to actual values.\n",
      "  - **Classification Quality Metrics**: Classification-based evaluation metrics provide insights into how well the model classifies data into categories.\n",
      "  - **Regression Metrics**: These include metrics for assessing performance in regression tasks such as errors in predictions, plotting predicted versus actual values, error distributions, and evaluating specific groups of predictions (e.g., underestimations and overestimations).**\n",
      "\n",
      "Evidently's reporting tools also allow for interactive visualizations, which highlights mistakes made by models and guides improvement efforts. Reports can summarize evaluation results effectively for teams to analyze.\n",
      "\n",
      "Metrics are aggregated effectively, and users can define which metrics to include based on their needs, enhancing transparency around model performance and data quality issues.\n",
      "\n",
      "### References\n",
      "- [Overview](https://github.com/evidentlyai/docs/blob/main/docs/platform/datasets_overview.mdx)\n",
      "- [All Metrics](https://github.com/evidentlyai/docs/blob/main/metrics/all_metrics.mdx)\n",
      "- [Regression metrics](https://github.com/evidentlyai/docs/blob/main/metrics/explainer_regression.mdx)\n",
      "## References\n",
      "- [Overview](https://github.com/evidentlyai/docs/blob/main/docs/platform/datasets_overview.mdx)\n",
      "- [All Metrics](https://github.com/evidentlyai/docs/blob/main/metrics/all_metrics.mdx)\n",
      "- [Regression metrics](https://github.com/evidentlyai/docs/blob/main/metrics/explainer_regression.mdx)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c40db6ff-b769-46bf-bf58-b6c92665a514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided text accurately describes the dataset level evaluation metrics from Evidently, detailing the key metrics used for assessing model performance, including Mean Error, Mean Absolute Error, and Mean Absolute Percentage Error, as well as classification and regression metrics. It also mentions the interactive visualizations and reporting tools, which align with the referenced sources.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]['Correctness reasoning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4986fa3-5b20-4b42-9462-f422b3a330a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently import Dataset, Report\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c23d80-3c40-49ca-8a9c-b8fb6541881a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5da53c-a701-4846-8946-dd7dc409c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_evals = Dataset.from_pandas(\n",
    "   df_evals,\n",
    "   data_definition=DataDefinition(),\n",
    "   descriptors=[\n",
    "       LLMEval(\"response\", template=appropriate_scope, provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "   ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
